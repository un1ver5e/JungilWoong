{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f3f67fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "import os # linux 명령어\n",
    "# ModelCheckpoint 더이상 모델이 좋아지지 않으면 저장하지 않음\n",
    "# EarlyStopping 더이상 모델이 좋아지지 않으면 epoch 전부 수행하지 않고 멈추기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d2dd9",
   "metadata": {},
   "source": [
    "# 베스트 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fbfdc",
   "metadata": {},
   "source": [
    "### 그래프로 정확도 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3b2cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0805cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.053</td>\n",
       "      <td>20.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.99373</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.047</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.99164</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.54</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.049</td>\n",
       "      <td>56.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.66</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.074</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99370</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.034</td>\n",
       "      <td>29.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99170</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.059</td>\n",
       "      <td>29.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.99177</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.41</td>\n",
       "      <td>12.1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>48.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.99110</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.51</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.147</td>\n",
       "      <td>38.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.059</td>\n",
       "      <td>45.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.55</td>\n",
       "      <td>10.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>8.7</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.053</td>\n",
       "      <td>27.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.43</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3      4     5      6        7     8     9     10  11  12\n",
       "5316  6.3  0.18  0.24   3.4  0.053  20.0  119.0  0.99373  3.11  0.52   9.2   6   0\n",
       "5210  6.8  0.14  0.18   1.4  0.047  30.0   90.0  0.99164  3.27  0.54  11.2   6   0\n",
       "3518  7.3  0.22  0.50  13.7  0.049  56.0  189.0  0.99940  3.24  0.66   9.0   6   0\n",
       "1622  7.6  0.67  0.14   1.5  0.074  25.0  168.0  0.99370  3.05  0.51   9.3   5   0\n",
       "2443  7.3  0.21  0.29   1.6  0.034  29.0  118.0  0.99170  3.30  0.50  11.0   8   0\n",
       "...   ...   ...   ...   ...    ...   ...    ...      ...   ...   ...   ...  ..  ..\n",
       "4931  6.5  0.22  0.28   3.7  0.059  29.0  151.0  0.99177  3.23  0.41  12.1   7   0\n",
       "3264  6.5  0.13  0.37   1.0  0.036  48.0  114.0  0.99110  3.41  0.51  11.5   8   0\n",
       "1653  6.8  0.20  0.59   0.9  0.147  38.0  132.0  0.99300  3.05  0.38   9.1   6   0\n",
       "2607  6.6  0.22  0.37   1.2  0.059  45.0  199.0  0.99300  3.37  0.55  10.3   7   0\n",
       "2732  8.7  0.22  0.42   2.3  0.053  27.0  114.0  0.99400  2.99  0.43  10.0   5   0\n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre = pd.read_csv(\"./Dataset/wine.csv\", header = None)\n",
    "df_pre.sample(frac=0.15) # 전체 데이터 15%만 sample로 random하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6ace568",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_pre.values\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ba36f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# {epoch:횟수} - {val_loss:손실값}\n",
    "modelpath = \"./model/{epoch:02d} - {val_loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "854fb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model set\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = X.shape[1], activation = \"relu\"))\n",
    "model.add(Dense(12, activation = \"relu\"))\n",
    "model.add(Dense(8, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# model compile\n",
    "model.compile(loss = \"BCE\", optimizer = \"adam\", metrics=[\"ACC\"])\n",
    "\n",
    "# checkpoint\n",
    "checkpointer = ModelCheckpoint(modelpath, monitor = 'val_loss', # ACC 넣으면 정확도\n",
    "                               verbose = 1, save_best_only = True) \n",
    "                    # verbose:게이지, save_best_only = True: 좋아지면 save\n",
    "\n",
    "# model run\n",
    "# history = model.fit(X, Y, epochs = 200, batch_size = 30, \n",
    "#           validation_split = 0.3, callbacks = [checkpointer], verbose = 0)\n",
    "            # validation_split, sklearn.train_test_split() = train set, test set 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a09c77be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history[\"val_loss\"]\n",
    "y_acc = history.history[\"ACC\"]\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c = \"red\", markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c = \"blue\", markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa764f",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "- 테스트 오차가 좋아지지 않으면 epoch를 전부 수행하지 않고 멈춤\n",
    "- 테스트 오차가 좋아지지 않아도 몇번 까지 기다릴지 설정 할 수 있음\n",
    "- patience = int : int 만큼 기다리고 나아지지 않으면 멈춰!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc44e965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 1/11 [=>............................] - ETA: 3s - loss: 26.9604 - ACC: 0.2740\n",
      "Epoch 00001: val_loss improved from inf to 18.89569, saving model to ./model\\01 - 18.8957.hdf5\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 20.4068 - ACC: 0.3077 - val_loss: 18.8957 - val_ACC: 0.0000e+00\n",
      "Epoch 2/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 14.7552 - ACC: 0.2980\n",
      "Epoch 00002: val_loss improved from 18.89569 to 8.24334, saving model to ./model\\02 - 8.2433.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.6432 - ACC: 0.3077 - val_loss: 8.2433 - val_ACC: 0.0000e+00\n",
      "Epoch 3/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 6.8871 - ACC: 0.2660\n",
      "Epoch 00003: val_loss improved from 8.24334 to 3.22446, saving model to ./model\\03 - 3.2245.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2089 - ACC: 0.3077 - val_loss: 3.2245 - val_ACC: 0.0000e+00\n",
      "Epoch 4/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 2.6150 - ACC: 0.2960\n",
      "Epoch 00004: val_loss improved from 3.22446 to 1.37278, saving model to ./model\\04 - 1.3728.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9243 - ACC: 0.3077 - val_loss: 1.3728 - val_ACC: 0.0000e+00\n",
      "Epoch 5/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 1.2112 - ACC: 0.2760\n",
      "Epoch 00005: val_loss improved from 1.37278 to 0.79888, saving model to ./model\\05 - 0.7989.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8681 - ACC: 0.3077 - val_loss: 0.7989 - val_ACC: 0.0000e+00\n",
      "Epoch 6/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.7021 - ACC: 0.3260\n",
      "Epoch 00006: val_loss improved from 0.79888 to 0.74219, saving model to ./model\\06 - 0.7422.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6833 - ACC: 0.3077 - val_loss: 0.7422 - val_ACC: 0.0000e+00\n",
      "Epoch 7/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6462 - ACC: 0.3420\n",
      "Epoch 00007: val_loss improved from 0.74219 to 0.73031, saving model to ./model\\07 - 0.7303.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6488 - ACC: 0.3113 - val_loss: 0.7303 - val_ACC: 0.1162\n",
      "Epoch 8/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.6385 - ACC: 0.3000\n",
      "Epoch 00008: val_loss improved from 0.73031 to 0.72701, saving model to ./model\\08 - 0.7270.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6141 - ACC: 0.4839 - val_loss: 0.7270 - val_ACC: 0.6762\n",
      "Epoch 9/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5754 - ACC: 0.7180\n",
      "Epoch 00009: val_loss improved from 0.72701 to 0.71684, saving model to ./model\\09 - 0.7168.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5825 - ACC: 0.7928 - val_loss: 0.7168 - val_ACC: 0.8523\n",
      "Epoch 10/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5718 - ACC: 0.8880\n",
      "Epoch 00010: val_loss improved from 0.71684 to 0.71440, saving model to ./model\\10 - 0.7144.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5566 - ACC: 0.9017 - val_loss: 0.7144 - val_ACC: 0.8692\n",
      "Epoch 11/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5410 - ACC: 0.8800\n",
      "Epoch 00011: val_loss improved from 0.71440 to 0.70891, saving model to ./model\\11 - 0.7089.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5418 - ACC: 0.9059 - val_loss: 0.7089 - val_ACC: 0.9123\n",
      "Epoch 12/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5372 - ACC: 0.9100\n",
      "Epoch 00012: val_loss improved from 0.70891 to 0.67878, saving model to ./model\\12 - 0.6788.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5316 - ACC: 0.9205 - val_loss: 0.6788 - val_ACC: 0.9477\n",
      "Epoch 13/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5516 - ACC: 0.9360\n",
      "Epoch 00013: val_loss did not improve from 0.67878\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5231 - ACC: 0.9259 - val_loss: 0.6812 - val_ACC: 0.9446\n",
      "Epoch 14/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5059 - ACC: 0.9480\n",
      "Epoch 00014: val_loss improved from 0.67878 to 0.67014, saving model to ./model\\14 - 0.6701.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5157 - ACC: 0.9248 - val_loss: 0.6701 - val_ACC: 0.9500\n",
      "Epoch 15/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5240 - ACC: 0.9020\n",
      "Epoch 00015: val_loss improved from 0.67014 to 0.65130, saving model to ./model\\15 - 0.6513.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5073 - ACC: 0.9244 - val_loss: 0.6513 - val_ACC: 0.9531\n",
      "Epoch 16/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4974 - ACC: 0.9160\n",
      "Epoch 00016: val_loss improved from 0.65130 to 0.64587, saving model to ./model\\16 - 0.6459.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4978 - ACC: 0.9246 - val_loss: 0.6459 - val_ACC: 0.9508\n",
      "Epoch 17/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4682 - ACC: 0.9240\n",
      "Epoch 00017: val_loss improved from 0.64587 to 0.62377, saving model to ./model\\17 - 0.6238.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4865 - ACC: 0.9238 - val_loss: 0.6238 - val_ACC: 0.9538\n",
      "Epoch 18/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4523 - ACC: 0.9280\n",
      "Epoch 00018: val_loss improved from 0.62377 to 0.60072, saving model to ./model\\18 - 0.6007.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4723 - ACC: 0.9250 - val_loss: 0.6007 - val_ACC: 0.9562\n",
      "Epoch 19/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4718 - ACC: 0.9080\n",
      "Epoch 00019: val_loss improved from 0.60072 to 0.57801, saving model to ./model\\19 - 0.5780.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4547 - ACC: 0.9253 - val_loss: 0.5780 - val_ACC: 0.9531\n",
      "Epoch 20/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4293 - ACC: 0.9380\n",
      "Epoch 00020: val_loss improved from 0.57801 to 0.53265, saving model to ./model\\20 - 0.5327.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4329 - ACC: 0.9255 - val_loss: 0.5327 - val_ACC: 0.9577\n",
      "Epoch 21/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4361 - ACC: 0.9260\n",
      "Epoch 00021: val_loss improved from 0.53265 to 0.49951, saving model to ./model\\21 - 0.4995.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4066 - ACC: 0.9259 - val_loss: 0.4995 - val_ACC: 0.9554\n",
      "Epoch 22/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4083 - ACC: 0.9260\n",
      "Epoch 00022: val_loss improved from 0.49951 to 0.44503, saving model to ./model\\22 - 0.4450.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3761 - ACC: 0.9269 - val_loss: 0.4450 - val_ACC: 0.9615\n",
      "Epoch 23/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3249 - ACC: 0.9580\n",
      "Epoch 00023: val_loss improved from 0.44503 to 0.39446, saving model to ./model\\23 - 0.3945.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3429 - ACC: 0.9298 - val_loss: 0.3945 - val_ACC: 0.9623\n",
      "Epoch 24/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.3408 - ACC: 0.9060\n",
      "Epoch 00024: val_loss improved from 0.39446 to 0.33441, saving model to ./model\\24 - 0.3344.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3101 - ACC: 0.9292 - val_loss: 0.3344 - val_ACC: 0.9677\n",
      "Epoch 25/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2941 - ACC: 0.9240\n",
      "Epoch 00025: val_loss improved from 0.33441 to 0.28955, saving model to ./model\\25 - 0.2895.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2811 - ACC: 0.9290 - val_loss: 0.2895 - val_ACC: 0.9662\n",
      "Epoch 26/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2729 - ACC: 0.9260\n",
      "Epoch 00026: val_loss improved from 0.28955 to 0.24713, saving model to ./model\\26 - 0.2471.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2568 - ACC: 0.9302 - val_loss: 0.2471 - val_ACC: 0.9685\n",
      "Epoch 27/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2519 - ACC: 0.9300\n",
      "Epoch 00027: val_loss improved from 0.24713 to 0.21880, saving model to ./model\\27 - 0.2188.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2384 - ACC: 0.9309 - val_loss: 0.2188 - val_ACC: 0.9685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2114 - ACC: 0.9400\n",
      "Epoch 00028: val_loss improved from 0.21880 to 0.20060, saving model to ./model\\28 - 0.2006.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2247 - ACC: 0.9317 - val_loss: 0.2006 - val_ACC: 0.9654\n",
      "Epoch 29/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2271 - ACC: 0.9300\n",
      "Epoch 00029: val_loss improved from 0.20060 to 0.17977, saving model to ./model\\29 - 0.1798.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2145 - ACC: 0.9338 - val_loss: 0.1798 - val_ACC: 0.9700\n",
      "Epoch 30/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2103 - ACC: 0.9300\n",
      "Epoch 00030: val_loss improved from 0.17977 to 0.16715, saving model to ./model\\30 - 0.1671.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2063 - ACC: 0.9334 - val_loss: 0.1671 - val_ACC: 0.9723\n",
      "Epoch 31/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2101 - ACC: 0.9220\n",
      "Epoch 00031: val_loss improved from 0.16715 to 0.15773, saving model to ./model\\31 - 0.1577.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1989 - ACC: 0.9350 - val_loss: 0.1577 - val_ACC: 0.9715\n",
      "Epoch 32/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1933 - ACC: 0.9360\n",
      "Epoch 00032: val_loss did not improve from 0.15773\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1921 - ACC: 0.9340 - val_loss: 0.1644 - val_ACC: 0.9662\n",
      "Epoch 33/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1905 - ACC: 0.9340\n",
      "Epoch 00033: val_loss improved from 0.15773 to 0.15598, saving model to ./model\\33 - 0.1560.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1891 - ACC: 0.9352 - val_loss: 0.1560 - val_ACC: 0.9723\n",
      "Epoch 34/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1801 - ACC: 0.9380\n",
      "Epoch 00034: val_loss improved from 0.15598 to 0.12870, saving model to ./model\\34 - 0.1287.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1853 - ACC: 0.9359 - val_loss: 0.1287 - val_ACC: 0.9777\n",
      "Epoch 35/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.2020 - ACC: 0.9280\n",
      "Epoch 00035: val_loss did not improve from 0.12870\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1796 - ACC: 0.9382 - val_loss: 0.1380 - val_ACC: 0.9731\n",
      "Epoch 36/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1655 - ACC: 0.9520\n",
      "Epoch 00036: val_loss did not improve from 0.12870\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1773 - ACC: 0.9375 - val_loss: 0.1546 - val_ACC: 0.9715\n",
      "Epoch 37/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1671 - ACC: 0.9440\n",
      "Epoch 00037: val_loss did not improve from 0.12870\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1778 - ACC: 0.9365 - val_loss: 0.1696 - val_ACC: 0.9677\n",
      "Epoch 38/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1716 - ACC: 0.9420\n",
      "Epoch 00038: val_loss improved from 0.12870 to 0.12833, saving model to ./model\\38 - 0.1283.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1744 - ACC: 0.9375 - val_loss: 0.1283 - val_ACC: 0.9746\n",
      "Epoch 39/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1358 - ACC: 0.9560\n",
      "Epoch 00039: val_loss improved from 0.12833 to 0.09061, saving model to ./model\\39 - 0.0906.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1678 - ACC: 0.9404 - val_loss: 0.0906 - val_ACC: 0.9823\n",
      "Epoch 40/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1847 - ACC: 0.9360\n",
      "Epoch 00040: val_loss did not improve from 0.09061\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1666 - ACC: 0.9415 - val_loss: 0.0964 - val_ACC: 0.9792\n",
      "Epoch 41/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1462 - ACC: 0.9400\n",
      "Epoch 00041: val_loss did not improve from 0.09061\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1621 - ACC: 0.9411 - val_loss: 0.1050 - val_ACC: 0.9785\n",
      "Epoch 42/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1748 - ACC: 0.9320\n",
      "Epoch 00042: val_loss did not improve from 0.09061\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1589 - ACC: 0.9415 - val_loss: 0.0957 - val_ACC: 0.9792\n",
      "Epoch 43/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1857 - ACC: 0.9280\n",
      "Epoch 00043: val_loss improved from 0.09061 to 0.08986, saving model to ./model\\43 - 0.0899.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1555 - ACC: 0.9419 - val_loss: 0.0899 - val_ACC: 0.9831\n",
      "Epoch 44/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1606 - ACC: 0.9360\n",
      "Epoch 00044: val_loss did not improve from 0.08986\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1536 - ACC: 0.9425 - val_loss: 0.0985 - val_ACC: 0.9808\n",
      "Epoch 45/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1582 - ACC: 0.9340\n",
      "Epoch 00045: val_loss did not improve from 0.08986\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1501 - ACC: 0.9438 - val_loss: 0.0919 - val_ACC: 0.9823\n",
      "Epoch 46/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1480 - ACC: 0.9480\n",
      "Epoch 00046: val_loss did not improve from 0.08986\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1477 - ACC: 0.9436 - val_loss: 0.1040 - val_ACC: 0.9800\n",
      "Epoch 47/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1178 - ACC: 0.9640\n",
      "Epoch 00047: val_loss did not improve from 0.08986\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1467 - ACC: 0.9455 - val_loss: 0.0970 - val_ACC: 0.9808\n",
      "Epoch 48/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1775 - ACC: 0.9300\n",
      "Epoch 00048: val_loss improved from 0.08986 to 0.08634, saving model to ./model\\48 - 0.0863.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1443 - ACC: 0.9461 - val_loss: 0.0863 - val_ACC: 0.9831\n",
      "Epoch 49/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1286 - ACC: 0.9580\n",
      "Epoch 00049: val_loss improved from 0.08634 to 0.07517, saving model to ./model\\49 - 0.0752.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1437 - ACC: 0.9471 - val_loss: 0.0752 - val_ACC: 0.9854\n",
      "Epoch 50/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1394 - ACC: 0.9520\n",
      "Epoch 00050: val_loss improved from 0.07517 to 0.06745, saving model to ./model\\50 - 0.0675.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1415 - ACC: 0.9492 - val_loss: 0.0675 - val_ACC: 0.9877\n",
      "Epoch 51/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1580 - ACC: 0.9320\n",
      "Epoch 00051: val_loss improved from 0.06745 to 0.06429, saving model to ./model\\51 - 0.0643.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1402 - ACC: 0.9484 - val_loss: 0.0643 - val_ACC: 0.9900\n",
      "Epoch 52/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1823 - ACC: 0.9340\n",
      "Epoch 00052: val_loss improved from 0.06429 to 0.06137, saving model to ./model\\52 - 0.0614.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1390 - ACC: 0.9498 - val_loss: 0.0614 - val_ACC: 0.9900\n",
      "Epoch 53/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1221 - ACC: 0.9520\n",
      "Epoch 00053: val_loss did not improve from 0.06137\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1379 - ACC: 0.9479 - val_loss: 0.0739 - val_ACC: 0.9846\n",
      "Epoch 54/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1197 - ACC: 0.9640\n",
      "Epoch 00054: val_loss did not improve from 0.06137\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1332 - ACC: 0.9511 - val_loss: 0.0736 - val_ACC: 0.9838\n",
      "Epoch 55/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1528 - ACC: 0.9440\n",
      "Epoch 00055: val_loss did not improve from 0.06137\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1315 - ACC: 0.9511 - val_loss: 0.0794 - val_ACC: 0.9815\n",
      "Epoch 56/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1149 - ACC: 0.9640\n",
      "Epoch 00056: val_loss did not improve from 0.06137\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1318 - ACC: 0.9513 - val_loss: 0.0943 - val_ACC: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1332 - ACC: 0.9580\n",
      "Epoch 00057: val_loss did not improve from 0.06137\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1289 - ACC: 0.9563 - val_loss: 0.0774 - val_ACC: 0.9823\n",
      "Epoch 58/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1390 - ACC: 0.9560\n",
      "Epoch 00058: val_loss did not improve from 0.06137\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1268 - ACC: 0.9554 - val_loss: 0.0784 - val_ACC: 0.9823\n",
      "Epoch 59/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1491 - ACC: 0.9480\n",
      "Epoch 00059: val_loss improved from 0.06137 to 0.05929, saving model to ./model\\59 - 0.0593.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1241 - ACC: 0.9575 - val_loss: 0.0593 - val_ACC: 0.9908\n",
      "Epoch 60/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1148 - ACC: 0.9620\n",
      "Epoch 00060: val_loss improved from 0.05929 to 0.05570, saving model to ./model\\60 - 0.0557.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1259 - ACC: 0.9527 - val_loss: 0.0557 - val_ACC: 0.9900\n",
      "Epoch 61/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1179 - ACC: 0.9420\n",
      "Epoch 00061: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1224 - ACC: 0.9567 - val_loss: 0.0586 - val_ACC: 0.9900\n",
      "Epoch 62/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1148 - ACC: 0.9740\n",
      "Epoch 00062: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1195 - ACC: 0.9582 - val_loss: 0.0704 - val_ACC: 0.9831\n",
      "Epoch 63/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0979 - ACC: 0.9700\n",
      "Epoch 00063: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1183 - ACC: 0.9598 - val_loss: 0.0619 - val_ACC: 0.9885\n",
      "Epoch 64/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1415 - ACC: 0.9560\n",
      "Epoch 00064: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1168 - ACC: 0.9588 - val_loss: 0.0699 - val_ACC: 0.9831\n",
      "Epoch 65/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1074 - ACC: 0.9600\n",
      "Epoch 00065: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1156 - ACC: 0.9606 - val_loss: 0.0757 - val_ACC: 0.9815\n",
      "Epoch 66/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1372 - ACC: 0.9600\n",
      "Epoch 00066: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1152 - ACC: 0.9615 - val_loss: 0.0603 - val_ACC: 0.9869\n",
      "Epoch 67/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1102 - ACC: 0.9640\n",
      "Epoch 00067: val_loss did not improve from 0.05570\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1122 - ACC: 0.9627 - val_loss: 0.0596 - val_ACC: 0.9869\n",
      "Epoch 68/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0930 - ACC: 0.9660\n",
      "Epoch 00068: val_loss improved from 0.05570 to 0.05132, saving model to ./model\\68 - 0.0513.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1115 - ACC: 0.9623 - val_loss: 0.0513 - val_ACC: 0.9915\n",
      "Epoch 69/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1146 - ACC: 0.9560\n",
      "Epoch 00069: val_loss improved from 0.05132 to 0.04400, saving model to ./model\\69 - 0.0440.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1104 - ACC: 0.9607 - val_loss: 0.0440 - val_ACC: 0.9923\n",
      "Epoch 70/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1521 - ACC: 0.9440\n",
      "Epoch 00070: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1085 - ACC: 0.9646 - val_loss: 0.0664 - val_ACC: 0.9838\n",
      "Epoch 71/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1086 - ACC: 0.9560\n",
      "Epoch 00071: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1099 - ACC: 0.9627 - val_loss: 0.0510 - val_ACC: 0.9908\n",
      "Epoch 72/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0929 - ACC: 0.9700\n",
      "Epoch 00072: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1107 - ACC: 0.9607 - val_loss: 0.0469 - val_ACC: 0.9915\n",
      "Epoch 73/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1092 - ACC: 0.9600\n",
      "Epoch 00073: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1062 - ACC: 0.9654 - val_loss: 0.0512 - val_ACC: 0.9908\n",
      "Epoch 74/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1141 - ACC: 0.9660\n",
      "Epoch 00074: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1037 - ACC: 0.9652 - val_loss: 0.0551 - val_ACC: 0.9877\n",
      "Epoch 75/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1026 - ACC: 0.9720\n",
      "Epoch 00075: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1019 - ACC: 0.9669 - val_loss: 0.0486 - val_ACC: 0.9908\n",
      "Epoch 76/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0920 - ACC: 0.9680\n",
      "Epoch 00076: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1014 - ACC: 0.9659 - val_loss: 0.0529 - val_ACC: 0.9885\n",
      "Epoch 77/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0985 - ACC: 0.9700\n",
      "Epoch 00077: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0993 - ACC: 0.9677 - val_loss: 0.0577 - val_ACC: 0.9862\n",
      "Epoch 78/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0708 - ACC: 0.9840\n",
      "Epoch 00078: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0990 - ACC: 0.9694 - val_loss: 0.0714 - val_ACC: 0.9823\n",
      "Epoch 79/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1043 - ACC: 0.9700\n",
      "Epoch 00079: val_loss did not improve from 0.04400\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0999 - ACC: 0.9690 - val_loss: 0.0462 - val_ACC: 0.9908\n",
      "Epoch 80/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0789 - ACC: 0.9760\n",
      "Epoch 00080: val_loss improved from 0.04400 to 0.04370, saving model to ./model\\80 - 0.0437.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0986 - ACC: 0.9679 - val_loss: 0.0437 - val_ACC: 0.9908\n",
      "Epoch 81/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0976 - ACC: 0.9660\n",
      "Epoch 00081: val_loss did not improve from 0.04370\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0968 - ACC: 0.9694 - val_loss: 0.0669 - val_ACC: 0.9823\n",
      "Epoch 82/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0814 - ACC: 0.9680\n",
      "Epoch 00082: val_loss improved from 0.04370 to 0.04295, saving model to ./model\\82 - 0.0430.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0950 - ACC: 0.9690 - val_loss: 0.0430 - val_ACC: 0.9915\n",
      "Epoch 83/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0978 - ACC: 0.9660\n",
      "Epoch 00083: val_loss improved from 0.04295 to 0.04055, saving model to ./model\\83 - 0.0405.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0948 - ACC: 0.9690 - val_loss: 0.0405 - val_ACC: 0.9915\n",
      "Epoch 84/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0746 - ACC: 0.9700\n",
      "Epoch 00084: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0936 - ACC: 0.9688 - val_loss: 0.0679 - val_ACC: 0.9800\n",
      "Epoch 85/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0759 - ACC: 0.9800\n",
      "Epoch 00085: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0930 - ACC: 0.9690 - val_loss: 0.0539 - val_ACC: 0.9831\n",
      "Epoch 86/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0572 - ACC: 0.9900\n",
      "Epoch 00086: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0900 - ACC: 0.9717 - val_loss: 0.0537 - val_ACC: 0.9831\n",
      "Epoch 87/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0943 - ACC: 0.9780\n",
      "Epoch 00087: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0892 - ACC: 0.9715 - val_loss: 0.0473 - val_ACC: 0.9900\n",
      "Epoch 88/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0941 - ACC: 0.9720\n",
      "Epoch 00088: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0863 - ACC: 0.9734 - val_loss: 0.0629 - val_ACC: 0.9792\n",
      "Epoch 89/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0943 - ACC: 0.9740\n",
      "Epoch 00089: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0860 - ACC: 0.9742 - val_loss: 0.0631 - val_ACC: 0.9777\n",
      "Epoch 90/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1192 - ACC: 0.9660\n",
      "Epoch 00090: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0860 - ACC: 0.9717 - val_loss: 0.0515 - val_ACC: 0.9862\n",
      "Epoch 91/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0812 - ACC: 0.9760\n",
      "Epoch 00091: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0853 - ACC: 0.9734 - val_loss: 0.0481 - val_ACC: 0.9892\n",
      "Epoch 92/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1432 - ACC: 0.9560\n",
      "Epoch 00092: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0862 - ACC: 0.9715 - val_loss: 0.0690 - val_ACC: 0.9738\n",
      "Epoch 93/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0866 - ACC: 0.9720\n",
      "Epoch 00093: val_loss did not improve from 0.04055\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0841 - ACC: 0.9733 - val_loss: 0.0442 - val_ACC: 0.9923\n",
      "Epoch 94/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0864 - ACC: 0.9740\n",
      "Epoch 00094: val_loss improved from 0.04055 to 0.03080, saving model to ./model\\94 - 0.0308.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0856 - ACC: 0.9731 - val_loss: 0.0308 - val_ACC: 0.9962\n",
      "Epoch 95/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1083 - ACC: 0.9620\n",
      "Epoch 00095: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0830 - ACC: 0.9733 - val_loss: 0.0525 - val_ACC: 0.9838\n",
      "Epoch 96/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0891 - ACC: 0.9760\n",
      "Epoch 00096: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0793 - ACC: 0.9758 - val_loss: 0.0754 - val_ACC: 0.9715\n",
      "Epoch 97/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0931 - ACC: 0.9700\n",
      "Epoch 00097: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0840 - ACC: 0.9748 - val_loss: 0.0421 - val_ACC: 0.9923\n",
      "Epoch 98/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0600 - ACC: 0.9820\n",
      "Epoch 00098: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0791 - ACC: 0.9759 - val_loss: 0.0432 - val_ACC: 0.9923\n",
      "Epoch 99/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0828 - ACC: 0.9760\n",
      "Epoch 00099: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0791 - ACC: 0.9758 - val_loss: 0.0372 - val_ACC: 0.9938\n",
      "Epoch 100/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0938 - ACC: 0.9600\n",
      "Epoch 00100: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0825 - ACC: 0.9742 - val_loss: 0.0661 - val_ACC: 0.9754\n",
      "Epoch 101/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0765 - ACC: 0.9760\n",
      "Epoch 00101: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0767 - ACC: 0.9765 - val_loss: 0.0572 - val_ACC: 0.9777\n",
      "Epoch 102/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0865 - ACC: 0.9800\n",
      "Epoch 00102: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0760 - ACC: 0.9761 - val_loss: 0.0474 - val_ACC: 0.9892\n",
      "Epoch 103/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0680 - ACC: 0.9880\n",
      "Epoch 00103: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0759 - ACC: 0.9781 - val_loss: 0.0471 - val_ACC: 0.9885\n",
      "Epoch 104/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0506 - ACC: 0.9820\n",
      "Epoch 00104: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0739 - ACC: 0.9790 - val_loss: 0.0389 - val_ACC: 0.9923\n",
      "Epoch 105/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0930 - ACC: 0.9720\n",
      "Epoch 00105: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0753 - ACC: 0.9775 - val_loss: 0.0422 - val_ACC: 0.9908\n",
      "Epoch 106/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0724 - ACC: 0.9780\n",
      "Epoch 00106: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0743 - ACC: 0.9788 - val_loss: 0.0614 - val_ACC: 0.9769\n",
      "Epoch 107/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0464 - ACC: 0.9960\n",
      "Epoch 00107: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0738 - ACC: 0.9779 - val_loss: 0.0568 - val_ACC: 0.9823\n",
      "Epoch 108/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0808 - ACC: 0.9840\n",
      "Epoch 00108: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0720 - ACC: 0.9792 - val_loss: 0.0429 - val_ACC: 0.9915\n",
      "Epoch 109/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0413 - ACC: 0.9900\n",
      "Epoch 00109: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0715 - ACC: 0.9806 - val_loss: 0.0401 - val_ACC: 0.9923\n",
      "Epoch 110/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0569 - ACC: 0.9860\n",
      "Epoch 00110: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0727 - ACC: 0.9781 - val_loss: 0.0390 - val_ACC: 0.9923\n",
      "Epoch 111/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0739 - ACC: 0.9780\n",
      "Epoch 00111: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0732 - ACC: 0.9777 - val_loss: 0.0460 - val_ACC: 0.9877\n",
      "Epoch 112/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0615 - ACC: 0.9780\n",
      "Epoch 00112: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0739 - ACC: 0.9775 - val_loss: 0.0898 - val_ACC: 0.9700\n",
      "Epoch 113/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0918 - ACC: 0.9680\n",
      "Epoch 00113: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0792 - ACC: 0.9744 - val_loss: 0.0548 - val_ACC: 0.9846\n",
      "Epoch 114/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0669 - ACC: 0.9800\n",
      "Epoch 00114: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0739 - ACC: 0.9784 - val_loss: 0.0361 - val_ACC: 0.9931\n",
      "Epoch 115/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0686 - ACC: 0.9700\n",
      "Epoch 00115: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0702 - ACC: 0.9779 - val_loss: 0.0412 - val_ACC: 0.9915\n",
      "Epoch 116/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0702 - ACC: 0.9760\n",
      "Epoch 00116: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0692 - ACC: 0.9806 - val_loss: 0.0515 - val_ACC: 0.9854\n",
      "Epoch 117/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0509 - ACC: 0.9940\n",
      "Epoch 00117: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0697 - ACC: 0.9792 - val_loss: 0.0427 - val_ACC: 0.9915\n",
      "Epoch 118/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0569 - ACC: 0.9780\n",
      "Epoch 00118: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0684 - ACC: 0.9810 - val_loss: 0.0487 - val_ACC: 0.9862\n",
      "Epoch 119/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0676 - ACC: 0.9800\n",
      "Epoch 00119: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0678 - ACC: 0.9813 - val_loss: 0.0563 - val_ACC: 0.9815\n",
      "Epoch 120/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0553 - ACC: 0.9880\n",
      "Epoch 00120: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0681 - ACC: 0.9806 - val_loss: 0.0366 - val_ACC: 0.9923\n",
      "Epoch 121/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0899 - ACC: 0.9820\n",
      "Epoch 00121: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0684 - ACC: 0.9796 - val_loss: 0.0314 - val_ACC: 0.9946\n",
      "Epoch 122/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0938 - ACC: 0.9580\n",
      "Epoch 00122: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0701 - ACC: 0.9788 - val_loss: 0.0564 - val_ACC: 0.9815\n",
      "Epoch 123/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0686 - ACC: 0.9780\n",
      "Epoch 00123: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0681 - ACC: 0.9804 - val_loss: 0.0581 - val_ACC: 0.9808\n",
      "Epoch 124/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0510 - ACC: 0.9900\n",
      "Epoch 00124: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0671 - ACC: 0.9802 - val_loss: 0.0566 - val_ACC: 0.9808\n",
      "Epoch 125/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0578 - ACC: 0.9860\n",
      "Epoch 00125: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0680 - ACC: 0.9808 - val_loss: 0.0528 - val_ACC: 0.9831\n",
      "Epoch 126/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0917 - ACC: 0.9760\n",
      "Epoch 00126: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0670 - ACC: 0.9813 - val_loss: 0.0471 - val_ACC: 0.9846\n",
      "Epoch 127/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0650 - ACC: 0.9880\n",
      "Epoch 00127: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0651 - ACC: 0.9813 - val_loss: 0.0425 - val_ACC: 0.9869\n",
      "Epoch 128/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0750 - ACC: 0.9800\n",
      "Epoch 00128: val_loss did not improve from 0.03080\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0655 - ACC: 0.9810 - val_loss: 0.0460 - val_ACC: 0.9862\n",
      "Epoch 129/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0686 - ACC: 0.9800\n",
      "Epoch 00129: val_loss improved from 0.03080 to 0.02831, saving model to ./model\\129 - 0.0283.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0661 - ACC: 0.9810 - val_loss: 0.0283 - val_ACC: 0.9946\n",
      "Epoch 130/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0821 - ACC: 0.9800\n",
      "Epoch 00130: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0692 - ACC: 0.9802 - val_loss: 0.0383 - val_ACC: 0.9900\n",
      "Epoch 131/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0615 - ACC: 0.9820\n",
      "Epoch 00131: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0647 - ACC: 0.9821 - val_loss: 0.0426 - val_ACC: 0.9877\n",
      "Epoch 132/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0726 - ACC: 0.9820\n",
      "Epoch 00132: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0641 - ACC: 0.9815 - val_loss: 0.0369 - val_ACC: 0.9923\n",
      "Epoch 133/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0752 - ACC: 0.9860\n",
      "Epoch 00133: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0634 - ACC: 0.9817 - val_loss: 0.0576 - val_ACC: 0.9800\n",
      "Epoch 134/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0638 - ACC: 0.9800\n",
      "Epoch 00134: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0640 - ACC: 0.9827 - val_loss: 0.0530 - val_ACC: 0.9808\n",
      "Epoch 135/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0574 - ACC: 0.9880\n",
      "Epoch 00135: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0642 - ACC: 0.9827 - val_loss: 0.0290 - val_ACC: 0.9946\n",
      "Epoch 136/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0586 - ACC: 0.9760\n",
      "Epoch 00136: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0656 - ACC: 0.9790 - val_loss: 0.0445 - val_ACC: 0.9862\n",
      "Epoch 137/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0496 - ACC: 0.9900\n",
      "Epoch 00137: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0640 - ACC: 0.9823 - val_loss: 0.0426 - val_ACC: 0.9869\n",
      "Epoch 138/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0725 - ACC: 0.9740\n",
      "Epoch 00138: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0636 - ACC: 0.9819 - val_loss: 0.0367 - val_ACC: 0.9915\n",
      "Epoch 139/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0607 - ACC: 0.9820\n",
      "Epoch 00139: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0659 - ACC: 0.9798 - val_loss: 0.0402 - val_ACC: 0.9885\n",
      "Epoch 140/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0420 - ACC: 0.9900\n",
      "Epoch 00140: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0636 - ACC: 0.9806 - val_loss: 0.0479 - val_ACC: 0.9846\n",
      "Epoch 141/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0404 - ACC: 0.9920\n",
      "Epoch 00141: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0641 - ACC: 0.9808 - val_loss: 0.0355 - val_ACC: 0.9923\n",
      "Epoch 142/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0656 - ACC: 0.9780\n",
      "Epoch 00142: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0634 - ACC: 0.9813 - val_loss: 0.0388 - val_ACC: 0.9908\n",
      "Epoch 143/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0545 - ACC: 0.9800\n",
      "Epoch 00143: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0659 - ACC: 0.9788 - val_loss: 0.0537 - val_ACC: 0.9823\n",
      "Epoch 144/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0376 - ACC: 0.9880\n",
      "Epoch 00144: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0630 - ACC: 0.9813 - val_loss: 0.0409 - val_ACC: 0.9885\n",
      "Epoch 145/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0488 - ACC: 0.9840\n",
      "Epoch 00145: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0622 - ACC: 0.9835 - val_loss: 0.0469 - val_ACC: 0.9854\n",
      "Epoch 146/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0457 - ACC: 0.9880\n",
      "Epoch 00146: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0624 - ACC: 0.9821 - val_loss: 0.0383 - val_ACC: 0.9900\n",
      "Epoch 147/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0367 - ACC: 0.9920\n",
      "Epoch 00147: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0614 - ACC: 0.9815 - val_loss: 0.0359 - val_ACC: 0.9908\n",
      "Epoch 148/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0670 - ACC: 0.9760\n",
      "Epoch 00148: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0622 - ACC: 0.9794 - val_loss: 0.0481 - val_ACC: 0.9846\n",
      "Epoch 149/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0308 - ACC: 0.9940\n",
      "Epoch 00149: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0614 - ACC: 0.9825 - val_loss: 0.0508 - val_ACC: 0.9823\n",
      "Epoch 150/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0452 - ACC: 0.9860\n",
      "Epoch 00150: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0615 - ACC: 0.9827 - val_loss: 0.0510 - val_ACC: 0.9831\n",
      "Epoch 151/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0544 - ACC: 0.9860\n",
      "Epoch 00151: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0605 - ACC: 0.9823 - val_loss: 0.0412 - val_ACC: 0.9885\n",
      "Epoch 152/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0582 - ACC: 0.9780\n",
      "Epoch 00152: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0601 - ACC: 0.9823 - val_loss: 0.0568 - val_ACC: 0.9808\n",
      "Epoch 153/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0608 - ACC: 0.9860\n",
      "Epoch 00153: val_loss did not improve from 0.02831\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0650 - ACC: 0.9800 - val_loss: 0.0422 - val_ACC: 0.9877\n",
      "Epoch 154/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0414 - ACC: 0.9840\n",
      "Epoch 00154: val_loss improved from 0.02831 to 0.02087, saving model to ./model\\154 - 0.0209.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0671 - ACC: 0.9784 - val_loss: 0.0209 - val_ACC: 0.9969\n",
      "Epoch 155/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0825 - ACC: 0.9740\n",
      "Epoch 00155: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0709 - ACC: 0.9761 - val_loss: 0.0380 - val_ACC: 0.9900\n",
      "Epoch 156/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0598 - ACC: 0.9820\n",
      "Epoch 00156: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0617 - ACC: 0.9813 - val_loss: 0.0676 - val_ACC: 0.9785\n",
      "Epoch 157/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0504 - ACC: 0.9800\n",
      "Epoch 00157: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0625 - ACC: 0.9810 - val_loss: 0.0696 - val_ACC: 0.9746\n",
      "Epoch 158/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0800 - ACC: 0.9760\n",
      "Epoch 00158: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0611 - ACC: 0.9821 - val_loss: 0.0462 - val_ACC: 0.9869\n",
      "Epoch 159/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0469 - ACC: 0.9880\n",
      "Epoch 00159: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0601 - ACC: 0.9823 - val_loss: 0.0249 - val_ACC: 0.9954\n",
      "Epoch 160/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0538 - ACC: 0.9820\n",
      "Epoch 00160: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0596 - ACC: 0.9813 - val_loss: 0.0338 - val_ACC: 0.9908\n",
      "Epoch 161/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0552 - ACC: 0.9860\n",
      "Epoch 00161: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0599 - ACC: 0.9823 - val_loss: 0.0420 - val_ACC: 0.9885\n",
      "Epoch 162/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0472 - ACC: 0.9840\n",
      "Epoch 00162: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0611 - ACC: 0.9802 - val_loss: 0.0429 - val_ACC: 0.9892\n",
      "Epoch 163/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0518 - ACC: 0.9860\n",
      "Epoch 00163: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0583 - ACC: 0.9829 - val_loss: 0.0432 - val_ACC: 0.9892\n",
      "Epoch 164/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0673 - ACC: 0.9740\n",
      "Epoch 00164: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0627 - ACC: 0.9802 - val_loss: 0.0728 - val_ACC: 0.9746\n",
      "Epoch 165/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1094 - ACC: 0.9740\n",
      "Epoch 00165: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0653 - ACC: 0.9804 - val_loss: 0.0365 - val_ACC: 0.9892\n",
      "Epoch 166/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.1112 - ACC: 0.9700\n",
      "Epoch 00166: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0603 - ACC: 0.9821 - val_loss: 0.0306 - val_ACC: 0.9915\n",
      "Epoch 167/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0708 - ACC: 0.9840\n",
      "Epoch 00167: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0589 - ACC: 0.9823 - val_loss: 0.0446 - val_ACC: 0.9885\n",
      "Epoch 168/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0531 - ACC: 0.9920\n",
      "Epoch 00168: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0587 - ACC: 0.9823 - val_loss: 0.0573 - val_ACC: 0.9785\n",
      "Epoch 169/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0748 - ACC: 0.9780\n",
      "Epoch 00169: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0570 - ACC: 0.9833 - val_loss: 0.0458 - val_ACC: 0.9854\n",
      "Epoch 170/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0734 - ACC: 0.9760\n",
      "Epoch 00170: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0583 - ACC: 0.9823 - val_loss: 0.0324 - val_ACC: 0.9900\n",
      "Epoch 171/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0769 - ACC: 0.9760\n",
      "Epoch 00171: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0584 - ACC: 0.9815 - val_loss: 0.0264 - val_ACC: 0.9946\n",
      "Epoch 172/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0397 - ACC: 0.9820\n",
      "Epoch 00172: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0576 - ACC: 0.9802 - val_loss: 0.0316 - val_ACC: 0.9915\n",
      "Epoch 173/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0432 - ACC: 0.9860\n",
      "Epoch 00173: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0572 - ACC: 0.9825 - val_loss: 0.0388 - val_ACC: 0.9892\n",
      "Epoch 174/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0434 - ACC: 0.9800\n",
      "Epoch 00174: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0559 - ACC: 0.9821 - val_loss: 0.0560 - val_ACC: 0.9808\n",
      "Epoch 175/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0579 - ACC: 0.9860\n",
      "Epoch 00175: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0567 - ACC: 0.9842 - val_loss: 0.0575 - val_ACC: 0.9831\n",
      "Epoch 176/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0394 - ACC: 0.9900\n",
      "Epoch 00176: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0589 - ACC: 0.9827 - val_loss: 0.0489 - val_ACC: 0.9846\n",
      "Epoch 177/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0704 - ACC: 0.9800\n",
      "Epoch 00177: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0559 - ACC: 0.9831 - val_loss: 0.0396 - val_ACC: 0.9892\n",
      "Epoch 178/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0332 - ACC: 0.9920\n",
      "Epoch 00178: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0552 - ACC: 0.9838 - val_loss: 0.0452 - val_ACC: 0.9885\n",
      "Epoch 179/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0516 - ACC: 0.9880\n",
      "Epoch 00179: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0560 - ACC: 0.9836 - val_loss: 0.0413 - val_ACC: 0.9885\n",
      "Epoch 180/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0545 - ACC: 0.9800\n",
      "Epoch 00180: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0557 - ACC: 0.9825 - val_loss: 0.0343 - val_ACC: 0.9892\n",
      "Epoch 181/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0360 - ACC: 0.9880\n",
      "Epoch 00181: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0549 - ACC: 0.9840 - val_loss: 0.0393 - val_ACC: 0.9892\n",
      "Epoch 182/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0649 - ACC: 0.9700\n",
      "Epoch 00182: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0547 - ACC: 0.9846 - val_loss: 0.0437 - val_ACC: 0.9885\n",
      "Epoch 183/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0508 - ACC: 0.9840\n",
      "Epoch 00183: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0584 - ACC: 0.9819 - val_loss: 0.0242 - val_ACC: 0.9938\n",
      "Epoch 184/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0643 - ACC: 0.9760\n",
      "Epoch 00184: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0562 - ACC: 0.9825 - val_loss: 0.0329 - val_ACC: 0.9900\n",
      "Epoch 185/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0245 - ACC: 0.9900\n",
      "Epoch 00185: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0549 - ACC: 0.9838 - val_loss: 0.0398 - val_ACC: 0.9892\n",
      "Epoch 186/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0382 - ACC: 0.9920\n",
      "Epoch 00186: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0557 - ACC: 0.9838 - val_loss: 0.0282 - val_ACC: 0.9915\n",
      "Epoch 187/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0724 - ACC: 0.9800\n",
      "Epoch 00187: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0563 - ACC: 0.9831 - val_loss: 0.0329 - val_ACC: 0.9900\n",
      "Epoch 188/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0363 - ACC: 0.9900\n",
      "Epoch 00188: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0541 - ACC: 0.9836 - val_loss: 0.0397 - val_ACC: 0.9885\n",
      "Epoch 189/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0441 - ACC: 0.9820\n",
      "Epoch 00189: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0551 - ACC: 0.9838 - val_loss: 0.0238 - val_ACC: 0.9931\n",
      "Epoch 190/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0781 - ACC: 0.9800\n",
      "Epoch 00190: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0567 - ACC: 0.9817 - val_loss: 0.0336 - val_ACC: 0.9900\n",
      "Epoch 191/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0292 - ACC: 0.9900\n",
      "Epoch 00191: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0584 - ACC: 0.9821 - val_loss: 0.0535 - val_ACC: 0.9831\n",
      "Epoch 192/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0573 - ACC: 0.9860\n",
      "Epoch 00192: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0562 - ACC: 0.9823 - val_loss: 0.0324 - val_ACC: 0.9908\n",
      "Epoch 193/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0551 - ACC: 0.9920\n",
      "Epoch 00193: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0547 - ACC: 0.9840 - val_loss: 0.0430 - val_ACC: 0.9885\n",
      "Epoch 194/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0431 - ACC: 0.9860\n",
      "Epoch 00194: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0535 - ACC: 0.9848 - val_loss: 0.0443 - val_ACC: 0.9885\n",
      "Epoch 195/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0481 - ACC: 0.9860\n",
      "Epoch 00195: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0541 - ACC: 0.9840 - val_loss: 0.0405 - val_ACC: 0.9885\n",
      "Epoch 196/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0536 - ACC: 0.9820\n",
      "Epoch 00196: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0562 - ACC: 0.9833 - val_loss: 0.0654 - val_ACC: 0.9769\n",
      "Epoch 197/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0928 - ACC: 0.9800\n",
      "Epoch 00197: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0593 - ACC: 0.9813 - val_loss: 0.0310 - val_ACC: 0.9908\n",
      "Epoch 198/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0582 - ACC: 0.9860\n",
      "Epoch 00198: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0537 - ACC: 0.9836 - val_loss: 0.0334 - val_ACC: 0.9900\n",
      "Epoch 199/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0517 - ACC: 0.9840\n",
      "Epoch 00199: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0530 - ACC: 0.9848 - val_loss: 0.0282 - val_ACC: 0.9915\n",
      "Epoch 200/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0757 - ACC: 0.9760\n",
      "Epoch 00200: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0548 - ACC: 0.9827 - val_loss: 0.0283 - val_ACC: 0.9915\n",
      "Epoch 201/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0392 - ACC: 0.9860\n",
      "Epoch 00201: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0538 - ACC: 0.9833 - val_loss: 0.0507 - val_ACC: 0.9846\n",
      "Epoch 202/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0429 - ACC: 0.9840\n",
      "Epoch 00202: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0532 - ACC: 0.9848 - val_loss: 0.0316 - val_ACC: 0.9900\n",
      "Epoch 203/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0446 - ACC: 0.9860\n",
      "Epoch 00203: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0535 - ACC: 0.9842 - val_loss: 0.0289 - val_ACC: 0.9915\n",
      "Epoch 204/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0544 - ACC: 0.9800\n",
      "Epoch 00204: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0527 - ACC: 0.9840 - val_loss: 0.0468 - val_ACC: 0.9877\n",
      "Epoch 205/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0519 - ACC: 0.9900\n",
      "Epoch 00205: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0536 - ACC: 0.9856 - val_loss: 0.0226 - val_ACC: 0.9931\n",
      "Epoch 206/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0470 - ACC: 0.9840\n",
      "Epoch 00206: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0562 - ACC: 0.9825 - val_loss: 0.0409 - val_ACC: 0.9892\n",
      "Epoch 207/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0524 - ACC: 0.9880\n",
      "Epoch 00207: val_loss did not improve from 0.02087\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0585 - ACC: 0.9817 - val_loss: 0.0284 - val_ACC: 0.9915\n",
      "Epoch 208/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0377 - ACC: 0.9860\n",
      "Epoch 00208: val_loss improved from 0.02087 to 0.01627, saving model to ./model\\208 - 0.0163.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0609 - ACC: 0.9827 - val_loss: 0.0163 - val_ACC: 0.9977\n",
      "Epoch 209/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0554 - ACC: 0.9800\n",
      "Epoch 00209: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0609 - ACC: 0.9808 - val_loss: 0.0429 - val_ACC: 0.9885\n",
      "Epoch 210/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0757 - ACC: 0.9820\n",
      "Epoch 00210: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0534 - ACC: 0.9846 - val_loss: 0.0425 - val_ACC: 0.9877\n",
      "Epoch 211/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0544 - ACC: 0.9820\n",
      "Epoch 00211: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0517 - ACC: 0.9850 - val_loss: 0.0348 - val_ACC: 0.9892\n",
      "Epoch 212/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0625 - ACC: 0.9820\n",
      "Epoch 00212: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0530 - ACC: 0.9850 - val_loss: 0.0258 - val_ACC: 0.9923\n",
      "Epoch 213/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0555 - ACC: 0.9840\n",
      "Epoch 00213: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0517 - ACC: 0.9861 - val_loss: 0.0186 - val_ACC: 0.9969\n",
      "Epoch 214/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0495 - ACC: 0.9800\n",
      "Epoch 00214: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0542 - ACC: 0.9836 - val_loss: 0.0490 - val_ACC: 0.9831\n",
      "Epoch 215/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0819 - ACC: 0.9780\n",
      "Epoch 00215: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0525 - ACC: 0.9840 - val_loss: 0.0282 - val_ACC: 0.9915\n",
      "Epoch 216/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0617 - ACC: 0.9820\n",
      "Epoch 00216: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0526 - ACC: 0.9840 - val_loss: 0.0381 - val_ACC: 0.9885\n",
      "Epoch 217/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0450 - ACC: 0.9880\n",
      "Epoch 00217: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0519 - ACC: 0.9852 - val_loss: 0.0499 - val_ACC: 0.9854\n",
      "Epoch 218/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0760 - ACC: 0.9840\n",
      "Epoch 00218: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0525 - ACC: 0.9848 - val_loss: 0.0294 - val_ACC: 0.9908\n",
      "Epoch 219/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0846 - ACC: 0.9780\n",
      "Epoch 00219: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0523 - ACC: 0.9844 - val_loss: 0.0409 - val_ACC: 0.9885\n",
      "Epoch 220/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0422 - ACC: 0.9900\n",
      "Epoch 00220: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0521 - ACC: 0.9858 - val_loss: 0.0468 - val_ACC: 0.9877\n",
      "Epoch 221/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0631 - ACC: 0.9860\n",
      "Epoch 00221: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0543 - ACC: 0.9838 - val_loss: 0.0311 - val_ACC: 0.9908\n",
      "Epoch 222/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0393 - ACC: 0.9940\n",
      "Epoch 00222: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0514 - ACC: 0.9854 - val_loss: 0.0436 - val_ACC: 0.9885\n",
      "Epoch 223/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0548 - ACC: 0.9800\n",
      "Epoch 00223: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0545 - ACC: 0.9833 - val_loss: 0.0581 - val_ACC: 0.9808\n",
      "Epoch 224/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0531 - ACC: 0.9840\n",
      "Epoch 00224: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0561 - ACC: 0.9827 - val_loss: 0.0178 - val_ACC: 0.9954\n",
      "Epoch 225/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0582 - ACC: 0.9820\n",
      "Epoch 00225: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0692 - ACC: 0.9767 - val_loss: 0.0169 - val_ACC: 0.9977\n",
      "Epoch 226/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0629 - ACC: 0.9780\n",
      "Epoch 00226: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0577 - ACC: 0.9833 - val_loss: 0.0375 - val_ACC: 0.9892\n",
      "Epoch 227/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0469 - ACC: 0.9880\n",
      "Epoch 00227: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0515 - ACC: 0.9854 - val_loss: 0.0246 - val_ACC: 0.9923\n",
      "Epoch 228/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0742 - ACC: 0.9780\n",
      "Epoch 00228: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0533 - ACC: 0.9850 - val_loss: 0.0279 - val_ACC: 0.9915\n",
      "Epoch 229/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0551 - ACC: 0.9840\n",
      "Epoch 00229: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0506 - ACC: 0.9854 - val_loss: 0.0228 - val_ACC: 0.9931\n",
      "Epoch 230/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0935 - ACC: 0.9740\n",
      "Epoch 00230: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0508 - ACC: 0.9858 - val_loss: 0.0307 - val_ACC: 0.9900\n",
      "Epoch 231/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0495 - ACC: 0.9860\n",
      "Epoch 00231: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0500 - ACC: 0.9856 - val_loss: 0.0297 - val_ACC: 0.9900\n",
      "Epoch 232/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0859 - ACC: 0.9780\n",
      "Epoch 00232: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0524 - ACC: 0.9844 - val_loss: 0.0283 - val_ACC: 0.9908\n",
      "Epoch 233/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0540 - ACC: 0.9760\n",
      "Epoch 00233: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0515 - ACC: 0.9858 - val_loss: 0.0473 - val_ACC: 0.9877\n",
      "Epoch 234/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0409 - ACC: 0.9920\n",
      "Epoch 00234: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0519 - ACC: 0.9856 - val_loss: 0.0364 - val_ACC: 0.9892\n",
      "Epoch 235/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0414 - ACC: 0.9820\n",
      "Epoch 00235: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0500 - ACC: 0.9854 - val_loss: 0.0318 - val_ACC: 0.9900\n",
      "Epoch 236/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0541 - ACC: 0.9840\n",
      "Epoch 00236: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0500 - ACC: 0.9852 - val_loss: 0.0305 - val_ACC: 0.9908\n",
      "Epoch 237/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0582 - ACC: 0.9880\n",
      "Epoch 00237: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0501 - ACC: 0.9865 - val_loss: 0.0568 - val_ACC: 0.9815\n",
      "Epoch 238/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0495 - ACC: 0.9920\n",
      "Epoch 00238: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0525 - ACC: 0.9850 - val_loss: 0.0726 - val_ACC: 0.9723\n",
      "Epoch 239/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0834 - ACC: 0.9700\n",
      "Epoch 00239: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0535 - ACC: 0.9838 - val_loss: 0.0309 - val_ACC: 0.9900\n",
      "Epoch 240/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0236 - ACC: 0.9940\n",
      "Epoch 00240: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0499 - ACC: 0.9848 - val_loss: 0.0351 - val_ACC: 0.9892\n",
      "Epoch 241/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0438 - ACC: 0.9840\n",
      "Epoch 00241: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0531 - ACC: 0.9840 - val_loss: 0.0384 - val_ACC: 0.9892\n",
      "Epoch 242/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0343 - ACC: 0.9860\n",
      "Epoch 00242: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0499 - ACC: 0.9856 - val_loss: 0.0344 - val_ACC: 0.9900\n",
      "Epoch 243/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0416 - ACC: 0.9920\n",
      "Epoch 00243: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0495 - ACC: 0.9850 - val_loss: 0.0291 - val_ACC: 0.9908\n",
      "Epoch 244/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0457 - ACC: 0.9820\n",
      "Epoch 00244: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0522 - ACC: 0.9854 - val_loss: 0.0211 - val_ACC: 0.9938\n",
      "Epoch 245/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0526 - ACC: 0.9860\n",
      "Epoch 00245: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0511 - ACC: 0.9865 - val_loss: 0.0308 - val_ACC: 0.9908\n",
      "Epoch 246/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0376 - ACC: 0.9900\n",
      "Epoch 00246: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0504 - ACC: 0.9856 - val_loss: 0.0478 - val_ACC: 0.9862\n",
      "Epoch 247/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0486 - ACC: 0.9920\n",
      "Epoch 00247: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0503 - ACC: 0.9856 - val_loss: 0.0308 - val_ACC: 0.9908\n",
      "Epoch 248/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0404 - ACC: 0.9840\n",
      "Epoch 00248: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0486 - ACC: 0.9860 - val_loss: 0.0310 - val_ACC: 0.9908\n",
      "Epoch 249/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0855 - ACC: 0.9740\n",
      "Epoch 00249: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0514 - ACC: 0.9861 - val_loss: 0.0224 - val_ACC: 0.9931\n",
      "Epoch 250/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0287 - ACC: 0.9880\n",
      "Epoch 00250: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0505 - ACC: 0.9848 - val_loss: 0.0423 - val_ACC: 0.9877\n",
      "Epoch 251/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0281 - ACC: 0.9940\n",
      "Epoch 00251: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0500 - ACC: 0.9856 - val_loss: 0.0412 - val_ACC: 0.9877\n",
      "Epoch 252/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0413 - ACC: 0.9900\n",
      "Epoch 00252: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0492 - ACC: 0.9861 - val_loss: 0.0522 - val_ACC: 0.9862\n",
      "Epoch 253/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0531 - ACC: 0.9840\n",
      "Epoch 00253: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0550 - ACC: 0.9823 - val_loss: 0.0177 - val_ACC: 0.9969\n",
      "Epoch 254/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0508 - ACC: 0.9840\n",
      "Epoch 00254: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0541 - ACC: 0.9838 - val_loss: 0.0283 - val_ACC: 0.9915\n",
      "Epoch 255/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0761 - ACC: 0.9820\n",
      "Epoch 00255: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0495 - ACC: 0.9856 - val_loss: 0.0377 - val_ACC: 0.9900\n",
      "Epoch 256/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0796 - ACC: 0.9840\n",
      "Epoch 00256: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0484 - ACC: 0.9858 - val_loss: 0.0396 - val_ACC: 0.9877\n",
      "Epoch 257/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0358 - ACC: 0.9920\n",
      "Epoch 00257: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0509 - ACC: 0.9846 - val_loss: 0.0202 - val_ACC: 0.9954\n",
      "Epoch 258/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0778 - ACC: 0.9780\n",
      "Epoch 00258: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0513 - ACC: 0.9854 - val_loss: 0.0372 - val_ACC: 0.9892\n",
      "Epoch 259/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0326 - ACC: 0.9860\n",
      "Epoch 00259: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0488 - ACC: 0.9860 - val_loss: 0.0548 - val_ACC: 0.9831\n",
      "Epoch 260/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0550 - ACC: 0.9860\n",
      "Epoch 00260: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0502 - ACC: 0.9846 - val_loss: 0.0548 - val_ACC: 0.9823\n",
      "Epoch 261/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0501 - ACC: 0.9880\n",
      "Epoch 00261: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0515 - ACC: 0.9860 - val_loss: 0.0345 - val_ACC: 0.9900\n",
      "Epoch 262/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0331 - ACC: 0.9880\n",
      "Epoch 00262: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0514 - ACC: 0.9852 - val_loss: 0.0238 - val_ACC: 0.9915\n",
      "Epoch 263/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0186 - ACC: 0.9960\n",
      "Epoch 00263: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0570 - ACC: 0.9833 - val_loss: 0.0558 - val_ACC: 0.9823\n",
      "Epoch 264/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0296 - ACC: 0.9960\n",
      "Epoch 00264: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0724 - ACC: 0.9767 - val_loss: 0.0292 - val_ACC: 0.9908\n",
      "Epoch 265/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0355 - ACC: 0.9920\n",
      "Epoch 00265: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0628 - ACC: 0.9794 - val_loss: 0.0200 - val_ACC: 0.9938\n",
      "Epoch 266/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0496 - ACC: 0.9840\n",
      "Epoch 00266: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0507 - ACC: 0.9856 - val_loss: 0.0209 - val_ACC: 0.9946\n",
      "Epoch 267/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0468 - ACC: 0.9820\n",
      "Epoch 00267: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0493 - ACC: 0.9850 - val_loss: 0.0285 - val_ACC: 0.9908\n",
      "Epoch 268/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0175 - ACC: 0.9960\n",
      "Epoch 00268: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0485 - ACC: 0.9854 - val_loss: 0.0579 - val_ACC: 0.9808\n",
      "Epoch 269/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0475 - ACC: 0.9880\n",
      "Epoch 00269: val_loss did not improve from 0.01627\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0547 - ACC: 0.9823 - val_loss: 0.0331 - val_ACC: 0.9900\n",
      "Epoch 270/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0351 - ACC: 0.9900\n",
      "Epoch 00270: val_loss improved from 0.01627 to 0.01542, saving model to ./model\\270 - 0.0154.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0556 - ACC: 0.9823 - val_loss: 0.0154 - val_ACC: 0.9969\n",
      "Epoch 271/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0412 - ACC: 0.9860\n",
      "Epoch 00271: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0538 - ACC: 0.9827 - val_loss: 0.0180 - val_ACC: 0.9954\n",
      "Epoch 272/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0508 - ACC: 0.9900\n",
      "Epoch 00272: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0570 - ACC: 0.9821 - val_loss: 0.0409 - val_ACC: 0.9869\n",
      "Epoch 273/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0514 - ACC: 0.9880\n",
      "Epoch 00273: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0499 - ACC: 0.9854 - val_loss: 0.0413 - val_ACC: 0.9877\n",
      "Epoch 274/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0345 - ACC: 0.9920\n",
      "Epoch 00274: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0493 - ACC: 0.9860 - val_loss: 0.0188 - val_ACC: 0.9954\n",
      "Epoch 275/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0294 - ACC: 0.9880\n",
      "Epoch 00275: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0516 - ACC: 0.9838 - val_loss: 0.0263 - val_ACC: 0.9915\n",
      "Epoch 276/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0619 - ACC: 0.9800\n",
      "Epoch 00276: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0482 - ACC: 0.9858 - val_loss: 0.0282 - val_ACC: 0.9915\n",
      "Epoch 277/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0881 - ACC: 0.9800\n",
      "Epoch 00277: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0507 - ACC: 0.9858 - val_loss: 0.0268 - val_ACC: 0.9915\n",
      "Epoch 278/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0534 - ACC: 0.9800\n",
      "Epoch 00278: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0477 - ACC: 0.9858 - val_loss: 0.0259 - val_ACC: 0.9915\n",
      "Epoch 279/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0592 - ACC: 0.9780\n",
      "Epoch 00279: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0484 - ACC: 0.9863 - val_loss: 0.0182 - val_ACC: 0.9962\n",
      "Epoch 280/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0459 - ACC: 0.9900\n",
      "Epoch 00280: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0478 - ACC: 0.9858 - val_loss: 0.0339 - val_ACC: 0.9900\n",
      "Epoch 281/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0615 - ACC: 0.9820\n",
      "Epoch 00281: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0470 - ACC: 0.9860 - val_loss: 0.0341 - val_ACC: 0.9892\n",
      "Epoch 282/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0487 - ACC: 0.9800\n",
      "Epoch 00282: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0496 - ACC: 0.9856 - val_loss: 0.0289 - val_ACC: 0.9915\n",
      "Epoch 283/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0289 - ACC: 0.9880\n",
      "Epoch 00283: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0471 - ACC: 0.9856 - val_loss: 0.0282 - val_ACC: 0.9915\n",
      "Epoch 284/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0351 - ACC: 0.9820\n",
      "Epoch 00284: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0476 - ACC: 0.9867 - val_loss: 0.0442 - val_ACC: 0.9846\n",
      "Epoch 285/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0343 - ACC: 0.9940\n",
      "Epoch 00285: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0476 - ACC: 0.9860 - val_loss: 0.0598 - val_ACC: 0.9808\n",
      "Epoch 286/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0534 - ACC: 0.9820\n",
      "Epoch 00286: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0509 - ACC: 0.9844 - val_loss: 0.0216 - val_ACC: 0.9946\n",
      "Epoch 287/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0367 - ACC: 0.9840\n",
      "Epoch 00287: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0490 - ACC: 0.9856 - val_loss: 0.0294 - val_ACC: 0.9908\n",
      "Epoch 288/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0544 - ACC: 0.9740\n",
      "Epoch 00288: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0480 - ACC: 0.9861 - val_loss: 0.0347 - val_ACC: 0.9900\n",
      "Epoch 289/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0640 - ACC: 0.9820\n",
      "Epoch 00289: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0476 - ACC: 0.9863 - val_loss: 0.0243 - val_ACC: 0.9915\n",
      "Epoch 290/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0435 - ACC: 0.9880\n",
      "Epoch 00290: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0476 - ACC: 0.9856 - val_loss: 0.0313 - val_ACC: 0.9915\n",
      "Epoch 291/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0523 - ACC: 0.9840\n",
      "Epoch 00291: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0468 - ACC: 0.9863 - val_loss: 0.0382 - val_ACC: 0.9885\n",
      "Epoch 292/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0502 - ACC: 0.9920\n",
      "Epoch 00292: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0501 - ACC: 0.9861 - val_loss: 0.0677 - val_ACC: 0.9746\n",
      "Epoch 293/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0891 - ACC: 0.9800\n",
      "Epoch 00293: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0523 - ACC: 0.9846 - val_loss: 0.0509 - val_ACC: 0.9838\n",
      "Epoch 294/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0532 - ACC: 0.9840\n",
      "Epoch 00294: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0500 - ACC: 0.9854 - val_loss: 0.0303 - val_ACC: 0.9915\n",
      "Epoch 295/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0300 - ACC: 0.9940\n",
      "Epoch 00295: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0490 - ACC: 0.9856 - val_loss: 0.0316 - val_ACC: 0.9900\n",
      "Epoch 296/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0919 - ACC: 0.9780\n",
      "Epoch 00296: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0478 - ACC: 0.9867 - val_loss: 0.0390 - val_ACC: 0.9885\n",
      "Epoch 297/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0343 - ACC: 0.9900\n",
      "Epoch 00297: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0472 - ACC: 0.9863 - val_loss: 0.0263 - val_ACC: 0.9915\n",
      "Epoch 298/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0258 - ACC: 0.9900\n",
      "Epoch 00298: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0472 - ACC: 0.9865 - val_loss: 0.0212 - val_ACC: 0.9938\n",
      "Epoch 299/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0341 - ACC: 0.9900\n",
      "Epoch 00299: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0475 - ACC: 0.9873 - val_loss: 0.0306 - val_ACC: 0.9900\n",
      "Epoch 300/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0298 - ACC: 0.9900\n",
      "Epoch 00300: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0468 - ACC: 0.9860 - val_loss: 0.0430 - val_ACC: 0.9862\n",
      "Epoch 301/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0659 - ACC: 0.9840\n",
      "Epoch 00301: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0468 - ACC: 0.9869 - val_loss: 0.0502 - val_ACC: 0.9846\n",
      "Epoch 302/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0469 - ACC: 0.9860\n",
      "Epoch 00302: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0515 - ACC: 0.9850 - val_loss: 0.0235 - val_ACC: 0.9923\n",
      "Epoch 303/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0295 - ACC: 0.9940\n",
      "Epoch 00303: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0481 - ACC: 0.9861 - val_loss: 0.0185 - val_ACC: 0.9954\n",
      "Epoch 304/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0630 - ACC: 0.9780\n",
      "Epoch 00304: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0491 - ACC: 0.9848 - val_loss: 0.0297 - val_ACC: 0.9915\n",
      "Epoch 305/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0366 - ACC: 0.9900\n",
      "Epoch 00305: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0495 - ACC: 0.9860 - val_loss: 0.0446 - val_ACC: 0.9854\n",
      "Epoch 306/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0315 - ACC: 0.9920\n",
      "Epoch 00306: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0466 - ACC: 0.9873 - val_loss: 0.0375 - val_ACC: 0.9862\n",
      "Epoch 307/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0594 - ACC: 0.9860\n",
      "Epoch 00307: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0473 - ACC: 0.9871 - val_loss: 0.0172 - val_ACC: 0.9962\n",
      "Epoch 308/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0571 - ACC: 0.9800\n",
      "Epoch 00308: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0494 - ACC: 0.9840 - val_loss: 0.0270 - val_ACC: 0.9931\n",
      "Epoch 309/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0298 - ACC: 0.9920\n",
      "Epoch 00309: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0468 - ACC: 0.9861 - val_loss: 0.0337 - val_ACC: 0.9885\n",
      "Epoch 310/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0529 - ACC: 0.9820\n",
      "Epoch 00310: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0521 - ACC: 0.9833 - val_loss: 0.0622 - val_ACC: 0.9777\n",
      "Epoch 311/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0553 - ACC: 0.9800\n",
      "Epoch 00311: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0510 - ACC: 0.9850 - val_loss: 0.0342 - val_ACC: 0.9885\n",
      "Epoch 312/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0492 - ACC: 0.9840\n",
      "Epoch 00312: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0459 - ACC: 0.9869 - val_loss: 0.0226 - val_ACC: 0.9938\n",
      "Epoch 313/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0355 - ACC: 0.9860\n",
      "Epoch 00313: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0467 - ACC: 0.9861 - val_loss: 0.0217 - val_ACC: 0.9938\n",
      "Epoch 314/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0190 - ACC: 0.9940\n",
      "Epoch 00314: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0465 - ACC: 0.9865 - val_loss: 0.0223 - val_ACC: 0.9931\n",
      "Epoch 315/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0255 - ACC: 0.9940\n",
      "Epoch 00315: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0484 - ACC: 0.9856 - val_loss: 0.0460 - val_ACC: 0.9846\n",
      "Epoch 316/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0269 - ACC: 0.9940\n",
      "Epoch 00316: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0464 - ACC: 0.9877 - val_loss: 0.0508 - val_ACC: 0.9838\n",
      "Epoch 317/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0640 - ACC: 0.9840\n",
      "Epoch 00317: val_loss did not improve from 0.01542\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0497 - ACC: 0.9863 - val_loss: 0.0245 - val_ACC: 0.9931\n",
      "Epoch 318/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0330 - ACC: 0.9900\n",
      "Epoch 00318: val_loss improved from 0.01542 to 0.01519, saving model to ./model\\318 - 0.0152.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0500 - ACC: 0.9856 - val_loss: 0.0152 - val_ACC: 0.9969\n",
      "Epoch 319/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0410 - ACC: 0.9880\n",
      "Epoch 00319: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0509 - ACC: 0.9850 - val_loss: 0.0332 - val_ACC: 0.9900\n",
      "Epoch 320/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0253 - ACC: 0.9940\n",
      "Epoch 00320: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0479 - ACC: 0.9871 - val_loss: 0.0465 - val_ACC: 0.9846\n",
      "Epoch 321/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0763 - ACC: 0.9820\n",
      "Epoch 00321: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0497 - ACC: 0.9856 - val_loss: 0.0289 - val_ACC: 0.9923\n",
      "Epoch 322/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0305 - ACC: 0.9860\n",
      "Epoch 00322: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0457 - ACC: 0.9863 - val_loss: 0.0351 - val_ACC: 0.9892\n",
      "Epoch 323/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0452 - ACC: 0.9860\n",
      "Epoch 00323: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0451 - ACC: 0.9873 - val_loss: 0.0403 - val_ACC: 0.9846\n",
      "Epoch 324/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0405 - ACC: 0.9900\n",
      "Epoch 00324: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0445 - ACC: 0.9873 - val_loss: 0.0269 - val_ACC: 0.9923\n",
      "Epoch 325/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0439 - ACC: 0.9860\n",
      "Epoch 00325: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0446 - ACC: 0.9861 - val_loss: 0.0356 - val_ACC: 0.9885\n",
      "Epoch 326/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0582 - ACC: 0.9860\n",
      "Epoch 00326: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0460 - ACC: 0.9877 - val_loss: 0.0245 - val_ACC: 0.9931\n",
      "Epoch 327/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0254 - ACC: 0.9900\n",
      "Epoch 00327: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0463 - ACC: 0.9858 - val_loss: 0.0185 - val_ACC: 0.9954\n",
      "Epoch 328/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0394 - ACC: 0.9820\n",
      "Epoch 00328: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0471 - ACC: 0.9856 - val_loss: 0.0314 - val_ACC: 0.9908\n",
      "Epoch 329/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0217 - ACC: 0.9920\n",
      "Epoch 00329: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0467 - ACC: 0.9861 - val_loss: 0.0359 - val_ACC: 0.9885\n",
      "Epoch 330/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0721 - ACC: 0.9840\n",
      "Epoch 00330: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0446 - ACC: 0.9867 - val_loss: 0.0258 - val_ACC: 0.9931\n",
      "Epoch 331/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0521 - ACC: 0.9880\n",
      "Epoch 00331: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0440 - ACC: 0.9869 - val_loss: 0.0348 - val_ACC: 0.9885\n",
      "Epoch 332/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0622 - ACC: 0.9860\n",
      "Epoch 00332: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0443 - ACC: 0.9869 - val_loss: 0.0517 - val_ACC: 0.9808\n",
      "Epoch 333/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0282 - ACC: 0.9920\n",
      "Epoch 00333: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0486 - ACC: 0.9852 - val_loss: 0.0290 - val_ACC: 0.9915\n",
      "Epoch 334/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0373 - ACC: 0.9900\n",
      "Epoch 00334: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0459 - ACC: 0.9873 - val_loss: 0.0265 - val_ACC: 0.9915\n",
      "Epoch 335/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0521 - ACC: 0.9880\n",
      "Epoch 00335: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0449 - ACC: 0.9867 - val_loss: 0.0360 - val_ACC: 0.9885\n",
      "Epoch 336/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0203 - ACC: 0.9980\n",
      "Epoch 00336: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0448 - ACC: 0.9877 - val_loss: 0.0403 - val_ACC: 0.9885\n",
      "Epoch 337/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0451 - ACC: 0.9880\n",
      "Epoch 00337: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0483 - ACC: 0.9860 - val_loss: 0.0456 - val_ACC: 0.9838\n",
      "Epoch 338/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0319 - ACC: 0.9900\n",
      "Epoch 00338: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0485 - ACC: 0.9869 - val_loss: 0.0221 - val_ACC: 0.9946\n",
      "Epoch 339/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0303 - ACC: 0.9920\n",
      "Epoch 00339: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0454 - ACC: 0.9865 - val_loss: 0.0252 - val_ACC: 0.9931\n",
      "Epoch 340/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0609 - ACC: 0.9840\n",
      "Epoch 00340: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0446 - ACC: 0.9863 - val_loss: 0.0379 - val_ACC: 0.9862\n",
      "Epoch 341/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0529 - ACC: 0.9880\n",
      "Epoch 00341: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0441 - ACC: 0.9865 - val_loss: 0.0274 - val_ACC: 0.9923\n",
      "Epoch 342/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0603 - ACC: 0.9880\n",
      "Epoch 00342: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0463 - ACC: 0.9863 - val_loss: 0.0353 - val_ACC: 0.9885\n",
      "Epoch 343/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0294 - ACC: 0.9900\n",
      "Epoch 00343: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0477 - ACC: 0.9861 - val_loss: 0.0635 - val_ACC: 0.9769\n",
      "Epoch 344/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0622 - ACC: 0.9800\n",
      "Epoch 00344: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0537 - ACC: 0.9836 - val_loss: 0.0225 - val_ACC: 0.9938\n",
      "Epoch 345/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0304 - ACC: 0.9940\n",
      "Epoch 00345: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0472 - ACC: 0.9861 - val_loss: 0.0327 - val_ACC: 0.9915\n",
      "Epoch 346/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0289 - ACC: 0.9920\n",
      "Epoch 00346: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0464 - ACC: 0.9867 - val_loss: 0.0371 - val_ACC: 0.9900\n",
      "Epoch 347/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0307 - ACC: 0.9900\n",
      "Epoch 00347: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0443 - ACC: 0.9869 - val_loss: 0.0374 - val_ACC: 0.9885\n",
      "Epoch 348/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0225 - ACC: 0.9940\n",
      "Epoch 00348: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0440 - ACC: 0.9875 - val_loss: 0.0366 - val_ACC: 0.9885\n",
      "Epoch 349/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0529 - ACC: 0.9880\n",
      "Epoch 00349: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0434 - ACC: 0.9875 - val_loss: 0.0276 - val_ACC: 0.9923\n",
      "Epoch 350/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0193 - ACC: 0.9920\n",
      "Epoch 00350: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0441 - ACC: 0.9875 - val_loss: 0.0240 - val_ACC: 0.9938\n",
      "Epoch 351/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0527 - ACC: 0.9840\n",
      "Epoch 00351: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0445 - ACC: 0.9883 - val_loss: 0.0198 - val_ACC: 0.9954\n",
      "Epoch 352/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0456 - ACC: 0.9800\n",
      "Epoch 00352: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0428 - ACC: 0.9865 - val_loss: 0.0336 - val_ACC: 0.9915\n",
      "Epoch 353/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0446 - ACC: 0.9840\n",
      "Epoch 00353: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0431 - ACC: 0.9877 - val_loss: 0.0350 - val_ACC: 0.9892\n",
      "Epoch 354/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0214 - ACC: 0.9940\n",
      "Epoch 00354: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9877 - val_loss: 0.0268 - val_ACC: 0.9923\n",
      "Epoch 355/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0451 - ACC: 0.9940\n",
      "Epoch 00355: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0460 - ACC: 0.9860 - val_loss: 0.0247 - val_ACC: 0.9931\n",
      "Epoch 356/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0515 - ACC: 0.9900\n",
      "Epoch 00356: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9875 - val_loss: 0.0325 - val_ACC: 0.9900\n",
      "Epoch 357/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0650 - ACC: 0.9860\n",
      "Epoch 00357: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0434 - ACC: 0.9877 - val_loss: 0.0354 - val_ACC: 0.9892\n",
      "Epoch 358/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0314 - ACC: 0.9880\n",
      "Epoch 00358: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9869 - val_loss: 0.0198 - val_ACC: 0.9954\n",
      "Epoch 359/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0521 - ACC: 0.9820\n",
      "Epoch 00359: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0479 - ACC: 0.9856 - val_loss: 0.0316 - val_ACC: 0.9915\n",
      "Epoch 360/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0331 - ACC: 0.9920\n",
      "Epoch 00360: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0438 - ACC: 0.9881 - val_loss: 0.0427 - val_ACC: 0.9862\n",
      "Epoch 361/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0391 - ACC: 0.9900\n",
      "Epoch 00361: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0439 - ACC: 0.9871 - val_loss: 0.0249 - val_ACC: 0.9931\n",
      "Epoch 362/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0474 - ACC: 0.9860\n",
      "Epoch 00362: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0489 - ACC: 0.9848 - val_loss: 0.0201 - val_ACC: 0.9954\n",
      "Epoch 363/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0232 - ACC: 0.9940\n",
      "Epoch 00363: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0435 - ACC: 0.9886 - val_loss: 0.0445 - val_ACC: 0.9831\n",
      "Epoch 364/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0324 - ACC: 0.9880\n",
      "Epoch 00364: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0460 - ACC: 0.9869 - val_loss: 0.0381 - val_ACC: 0.9885\n",
      "Epoch 365/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0289 - ACC: 0.9900\n",
      "Epoch 00365: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0434 - ACC: 0.9875 - val_loss: 0.0340 - val_ACC: 0.9885\n",
      "Epoch 366/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0209 - ACC: 0.9960\n",
      "Epoch 00366: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0427 - ACC: 0.9885 - val_loss: 0.0295 - val_ACC: 0.9923\n",
      "Epoch 367/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0893 - ACC: 0.9820\n",
      "Epoch 00367: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0423 - ACC: 0.9879 - val_loss: 0.0273 - val_ACC: 0.9923\n",
      "Epoch 368/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0453 - ACC: 0.9840\n",
      "Epoch 00368: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0441 - ACC: 0.9894 - val_loss: 0.0414 - val_ACC: 0.9846\n",
      "Epoch 369/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0436 - ACC: 0.9840\n",
      "Epoch 00369: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0454 - ACC: 0.9865 - val_loss: 0.0255 - val_ACC: 0.9931\n",
      "Epoch 370/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0227 - ACC: 0.9960\n",
      "Epoch 00370: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9873 - val_loss: 0.0174 - val_ACC: 0.9954\n",
      "Epoch 371/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0467 - ACC: 0.9820\n",
      "Epoch 00371: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0441 - ACC: 0.9865 - val_loss: 0.0222 - val_ACC: 0.9954\n",
      "Epoch 372/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0713 - ACC: 0.9860\n",
      "Epoch 00372: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0453 - ACC: 0.9875 - val_loss: 0.0442 - val_ACC: 0.9846\n",
      "Epoch 373/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0497 - ACC: 0.9900\n",
      "Epoch 00373: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0473 - ACC: 0.9861 - val_loss: 0.0492 - val_ACC: 0.9808\n",
      "Epoch 374/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0617 - ACC: 0.9780\n",
      "Epoch 00374: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0465 - ACC: 0.9863 - val_loss: 0.0234 - val_ACC: 0.9938\n",
      "Epoch 375/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0602 - ACC: 0.9780\n",
      "Epoch 00375: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0436 - ACC: 0.9873 - val_loss: 0.0319 - val_ACC: 0.9908\n",
      "Epoch 376/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0373 - ACC: 0.9880\n",
      "Epoch 00376: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0486 - ACC: 0.9854 - val_loss: 0.0380 - val_ACC: 0.9885\n",
      "Epoch 377/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0281 - ACC: 0.9940\n",
      "Epoch 00377: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0425 - ACC: 0.9881 - val_loss: 0.0281 - val_ACC: 0.9923\n",
      "Epoch 378/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0160 - ACC: 0.9940\n",
      "Epoch 00378: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0418 - ACC: 0.9883 - val_loss: 0.0331 - val_ACC: 0.9923\n",
      "Epoch 379/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0426 - ACC: 0.9900\n",
      "Epoch 00379: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0416 - ACC: 0.9890 - val_loss: 0.0271 - val_ACC: 0.9931\n",
      "Epoch 380/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0383 - ACC: 0.9880\n",
      "Epoch 00380: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9873 - val_loss: 0.0216 - val_ACC: 0.9946\n",
      "Epoch 381/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0359 - ACC: 0.9880\n",
      "Epoch 00381: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0430 - ACC: 0.9885 - val_loss: 0.0388 - val_ACC: 0.9869\n",
      "Epoch 382/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0453 - ACC: 0.9840\n",
      "Epoch 00382: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0420 - ACC: 0.9879 - val_loss: 0.0284 - val_ACC: 0.9923\n",
      "Epoch 383/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0368 - ACC: 0.9900\n",
      "Epoch 00383: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0431 - ACC: 0.9879 - val_loss: 0.0174 - val_ACC: 0.9954\n",
      "Epoch 384/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0612 - ACC: 0.9800\n",
      "Epoch 00384: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0428 - ACC: 0.9881 - val_loss: 0.0342 - val_ACC: 0.9915\n",
      "Epoch 385/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0459 - ACC: 0.9920\n",
      "Epoch 00385: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0423 - ACC: 0.9875 - val_loss: 0.0411 - val_ACC: 0.9854\n",
      "Epoch 386/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0308 - ACC: 0.9940\n",
      "Epoch 00386: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0445 - ACC: 0.9873 - val_loss: 0.0286 - val_ACC: 0.9923\n",
      "Epoch 387/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0577 - ACC: 0.9880\n",
      "Epoch 00387: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0535 - ACC: 0.9835 - val_loss: 0.0251 - val_ACC: 0.9931\n",
      "Epoch 388/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0322 - ACC: 0.9900\n",
      "Epoch 00388: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0478 - ACC: 0.9861 - val_loss: 0.0480 - val_ACC: 0.9815\n",
      "Epoch 389/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0275 - ACC: 0.9920\n",
      "Epoch 00389: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0427 - ACC: 0.9877 - val_loss: 0.0373 - val_ACC: 0.9900\n",
      "Epoch 390/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0358 - ACC: 0.9880\n",
      "Epoch 00390: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0433 - ACC: 0.9871 - val_loss: 0.0173 - val_ACC: 0.9962\n",
      "Epoch 391/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0412 - ACC: 0.9840\n",
      "Epoch 00391: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0422 - ACC: 0.9890 - val_loss: 0.0295 - val_ACC: 0.9923\n",
      "Epoch 392/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0676 - ACC: 0.9840\n",
      "Epoch 00392: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9886 - val_loss: 0.0250 - val_ACC: 0.9938\n",
      "Epoch 393/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0535 - ACC: 0.9860\n",
      "Epoch 00393: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0411 - ACC: 0.9877 - val_loss: 0.0261 - val_ACC: 0.9938\n",
      "Epoch 394/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0404 - ACC: 0.9880\n",
      "Epoch 00394: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9885 - val_loss: 0.0210 - val_ACC: 0.9946\n",
      "Epoch 395/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0463 - ACC: 0.9880\n",
      "Epoch 00395: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0435 - ACC: 0.9873 - val_loss: 0.0307 - val_ACC: 0.9923\n",
      "Epoch 396/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0471 - ACC: 0.9820\n",
      "Epoch 00396: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0414 - ACC: 0.9885 - val_loss: 0.0521 - val_ACC: 0.9808\n",
      "Epoch 397/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0398 - ACC: 0.9860\n",
      "Epoch 00397: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9873 - val_loss: 0.0394 - val_ACC: 0.9862\n",
      "Epoch 398/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0410 - ACC: 0.9900\n",
      "Epoch 00398: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0443 - ACC: 0.9873 - val_loss: 0.0183 - val_ACC: 0.9954\n",
      "Epoch 399/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0549 - ACC: 0.9820\n",
      "Epoch 00399: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0454 - ACC: 0.9867 - val_loss: 0.0221 - val_ACC: 0.9946\n",
      "Epoch 400/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0426 - ACC: 0.9860\n",
      "Epoch 00400: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0417 - ACC: 0.9888 - val_loss: 0.0240 - val_ACC: 0.9946\n",
      "Epoch 401/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0196 - ACC: 0.9940\n",
      "Epoch 00401: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0418 - ACC: 0.9883 - val_loss: 0.0283 - val_ACC: 0.9923\n",
      "Epoch 402/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0356 - ACC: 0.9880\n",
      "Epoch 00402: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0434 - ACC: 0.9869 - val_loss: 0.0242 - val_ACC: 0.9938\n",
      "Epoch 403/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0345 - ACC: 0.9940\n",
      "Epoch 00403: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0465 - ACC: 0.9860 - val_loss: 0.0540 - val_ACC: 0.9800\n",
      "Epoch 404/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0568 - ACC: 0.9820\n",
      "Epoch 00404: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0427 - ACC: 0.9865 - val_loss: 0.0281 - val_ACC: 0.9923\n",
      "Epoch 405/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0621 - ACC: 0.9840\n",
      "Epoch 00405: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0441 - ACC: 0.9867 - val_loss: 0.0244 - val_ACC: 0.9938\n",
      "Epoch 406/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0332 - ACC: 0.9900\n",
      "Epoch 00406: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0417 - ACC: 0.9883 - val_loss: 0.0438 - val_ACC: 0.9838\n",
      "Epoch 407/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0274 - ACC: 0.9860\n",
      "Epoch 00407: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0423 - ACC: 0.9871 - val_loss: 0.0312 - val_ACC: 0.9915\n",
      "Epoch 408/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0726 - ACC: 0.9820\n",
      "Epoch 00408: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9885 - val_loss: 0.0299 - val_ACC: 0.9931\n",
      "Epoch 409/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0220 - ACC: 0.9960\n",
      "Epoch 00409: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0403 - ACC: 0.9890 - val_loss: 0.0584 - val_ACC: 0.9808\n",
      "Epoch 410/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0529 - ACC: 0.9900\n",
      "Epoch 00410: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0447 - ACC: 0.9883 - val_loss: 0.0302 - val_ACC: 0.9923\n",
      "Epoch 411/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0464 - ACC: 0.9880\n",
      "Epoch 00411: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0419 - ACC: 0.9877 - val_loss: 0.0266 - val_ACC: 0.9938\n",
      "Epoch 412/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0453 - ACC: 0.9860\n",
      "Epoch 00412: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0414 - ACC: 0.9879 - val_loss: 0.0375 - val_ACC: 0.9869\n",
      "Epoch 413/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0573 - ACC: 0.9880\n",
      "Epoch 00413: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0416 - ACC: 0.9885 - val_loss: 0.0511 - val_ACC: 0.9808\n",
      "Epoch 414/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0778 - ACC: 0.9840\n",
      "Epoch 00414: val_loss did not improve from 0.01519\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0432 - ACC: 0.9881 - val_loss: 0.0383 - val_ACC: 0.9869\n",
      "Epoch 415/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0222 - ACC: 0.9940\n",
      "Epoch 00415: val_loss improved from 0.01519 to 0.01422, saving model to ./model\\415 - 0.0142.hdf5\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0472 - ACC: 0.9860 - val_loss: 0.0142 - val_ACC: 0.9954\n",
      "Epoch 416/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0214 - ACC: 0.9900\n",
      "Epoch 00416: val_loss did not improve from 0.01422\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0506 - ACC: 0.9852 - val_loss: 0.0215 - val_ACC: 0.9938\n",
      "Epoch 417/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0203 - ACC: 0.9940\n",
      "Epoch 00417: val_loss did not improve from 0.01422\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0473 - ACC: 0.9856 - val_loss: 0.1123 - val_ACC: 0.9631\n",
      "Epoch 418/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0762 - ACC: 0.9780\n",
      "Epoch 00418: val_loss did not improve from 0.01422\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0685 - ACC: 0.9794 - val_loss: 0.0250 - val_ACC: 0.9938\n",
      "Epoch 419/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0343 - ACC: 0.9920\n",
      "Epoch 00419: val_loss improved from 0.01422 to 0.01416, saving model to ./model\\419 - 0.0142.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0518 - ACC: 0.9848 - val_loss: 0.0142 - val_ACC: 0.9954\n",
      "Epoch 420/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0249 - ACC: 0.9900\n",
      "Epoch 00420: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0455 - ACC: 0.9860 - val_loss: 0.0230 - val_ACC: 0.9938\n",
      "Epoch 421/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0448 - ACC: 0.9920\n",
      "Epoch 00421: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0410 - ACC: 0.9883 - val_loss: 0.0303 - val_ACC: 0.9931\n",
      "Epoch 422/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0573 - ACC: 0.9860\n",
      "Epoch 00422: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0409 - ACC: 0.9875 - val_loss: 0.0306 - val_ACC: 0.9915\n",
      "Epoch 423/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0272 - ACC: 0.9900\n",
      "Epoch 00423: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9886 - val_loss: 0.0250 - val_ACC: 0.9938\n",
      "Epoch 424/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0608 - ACC: 0.9840\n",
      "Epoch 00424: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0403 - ACC: 0.9892 - val_loss: 0.0422 - val_ACC: 0.9862\n",
      "Epoch 425/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0266 - ACC: 0.9880\n",
      "Epoch 00425: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0433 - ACC: 0.9869 - val_loss: 0.0254 - val_ACC: 0.9931\n",
      "Epoch 426/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0545 - ACC: 0.9900\n",
      "Epoch 00426: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9892 - val_loss: 0.0278 - val_ACC: 0.9923\n",
      "Epoch 427/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0283 - ACC: 0.9900\n",
      "Epoch 00427: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0393 - ACC: 0.9892 - val_loss: 0.0169 - val_ACC: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0716 - ACC: 0.9800\n",
      "Epoch 00428: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0432 - ACC: 0.9873 - val_loss: 0.0277 - val_ACC: 0.9938\n",
      "Epoch 429/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0355 - ACC: 0.9920\n",
      "Epoch 00429: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0446 - ACC: 0.9869 - val_loss: 0.0414 - val_ACC: 0.9869\n",
      "Epoch 430/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0410 - ACC: 0.9880\n",
      "Epoch 00430: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0414 - ACC: 0.9886 - val_loss: 0.0483 - val_ACC: 0.9800\n",
      "Epoch 431/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0592 - ACC: 0.9880\n",
      "Epoch 00431: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0407 - ACC: 0.9888 - val_loss: 0.0213 - val_ACC: 0.9954\n",
      "Epoch 432/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0296 - ACC: 0.9920\n",
      "Epoch 00432: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0456 - ACC: 0.9863 - val_loss: 0.0263 - val_ACC: 0.9938\n",
      "Epoch 433/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0504 - ACC: 0.9880\n",
      "Epoch 00433: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0445 - ACC: 0.9865 - val_loss: 0.0386 - val_ACC: 0.9831\n",
      "Epoch 434/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0269 - ACC: 0.9940\n",
      "Epoch 00434: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0405 - ACC: 0.9890 - val_loss: 0.0262 - val_ACC: 0.9938\n",
      "Epoch 435/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0178 - ACC: 0.9960\n",
      "Epoch 00435: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0406 - ACC: 0.9881 - val_loss: 0.0188 - val_ACC: 0.9946\n",
      "Epoch 436/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0613 - ACC: 0.9800\n",
      "Epoch 00436: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0444 - ACC: 0.9860 - val_loss: 0.0292 - val_ACC: 0.9931\n",
      "Epoch 437/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0477 - ACC: 0.9880\n",
      "Epoch 00437: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0409 - ACC: 0.9890 - val_loss: 0.0620 - val_ACC: 0.9792\n",
      "Epoch 438/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0467 - ACC: 0.9880\n",
      "Epoch 00438: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0443 - ACC: 0.9869 - val_loss: 0.0447 - val_ACC: 0.9823\n",
      "Epoch 439/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0117 - ACC: 1.0000\n",
      "Epoch 00439: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0404 - ACC: 0.9894 - val_loss: 0.0401 - val_ACC: 0.9846\n",
      "Epoch 440/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0520 - ACC: 0.9900\n",
      "Epoch 00440: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0401 - ACC: 0.9894 - val_loss: 0.0292 - val_ACC: 0.9931\n",
      "Epoch 441/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0521 - ACC: 0.9860\n",
      "Epoch 00441: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0398 - ACC: 0.9888 - val_loss: 0.0469 - val_ACC: 0.9838\n",
      "Epoch 442/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0555 - ACC: 0.9840\n",
      "Epoch 00442: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0420 - ACC: 0.9883 - val_loss: 0.0460 - val_ACC: 0.9815\n",
      "Epoch 443/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0630 - ACC: 0.9840\n",
      "Epoch 00443: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0428 - ACC: 0.9885 - val_loss: 0.0299 - val_ACC: 0.9915\n",
      "Epoch 444/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0235 - ACC: 0.9960\n",
      "Epoch 00444: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9885 - val_loss: 0.0254 - val_ACC: 0.9938\n",
      "Epoch 445/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0398 - ACC: 0.9920\n",
      "Epoch 00445: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0400 - ACC: 0.9888 - val_loss: 0.0379 - val_ACC: 0.9869\n",
      "Epoch 446/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0359 - ACC: 0.9920\n",
      "Epoch 00446: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0400 - ACC: 0.9892 - val_loss: 0.0314 - val_ACC: 0.9923\n",
      "Epoch 447/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0441 - ACC: 0.9880\n",
      "Epoch 00447: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0401 - ACC: 0.9888 - val_loss: 0.0524 - val_ACC: 0.9808\n",
      "Epoch 448/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0494 - ACC: 0.9820\n",
      "Epoch 00448: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0401 - ACC: 0.9885 - val_loss: 0.0302 - val_ACC: 0.9931\n",
      "Epoch 449/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0365 - ACC: 0.9860\n",
      "Epoch 00449: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0415 - ACC: 0.9885 - val_loss: 0.0393 - val_ACC: 0.9869\n",
      "Epoch 450/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0481 - ACC: 0.9860\n",
      "Epoch 00450: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0451 - ACC: 0.9860 - val_loss: 0.0163 - val_ACC: 0.9954\n",
      "Epoch 451/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0610 - ACC: 0.9840\n",
      "Epoch 00451: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0483 - ACC: 0.9869 - val_loss: 0.0439 - val_ACC: 0.9854\n",
      "Epoch 452/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0374 - ACC: 0.9900\n",
      "Epoch 00452: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0415 - ACC: 0.9888 - val_loss: 0.0394 - val_ACC: 0.9869\n",
      "Epoch 453/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0331 - ACC: 0.9860\n",
      "Epoch 00453: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0417 - ACC: 0.9875 - val_loss: 0.0212 - val_ACC: 0.9938\n",
      "Epoch 454/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0510 - ACC: 0.9860\n",
      "Epoch 00454: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0467 - ACC: 0.9856 - val_loss: 0.0185 - val_ACC: 0.9954\n",
      "Epoch 455/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0686 - ACC: 0.9800\n",
      "Epoch 00455: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0453 - ACC: 0.9875 - val_loss: 0.0424 - val_ACC: 0.9831\n",
      "Epoch 456/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0463 - ACC: 0.9840\n",
      "Epoch 00456: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0409 - ACC: 0.9888 - val_loss: 0.0838 - val_ACC: 0.9715\n",
      "Epoch 457/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0333 - ACC: 0.9860\n",
      "Epoch 00457: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0436 - ACC: 0.9873 - val_loss: 0.0233 - val_ACC: 0.9938\n",
      "Epoch 458/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0342 - ACC: 0.9860\n",
      "Epoch 00458: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0416 - ACC: 0.9881 - val_loss: 0.0180 - val_ACC: 0.9954\n",
      "Epoch 459/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0191 - ACC: 0.9940\n",
      "Epoch 00459: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0421 - ACC: 0.9881 - val_loss: 0.0388 - val_ACC: 0.9877\n",
      "Epoch 460/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0370 - ACC: 0.9880\n",
      "Epoch 00460: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0417 - ACC: 0.9881 - val_loss: 0.0302 - val_ACC: 0.9923\n",
      "Epoch 461/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0346 - ACC: 0.9900\n",
      "Epoch 00461: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0397 - ACC: 0.9894 - val_loss: 0.0258 - val_ACC: 0.9938\n",
      "Epoch 462/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0362 - ACC: 0.9860\n",
      "Epoch 00462: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0405 - ACC: 0.9877 - val_loss: 0.0402 - val_ACC: 0.9877\n",
      "Epoch 463/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0407 - ACC: 0.9820\n",
      "Epoch 00463: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0405 - ACC: 0.9877 - val_loss: 0.0275 - val_ACC: 0.9938\n",
      "Epoch 464/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0373 - ACC: 0.9860\n",
      "Epoch 00464: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0406 - ACC: 0.9875 - val_loss: 0.0387 - val_ACC: 0.9877\n",
      "Epoch 465/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0460 - ACC: 0.9820\n",
      "Epoch 00465: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0411 - ACC: 0.9879 - val_loss: 0.0547 - val_ACC: 0.9808\n",
      "Epoch 466/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0691 - ACC: 0.9860\n",
      "Epoch 00466: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0439 - ACC: 0.9869 - val_loss: 0.0279 - val_ACC: 0.9931\n",
      "Epoch 467/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0391 - ACC: 0.9860\n",
      "Epoch 00467: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0391 - ACC: 0.9890 - val_loss: 0.0456 - val_ACC: 0.9815\n",
      "Epoch 468/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0421 - ACC: 0.9900\n",
      "Epoch 00468: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0404 - ACC: 0.9892 - val_loss: 0.0202 - val_ACC: 0.9938\n",
      "Epoch 469/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0292 - ACC: 0.9820\n",
      "Epoch 00469: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9883 - val_loss: 0.0283 - val_ACC: 0.9938\n",
      "Epoch 470/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0198 - ACC: 0.9940\n",
      "Epoch 00470: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0388 - ACC: 0.9902 - val_loss: 0.0247 - val_ACC: 0.9938\n",
      "Epoch 471/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0319 - ACC: 0.9880\n",
      "Epoch 00471: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0389 - ACC: 0.9894 - val_loss: 0.0238 - val_ACC: 0.9938\n",
      "Epoch 472/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0228 - ACC: 0.9880\n",
      "Epoch 00472: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0396 - ACC: 0.9883 - val_loss: 0.0411 - val_ACC: 0.9877\n",
      "Epoch 473/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0249 - ACC: 0.9900\n",
      "Epoch 00473: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0394 - ACC: 0.9885 - val_loss: 0.0349 - val_ACC: 0.9892\n",
      "Epoch 474/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0283 - ACC: 0.9880\n",
      "Epoch 00474: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0394 - ACC: 0.9900 - val_loss: 0.0383 - val_ACC: 0.9877\n",
      "Epoch 475/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0290 - ACC: 0.9940\n",
      "Epoch 00475: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9885 - val_loss: 0.0506 - val_ACC: 0.9808\n",
      "Epoch 476/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0337 - ACC: 0.9880\n",
      "Epoch 00476: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0424 - ACC: 0.9869 - val_loss: 0.0206 - val_ACC: 0.9938\n",
      "Epoch 477/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0556 - ACC: 0.9820\n",
      "Epoch 00477: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0405 - ACC: 0.9881 - val_loss: 0.0205 - val_ACC: 0.9946\n",
      "Epoch 478/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0433 - ACC: 0.9900\n",
      "Epoch 00478: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0403 - ACC: 0.9885 - val_loss: 0.0319 - val_ACC: 0.9892\n",
      "Epoch 479/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0416 - ACC: 0.9880\n",
      "Epoch 00479: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0390 - ACC: 0.9894 - val_loss: 0.0210 - val_ACC: 0.9946\n",
      "Epoch 480/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0721 - ACC: 0.9800\n",
      "Epoch 00480: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0425 - ACC: 0.9869 - val_loss: 0.0287 - val_ACC: 0.9938\n",
      "Epoch 481/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0529 - ACC: 0.9860\n",
      "Epoch 00481: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0410 - ACC: 0.9890 - val_loss: 0.0363 - val_ACC: 0.9885\n",
      "Epoch 482/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0384 - ACC: 0.9920\n",
      "Epoch 00482: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0393 - ACC: 0.9888 - val_loss: 0.0324 - val_ACC: 0.9923\n",
      "Epoch 483/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0275 - ACC: 0.9920\n",
      "Epoch 00483: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0399 - ACC: 0.9892 - val_loss: 0.0531 - val_ACC: 0.9815\n",
      "Epoch 484/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0467 - ACC: 0.9880\n",
      "Epoch 00484: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0406 - ACC: 0.9885 - val_loss: 0.0458 - val_ACC: 0.9846\n",
      "Epoch 485/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0196 - ACC: 0.9940\n",
      "Epoch 00485: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0392 - ACC: 0.9894 - val_loss: 0.0340 - val_ACC: 0.9915\n",
      "Epoch 486/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0326 - ACC: 0.9880\n",
      "Epoch 00486: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9898 - val_loss: 0.0253 - val_ACC: 0.9938\n",
      "Epoch 487/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0423 - ACC: 0.9880\n",
      "Epoch 00487: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0393 - ACC: 0.9896 - val_loss: 0.0280 - val_ACC: 0.9931\n",
      "Epoch 488/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0263 - ACC: 0.9920\n",
      "Epoch 00488: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9902 - val_loss: 0.0217 - val_ACC: 0.9938\n",
      "Epoch 489/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0544 - ACC: 0.9820\n",
      "Epoch 00489: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9890 - val_loss: 0.0279 - val_ACC: 0.9938\n",
      "Epoch 490/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0423 - ACC: 0.9900\n",
      "Epoch 00490: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0399 - ACC: 0.9892 - val_loss: 0.0414 - val_ACC: 0.9869\n",
      "Epoch 491/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0485 - ACC: 0.9900\n",
      "Epoch 00491: val_loss did not improve from 0.01416\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0400 - ACC: 0.9881 - val_loss: 0.0380 - val_ACC: 0.9877\n",
      "Epoch 492/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0394 - ACC: 0.9860\n",
      "Epoch 00492: val_loss improved from 0.01416 to 0.01375, saving model to ./model\\492 - 0.0138.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0400 - ACC: 0.9886 - val_loss: 0.0138 - val_ACC: 0.9954\n",
      "Epoch 493/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0415 - ACC: 0.9840\n",
      "Epoch 00493: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0444 - ACC: 0.9856 - val_loss: 0.0321 - val_ACC: 0.9923\n",
      "Epoch 494/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0338 - ACC: 0.9920\n",
      "Epoch 00494: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0422 - ACC: 0.9875 - val_loss: 0.0339 - val_ACC: 0.9915\n",
      "Epoch 495/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0279 - ACC: 0.9940\n",
      "Epoch 00495: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9896 - val_loss: 0.0213 - val_ACC: 0.9938\n",
      "Epoch 496/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0342 - ACC: 0.9900\n",
      "Epoch 00496: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0400 - ACC: 0.9886 - val_loss: 0.0246 - val_ACC: 0.9938\n",
      "Epoch 497/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0517 - ACC: 0.9840\n",
      "Epoch 00497: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0389 - ACC: 0.9885 - val_loss: 0.0401 - val_ACC: 0.9877\n",
      "Epoch 498/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0420 - ACC: 0.9880\n",
      "Epoch 00498: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0455 - ACC: 0.9867 - val_loss: 0.0254 - val_ACC: 0.9938\n",
      "Epoch 499/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0588 - ACC: 0.9840\n",
      "Epoch 00499: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0420 - ACC: 0.9888 - val_loss: 0.0278 - val_ACC: 0.9923\n",
      "Epoch 500/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0504 - ACC: 0.9900\n",
      "Epoch 00500: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0393 - ACC: 0.9892 - val_loss: 0.0352 - val_ACC: 0.9892\n",
      "Epoch 501/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0407 - ACC: 0.9920\n",
      "Epoch 00501: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9896 - val_loss: 0.0237 - val_ACC: 0.9938\n",
      "Epoch 502/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0903 - ACC: 0.9820\n",
      "Epoch 00502: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9888 - val_loss: 0.0289 - val_ACC: 0.9923\n",
      "Epoch 503/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0326 - ACC: 0.9900\n",
      "Epoch 00503: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0392 - ACC: 0.9890 - val_loss: 0.0335 - val_ACC: 0.9900\n",
      "Epoch 504/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0131 - ACC: 0.9980\n",
      "Epoch 00504: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0410 - ACC: 0.9890 - val_loss: 0.0187 - val_ACC: 0.9938\n",
      "Epoch 505/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0627 - ACC: 0.9800\n",
      "Epoch 00505: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0409 - ACC: 0.9886 - val_loss: 0.0360 - val_ACC: 0.9885\n",
      "Epoch 506/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0310 - ACC: 0.9900\n",
      "Epoch 00506: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0382 - ACC: 0.9896 - val_loss: 0.0322 - val_ACC: 0.9892\n",
      "Epoch 507/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0498 - ACC: 0.9880\n",
      "Epoch 00507: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9894 - val_loss: 0.0318 - val_ACC: 0.9923\n",
      "Epoch 508/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0166 - ACC: 0.9960\n",
      "Epoch 00508: val_loss did not improve from 0.01375\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0422 - ACC: 0.9885 - val_loss: 0.0675 - val_ACC: 0.9800\n",
      "Epoch 509/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0195 - ACC: 0.9940\n",
      "Epoch 00509: val_loss improved from 0.01375 to 0.01346, saving model to ./model\\509 - 0.0135.hdf5\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0480 - ACC: 0.9852 - val_loss: 0.0135 - val_ACC: 0.9969\n",
      "Epoch 510/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0609 - ACC: 0.9800\n",
      "Epoch 00510: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0482 - ACC: 0.9846 - val_loss: 0.0302 - val_ACC: 0.9900\n",
      "Epoch 511/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0303 - ACC: 0.9880\n",
      "Epoch 00511: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0406 - ACC: 0.9892 - val_loss: 0.0411 - val_ACC: 0.9877\n",
      "Epoch 512/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0323 - ACC: 0.9900\n",
      "Epoch 00512: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0394 - ACC: 0.9890 - val_loss: 0.0355 - val_ACC: 0.9885\n",
      "Epoch 513/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0245 - ACC: 0.9960\n",
      "Epoch 00513: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0378 - ACC: 0.9896 - val_loss: 0.0198 - val_ACC: 0.9938\n",
      "Epoch 514/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0388 - ACC: 0.9880\n",
      "Epoch 00514: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0393 - ACC: 0.9892 - val_loss: 0.0188 - val_ACC: 0.9954\n",
      "Epoch 515/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0462 - ACC: 0.9860\n",
      "Epoch 00515: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0429 - ACC: 0.9863 - val_loss: 0.0442 - val_ACC: 0.9838\n",
      "Epoch 516/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0468 - ACC: 0.9820\n",
      "Epoch 00516: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0418 - ACC: 0.9875 - val_loss: 0.0562 - val_ACC: 0.9831\n",
      "Epoch 517/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0437 - ACC: 0.9840\n",
      "Epoch 00517: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0391 - ACC: 0.9892 - val_loss: 0.0223 - val_ACC: 0.9938\n",
      "Epoch 518/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0484 - ACC: 0.9860\n",
      "Epoch 00518: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0413 - ACC: 0.9892 - val_loss: 0.0505 - val_ACC: 0.9815\n",
      "Epoch 519/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0470 - ACC: 0.9840\n",
      "Epoch 00519: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0401 - ACC: 0.9896 - val_loss: 0.0179 - val_ACC: 0.9946\n",
      "Epoch 520/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0515 - ACC: 0.9840\n",
      "Epoch 00520: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0417 - ACC: 0.9871 - val_loss: 0.0490 - val_ACC: 0.9877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 521/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0266 - ACC: 0.9920\n",
      "Epoch 00521: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0390 - ACC: 0.9888 - val_loss: 0.0319 - val_ACC: 0.9923\n",
      "Epoch 522/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0202 - ACC: 0.9960\n",
      "Epoch 00522: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0376 - ACC: 0.9906 - val_loss: 0.0340 - val_ACC: 0.9892\n",
      "Epoch 523/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0372 - ACC: 0.9860\n",
      "Epoch 00523: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0387 - ACC: 0.9902 - val_loss: 0.0282 - val_ACC: 0.9931\n",
      "Epoch 524/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0180 - ACC: 0.9940\n",
      "Epoch 00524: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9898 - val_loss: 0.0295 - val_ACC: 0.9938\n",
      "Epoch 525/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0430 - ACC: 0.9900\n",
      "Epoch 00525: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0409 - ACC: 0.9877 - val_loss: 0.0252 - val_ACC: 0.9938\n",
      "Epoch 526/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0468 - ACC: 0.9860\n",
      "Epoch 00526: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0394 - ACC: 0.9886 - val_loss: 0.0374 - val_ACC: 0.9869\n",
      "Epoch 527/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0293 - ACC: 0.9920\n",
      "Epoch 00527: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0421 - ACC: 0.9881 - val_loss: 0.0224 - val_ACC: 0.9938\n",
      "Epoch 528/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0315 - ACC: 0.9920\n",
      "Epoch 00528: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0444 - ACC: 0.9863 - val_loss: 0.0203 - val_ACC: 0.9946\n",
      "Epoch 529/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0220 - ACC: 0.9940\n",
      "Epoch 00529: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0393 - ACC: 0.9896 - val_loss: 0.0291 - val_ACC: 0.9923\n",
      "Epoch 530/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0482 - ACC: 0.9860\n",
      "Epoch 00530: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0390 - ACC: 0.9900 - val_loss: 0.0158 - val_ACC: 0.9946\n",
      "Epoch 531/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0930 - ACC: 0.9680\n",
      "Epoch 00531: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0431 - ACC: 0.9869 - val_loss: 0.0444 - val_ACC: 0.9869\n",
      "Epoch 532/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0287 - ACC: 0.9940\n",
      "Epoch 00532: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9888 - val_loss: 0.0212 - val_ACC: 0.9938\n",
      "Epoch 533/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0632 - ACC: 0.9820\n",
      "Epoch 00533: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0503 - ACC: 0.9844 - val_loss: 0.0205 - val_ACC: 0.9946\n",
      "Epoch 534/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0404 - ACC: 0.9840\n",
      "Epoch 00534: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0453 - ACC: 0.9863 - val_loss: 0.0478 - val_ACC: 0.9831\n",
      "Epoch 535/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0527 - ACC: 0.9880\n",
      "Epoch 00535: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0391 - ACC: 0.9890 - val_loss: 0.0273 - val_ACC: 0.9923\n",
      "Epoch 536/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0829 - ACC: 0.9720\n",
      "Epoch 00536: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0377 - ACC: 0.9888 - val_loss: 0.0381 - val_ACC: 0.9862\n",
      "Epoch 537/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0502 - ACC: 0.9920\n",
      "Epoch 00537: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0386 - ACC: 0.9890 - val_loss: 0.0405 - val_ACC: 0.9846\n",
      "Epoch 538/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0186 - ACC: 0.9940\n",
      "Epoch 00538: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0381 - ACC: 0.9898 - val_loss: 0.0218 - val_ACC: 0.9938\n",
      "Epoch 539/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0540 - ACC: 0.9900\n",
      "Epoch 00539: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0386 - ACC: 0.9890 - val_loss: 0.0199 - val_ACC: 0.9938\n",
      "Epoch 540/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0621 - ACC: 0.9780\n",
      "Epoch 00540: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9883 - val_loss: 0.0381 - val_ACC: 0.9854\n",
      "Epoch 541/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0323 - ACC: 0.9940\n",
      "Epoch 00541: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9892 - val_loss: 0.0228 - val_ACC: 0.9938\n",
      "Epoch 542/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0463 - ACC: 0.9900\n",
      "Epoch 00542: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0386 - ACC: 0.9900 - val_loss: 0.0294 - val_ACC: 0.9931\n",
      "Epoch 543/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0474 - ACC: 0.9860\n",
      "Epoch 00543: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0382 - ACC: 0.9898 - val_loss: 0.0277 - val_ACC: 0.9931\n",
      "Epoch 544/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0328 - ACC: 0.9940\n",
      "Epoch 00544: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0382 - ACC: 0.9894 - val_loss: 0.0238 - val_ACC: 0.9938\n",
      "Epoch 545/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0701 - ACC: 0.9800\n",
      "Epoch 00545: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9881 - val_loss: 0.0416 - val_ACC: 0.9846\n",
      "Epoch 546/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0245 - ACC: 0.9920\n",
      "Epoch 00546: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0379 - ACC: 0.9902 - val_loss: 0.0335 - val_ACC: 0.9931\n",
      "Epoch 547/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0290 - ACC: 0.9880\n",
      "Epoch 00547: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0388 - ACC: 0.9885 - val_loss: 0.0286 - val_ACC: 0.9931\n",
      "Epoch 548/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0447 - ACC: 0.9880\n",
      "Epoch 00548: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0379 - ACC: 0.9902 - val_loss: 0.0288 - val_ACC: 0.9931\n",
      "Epoch 549/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0472 - ACC: 0.9860\n",
      "Epoch 00549: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0382 - ACC: 0.9894 - val_loss: 0.0444 - val_ACC: 0.9854\n",
      "Epoch 550/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0240 - ACC: 0.9940\n",
      "Epoch 00550: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0390 - ACC: 0.9910 - val_loss: 0.0238 - val_ACC: 0.9938\n",
      "Epoch 551/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0266 - ACC: 0.9900\n",
      "Epoch 00551: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0450 - ACC: 0.9860 - val_loss: 0.0231 - val_ACC: 0.9938\n",
      "Epoch 552/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0477 - ACC: 0.9860\n",
      "Epoch 00552: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9898 - val_loss: 0.0359 - val_ACC: 0.9885\n",
      "Epoch 553/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0265 - ACC: 0.9940\n",
      "Epoch 00553: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0416 - ACC: 0.9879 - val_loss: 0.0615 - val_ACC: 0.9815\n",
      "Epoch 554/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0476 - ACC: 0.9860\n",
      "Epoch 00554: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0405 - ACC: 0.9896 - val_loss: 0.0233 - val_ACC: 0.9938\n",
      "Epoch 555/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0439 - ACC: 0.9880\n",
      "Epoch 00555: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0377 - ACC: 0.9885 - val_loss: 0.0261 - val_ACC: 0.9938\n",
      "Epoch 556/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0447 - ACC: 0.9900\n",
      "Epoch 00556: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9902 - val_loss: 0.0255 - val_ACC: 0.9938\n",
      "Epoch 557/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0299 - ACC: 0.9880\n",
      "Epoch 00557: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0375 - ACC: 0.9908 - val_loss: 0.0415 - val_ACC: 0.9869\n",
      "Epoch 558/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0179 - ACC: 0.9960\n",
      "Epoch 00558: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0376 - ACC: 0.9902 - val_loss: 0.0258 - val_ACC: 0.9931\n",
      "Epoch 559/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0213 - ACC: 0.9960\n",
      "Epoch 00559: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0386 - ACC: 0.9892 - val_loss: 0.0268 - val_ACC: 0.9938\n",
      "Epoch 560/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0223 - ACC: 0.9960\n",
      "Epoch 00560: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0381 - ACC: 0.9894 - val_loss: 0.0331 - val_ACC: 0.9892\n",
      "Epoch 561/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0640 - ACC: 0.9860\n",
      "Epoch 00561: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0373 - ACC: 0.9900 - val_loss: 0.0365 - val_ACC: 0.9885\n",
      "Epoch 562/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0150 - ACC: 0.9960\n",
      "Epoch 00562: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0406 - ACC: 0.9879 - val_loss: 0.0292 - val_ACC: 0.9931\n",
      "Epoch 563/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0332 - ACC: 0.9880\n",
      "Epoch 00563: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0378 - ACC: 0.9906 - val_loss: 0.0288 - val_ACC: 0.9931\n",
      "Epoch 564/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0509 - ACC: 0.9840\n",
      "Epoch 00564: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0371 - ACC: 0.9906 - val_loss: 0.0306 - val_ACC: 0.9931\n",
      "Epoch 565/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0373 - ACC: 0.9900\n",
      "Epoch 00565: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0374 - ACC: 0.9900 - val_loss: 0.0267 - val_ACC: 0.9938\n",
      "Epoch 566/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0543 - ACC: 0.9900\n",
      "Epoch 00566: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9890 - val_loss: 0.0558 - val_ACC: 0.9823\n",
      "Epoch 567/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0172 - ACC: 0.9960\n",
      "Epoch 00567: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9892 - val_loss: 0.0378 - val_ACC: 0.9877\n",
      "Epoch 568/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0447 - ACC: 0.9880\n",
      "Epoch 00568: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0386 - ACC: 0.9892 - val_loss: 0.0340 - val_ACC: 0.9892\n",
      "Epoch 569/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0226 - ACC: 0.9940\n",
      "Epoch 00569: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0400 - ACC: 0.9892 - val_loss: 0.0218 - val_ACC: 0.9938\n",
      "Epoch 570/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0419 - ACC: 0.9880\n",
      "Epoch 00570: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0372 - ACC: 0.9900 - val_loss: 0.0144 - val_ACC: 0.9954\n",
      "Epoch 571/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0348 - ACC: 0.9920\n",
      "Epoch 00571: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0427 - ACC: 0.9888 - val_loss: 0.0333 - val_ACC: 0.9892\n",
      "Epoch 572/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0198 - ACC: 0.9940\n",
      "Epoch 00572: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0391 - ACC: 0.9908 - val_loss: 0.0359 - val_ACC: 0.9892\n",
      "Epoch 573/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0618 - ACC: 0.9840\n",
      "Epoch 00573: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0396 - ACC: 0.9886 - val_loss: 0.0586 - val_ACC: 0.9762\n",
      "Epoch 574/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0643 - ACC: 0.9860\n",
      "Epoch 00574: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0395 - ACC: 0.9890 - val_loss: 0.0266 - val_ACC: 0.9908\n",
      "Epoch 575/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0443 - ACC: 0.9880\n",
      "Epoch 00575: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0416 - ACC: 0.9875 - val_loss: 0.0179 - val_ACC: 0.9938\n",
      "Epoch 576/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0436 - ACC: 0.9900\n",
      "Epoch 00576: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0448 - ACC: 0.9873 - val_loss: 0.0828 - val_ACC: 0.9731\n",
      "Epoch 577/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0740 - ACC: 0.9760\n",
      "Epoch 00577: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0404 - ACC: 0.9892 - val_loss: 0.0282 - val_ACC: 0.9931\n",
      "Epoch 578/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0458 - ACC: 0.9880\n",
      "Epoch 00578: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9898 - val_loss: 0.0265 - val_ACC: 0.9931\n",
      "Epoch 579/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0155 - ACC: 0.9940\n",
      "Epoch 00579: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0368 - ACC: 0.9904 - val_loss: 0.0305 - val_ACC: 0.9908\n",
      "Epoch 580/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0597 - ACC: 0.9760\n",
      "Epoch 00580: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9904 - val_loss: 0.0493 - val_ACC: 0.9831\n",
      "Epoch 581/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0417 - ACC: 0.9860\n",
      "Epoch 00581: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9894 - val_loss: 0.0308 - val_ACC: 0.9892\n",
      "Epoch 582/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0471 - ACC: 0.9900\n",
      "Epoch 00582: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0372 - ACC: 0.9900 - val_loss: 0.0357 - val_ACC: 0.9885\n",
      "Epoch 583/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0514 - ACC: 0.9880\n",
      "Epoch 00583: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0370 - ACC: 0.9896 - val_loss: 0.0252 - val_ACC: 0.9938\n",
      "Epoch 584/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0543 - ACC: 0.9840\n",
      "Epoch 00584: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0375 - ACC: 0.9898 - val_loss: 0.0301 - val_ACC: 0.9931\n",
      "Epoch 585/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0355 - ACC: 0.9900\n",
      "Epoch 00585: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0388 - ACC: 0.9898 - val_loss: 0.0280 - val_ACC: 0.9931\n",
      "Epoch 586/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0587 - ACC: 0.9840\n",
      "Epoch 00586: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0364 - ACC: 0.9911 - val_loss: 0.0165 - val_ACC: 0.9938\n",
      "Epoch 587/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0473 - ACC: 0.9860\n",
      "Epoch 00587: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0404 - ACC: 0.9890 - val_loss: 0.0400 - val_ACC: 0.9877\n",
      "Epoch 588/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0256 - ACC: 0.9920\n",
      "Epoch 00588: val_loss did not improve from 0.01346\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0371 - ACC: 0.9896 - val_loss: 0.0292 - val_ACC: 0.9923\n",
      "Epoch 589/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0318 - ACC: 0.9940\n",
      "Epoch 00589: val_loss improved from 0.01346 to 0.01052, saving model to ./model\\589 - 0.0105.hdf5\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0381 - ACC: 0.9896 - val_loss: 0.0105 - val_ACC: 0.9969\n",
      "Epoch 590/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0565 - ACC: 0.9820\n",
      "Epoch 00590: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0509 - ACC: 0.9844 - val_loss: 0.0460 - val_ACC: 0.9831\n",
      "Epoch 591/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0544 - ACC: 0.9880\n",
      "Epoch 00591: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0438 - ACC: 0.9869 - val_loss: 0.0465 - val_ACC: 0.9831\n",
      "Epoch 592/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0686 - ACC: 0.9860\n",
      "Epoch 00592: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0374 - ACC: 0.9902 - val_loss: 0.0404 - val_ACC: 0.9877\n",
      "Epoch 593/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0322 - ACC: 0.9940\n",
      "Epoch 00593: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0379 - ACC: 0.9894 - val_loss: 0.0245 - val_ACC: 0.9938\n",
      "Epoch 594/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0335 - ACC: 0.9880\n",
      "Epoch 00594: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0382 - ACC: 0.9890 - val_loss: 0.0289 - val_ACC: 0.9915\n",
      "Epoch 595/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0295 - ACC: 0.9900\n",
      "Epoch 00595: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0378 - ACC: 0.9906 - val_loss: 0.0351 - val_ACC: 0.9892\n",
      "Epoch 596/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0300 - ACC: 0.9940\n",
      "Epoch 00596: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9896 - val_loss: 0.0557 - val_ACC: 0.9823\n",
      "Epoch 597/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0447 - ACC: 0.9880\n",
      "Epoch 00597: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9894 - val_loss: 0.0200 - val_ACC: 0.9938\n",
      "Epoch 598/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0468 - ACC: 0.9840\n",
      "Epoch 00598: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0369 - ACC: 0.9898 - val_loss: 0.0394 - val_ACC: 0.9877\n",
      "Epoch 599/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0374 - ACC: 0.9880\n",
      "Epoch 00599: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0369 - ACC: 0.9904 - val_loss: 0.0270 - val_ACC: 0.9938\n",
      "Epoch 600/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0335 - ACC: 0.9960\n",
      "Epoch 00600: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9904 - val_loss: 0.0162 - val_ACC: 0.9954\n",
      "Epoch 601/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0432 - ACC: 0.9860\n",
      "Epoch 00601: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0463 - ACC: 0.9863 - val_loss: 0.0344 - val_ACC: 0.9892\n",
      "Epoch 602/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0366 - ACC: 0.9920\n",
      "Epoch 00602: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0391 - ACC: 0.9888 - val_loss: 0.0355 - val_ACC: 0.9885\n",
      "Epoch 603/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0702 - ACC: 0.9800\n",
      "Epoch 00603: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0387 - ACC: 0.9888 - val_loss: 0.0151 - val_ACC: 0.9954\n",
      "Epoch 604/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0135 - ACC: 0.9980\n",
      "Epoch 00604: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0443 - ACC: 0.9863 - val_loss: 0.0183 - val_ACC: 0.9938\n",
      "Epoch 605/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0367 - ACC: 0.9860\n",
      "Epoch 00605: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9896 - val_loss: 0.0382 - val_ACC: 0.9862\n",
      "Epoch 606/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0440 - ACC: 0.9840\n",
      "Epoch 00606: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9908 - val_loss: 0.0306 - val_ACC: 0.9908\n",
      "Epoch 607/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0133 - ACC: 0.9960\n",
      "Epoch 00607: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0363 - ACC: 0.9904 - val_loss: 0.0458 - val_ACC: 0.9854\n",
      "Epoch 608/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0219 - ACC: 0.9960\n",
      "Epoch 00608: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0394 - ACC: 0.9902 - val_loss: 0.0316 - val_ACC: 0.9931\n",
      "Epoch 609/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0338 - ACC: 0.9920\n",
      "Epoch 00609: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9904 - val_loss: 0.0322 - val_ACC: 0.9931\n",
      "Epoch 610/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0356 - ACC: 0.9840\n",
      "Epoch 00610: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0368 - ACC: 0.9900 - val_loss: 0.0376 - val_ACC: 0.9892\n",
      "Epoch 611/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0365 - ACC: 0.9920\n",
      "Epoch 00611: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0376 - ACC: 0.9904 - val_loss: 0.0245 - val_ACC: 0.9938\n",
      "Epoch 612/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0420 - ACC: 0.9900\n",
      "Epoch 00612: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0375 - ACC: 0.9890 - val_loss: 0.0333 - val_ACC: 0.9900\n",
      "Epoch 613/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0432 - ACC: 0.9920\n",
      "Epoch 00613: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9896 - val_loss: 0.0456 - val_ACC: 0.9854\n",
      "Epoch 614/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0361 - ACC: 0.9900\n",
      "Epoch 00614: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0393 - ACC: 0.9892 - val_loss: 0.0255 - val_ACC: 0.9938\n",
      "Epoch 615/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0437 - ACC: 0.9900\n",
      "Epoch 00615: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0376 - ACC: 0.9906 - val_loss: 0.0217 - val_ACC: 0.9938\n",
      "Epoch 616/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0498 - ACC: 0.9840\n",
      "Epoch 00616: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0368 - ACC: 0.9890 - val_loss: 0.0249 - val_ACC: 0.9938\n",
      "Epoch 617/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0346 - ACC: 0.9900\n",
      "Epoch 00617: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0361 - ACC: 0.9898 - val_loss: 0.0327 - val_ACC: 0.9892\n",
      "Epoch 618/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0440 - ACC: 0.9880\n",
      "Epoch 00618: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0364 - ACC: 0.9915 - val_loss: 0.0197 - val_ACC: 0.9938\n",
      "Epoch 619/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0258 - ACC: 0.9940\n",
      "Epoch 00619: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0420 - ACC: 0.9861 - val_loss: 0.0221 - val_ACC: 0.9938\n",
      "Epoch 620/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0747 - ACC: 0.9840\n",
      "Epoch 00620: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0398 - ACC: 0.9877 - val_loss: 0.0459 - val_ACC: 0.9854\n",
      "Epoch 621/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0342 - ACC: 0.9940\n",
      "Epoch 00621: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0429 - ACC: 0.9869 - val_loss: 0.0261 - val_ACC: 0.9938\n",
      "Epoch 622/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0534 - ACC: 0.9840\n",
      "Epoch 00622: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0370 - ACC: 0.9904 - val_loss: 0.0280 - val_ACC: 0.9938\n",
      "Epoch 623/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0281 - ACC: 0.9900\n",
      "Epoch 00623: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9908 - val_loss: 0.0274 - val_ACC: 0.9908\n",
      "Epoch 624/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0369 - ACC: 0.9920\n",
      "Epoch 00624: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0362 - ACC: 0.9910 - val_loss: 0.0284 - val_ACC: 0.9938\n",
      "Epoch 625/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0595 - ACC: 0.9840\n",
      "Epoch 00625: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0358 - ACC: 0.9913 - val_loss: 0.0193 - val_ACC: 0.9938\n",
      "Epoch 626/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0266 - ACC: 0.9900\n",
      "Epoch 00626: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0364 - ACC: 0.9911 - val_loss: 0.0279 - val_ACC: 0.9908\n",
      "Epoch 627/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0402 - ACC: 0.9880\n",
      "Epoch 00627: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0358 - ACC: 0.9908 - val_loss: 0.0386 - val_ACC: 0.9869\n",
      "Epoch 628/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0542 - ACC: 0.9840\n",
      "Epoch 00628: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0374 - ACC: 0.9902 - val_loss: 0.0461 - val_ACC: 0.9854\n",
      "Epoch 629/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0392 - ACC: 0.9880\n",
      "Epoch 00629: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0390 - ACC: 0.9877 - val_loss: 0.0206 - val_ACC: 0.9938\n",
      "Epoch 630/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0346 - ACC: 0.9900\n",
      "Epoch 00630: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0406 - ACC: 0.9894 - val_loss: 0.0438 - val_ACC: 0.9862\n",
      "Epoch 631/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0345 - ACC: 0.9900\n",
      "Epoch 00631: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0425 - ACC: 0.9881 - val_loss: 0.0164 - val_ACC: 0.9946\n",
      "Epoch 632/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0408 - ACC: 0.9840\n",
      "Epoch 00632: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0389 - ACC: 0.9888 - val_loss: 0.0340 - val_ACC: 0.9900\n",
      "Epoch 633/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0440 - ACC: 0.9900\n",
      "Epoch 00633: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0365 - ACC: 0.9898 - val_loss: 0.0259 - val_ACC: 0.9938\n",
      "Epoch 634/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0251 - ACC: 0.9940\n",
      "Epoch 00634: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0383 - ACC: 0.9896 - val_loss: 0.0587 - val_ACC: 0.9846\n",
      "Epoch 635/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0609 - ACC: 0.9820\n",
      "Epoch 00635: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0437 - ACC: 0.9873 - val_loss: 0.0275 - val_ACC: 0.9938\n",
      "Epoch 636/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0353 - ACC: 0.9860\n",
      "Epoch 00636: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0409 - ACC: 0.9869 - val_loss: 0.0157 - val_ACC: 0.9946\n",
      "Epoch 637/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0267 - ACC: 0.9940\n",
      "Epoch 00637: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0460 - ACC: 0.9863 - val_loss: 0.0308 - val_ACC: 0.9923\n",
      "Epoch 638/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0233 - ACC: 0.9940\n",
      "Epoch 00638: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0388 - ACC: 0.9896 - val_loss: 0.0361 - val_ACC: 0.9892\n",
      "Epoch 639/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0493 - ACC: 0.9900\n",
      "Epoch 00639: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0360 - ACC: 0.9911 - val_loss: 0.0301 - val_ACC: 0.9908\n",
      "Epoch 640/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0648 - ACC: 0.9820\n",
      "Epoch 00640: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0357 - ACC: 0.9913 - val_loss: 0.0283 - val_ACC: 0.9908\n",
      "Epoch 641/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0198 - ACC: 0.9940\n",
      "Epoch 00641: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0357 - ACC: 0.9906 - val_loss: 0.0283 - val_ACC: 0.9931\n",
      "Epoch 642/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0453 - ACC: 0.9920\n",
      "Epoch 00642: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0358 - ACC: 0.9911 - val_loss: 0.0266 - val_ACC: 0.9938\n",
      "Epoch 643/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0536 - ACC: 0.9880\n",
      "Epoch 00643: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0372 - ACC: 0.9906 - val_loss: 0.0424 - val_ACC: 0.9869\n",
      "Epoch 644/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0248 - ACC: 0.9940\n",
      "Epoch 00644: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0354 - ACC: 0.9908 - val_loss: 0.0453 - val_ACC: 0.9854\n",
      "Epoch 645/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0314 - ACC: 0.9880\n",
      "Epoch 00645: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0361 - ACC: 0.9910 - val_loss: 0.0477 - val_ACC: 0.9854\n",
      "Epoch 646/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0232 - ACC: 0.9980\n",
      "Epoch 00646: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9906 - val_loss: 0.0287 - val_ACC: 0.9938\n",
      "Epoch 647/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0172 - ACC: 0.9940\n",
      "Epoch 00647: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0363 - ACC: 0.9906 - val_loss: 0.0239 - val_ACC: 0.9938\n",
      "Epoch 648/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0684 - ACC: 0.9840\n",
      "Epoch 00648: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0359 - ACC: 0.9904 - val_loss: 0.0224 - val_ACC: 0.9938\n",
      "Epoch 649/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0380 - ACC: 0.9860\n",
      "Epoch 00649: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0381 - ACC: 0.9892 - val_loss: 0.0548 - val_ACC: 0.9823\n",
      "Epoch 650/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0199 - ACC: 0.9940\n",
      "Epoch 00650: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9894 - val_loss: 0.0428 - val_ACC: 0.9854\n",
      "Epoch 651/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0594 - ACC: 0.9800\n",
      "Epoch 00651: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0356 - ACC: 0.9910 - val_loss: 0.0330 - val_ACC: 0.9892\n",
      "Epoch 652/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0212 - ACC: 0.9960\n",
      "Epoch 00652: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0353 - ACC: 0.9910 - val_loss: 0.0320 - val_ACC: 0.9900\n",
      "Epoch 653/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0133 - ACC: 0.9980\n",
      "Epoch 00653: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0357 - ACC: 0.9906 - val_loss: 0.0341 - val_ACC: 0.9892\n",
      "Epoch 654/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0528 - ACC: 0.9880\n",
      "Epoch 00654: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9896 - val_loss: 0.0437 - val_ACC: 0.9862\n",
      "Epoch 655/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0363 - ACC: 0.9920\n",
      "Epoch 00655: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0366 - ACC: 0.9908 - val_loss: 0.0196 - val_ACC: 0.9938\n",
      "Epoch 656/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0297 - ACC: 0.9940\n",
      "Epoch 00656: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0407 - ACC: 0.9888 - val_loss: 0.0398 - val_ACC: 0.9854\n",
      "Epoch 657/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0199 - ACC: 0.9940\n",
      "Epoch 00657: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0438 - ACC: 0.9869 - val_loss: 0.0366 - val_ACC: 0.9885\n",
      "Epoch 658/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0323 - ACC: 0.9940\n",
      "Epoch 00658: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0460 - ACC: 0.9865 - val_loss: 0.0156 - val_ACC: 0.9946\n",
      "Epoch 659/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0403 - ACC: 0.9880\n",
      "Epoch 00659: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0412 - ACC: 0.9883 - val_loss: 0.0401 - val_ACC: 0.9892\n",
      "Epoch 660/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0346 - ACC: 0.9940\n",
      "Epoch 00660: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0364 - ACC: 0.9913 - val_loss: 0.0266 - val_ACC: 0.9938\n",
      "Epoch 661/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0445 - ACC: 0.9940\n",
      "Epoch 00661: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0356 - ACC: 0.9910 - val_loss: 0.0303 - val_ACC: 0.9908\n",
      "Epoch 662/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0485 - ACC: 0.9880\n",
      "Epoch 00662: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0380 - ACC: 0.9902 - val_loss: 0.0524 - val_ACC: 0.9846\n",
      "Epoch 663/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0546 - ACC: 0.9780\n",
      "Epoch 00663: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0378 - ACC: 0.9883 - val_loss: 0.0292 - val_ACC: 0.9938\n",
      "Epoch 664/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0447 - ACC: 0.9900\n",
      "Epoch 00664: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0365 - ACC: 0.9894 - val_loss: 0.0202 - val_ACC: 0.9938\n",
      "Epoch 665/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0198 - ACC: 0.9920\n",
      "Epoch 00665: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0371 - ACC: 0.9890 - val_loss: 0.0205 - val_ACC: 0.9938\n",
      "Epoch 666/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0387 - ACC: 0.9840\n",
      "Epoch 00666: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0376 - ACC: 0.9898 - val_loss: 0.0434 - val_ACC: 0.9869\n",
      "Epoch 667/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0552 - ACC: 0.9840\n",
      "Epoch 00667: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9896 - val_loss: 0.0272 - val_ACC: 0.9938\n",
      "Epoch 668/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0322 - ACC: 0.9940\n",
      "Epoch 00668: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0359 - ACC: 0.9900 - val_loss: 0.0417 - val_ACC: 0.9862\n",
      "Epoch 669/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0206 - ACC: 0.9980\n",
      "Epoch 00669: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0375 - ACC: 0.9898 - val_loss: 0.0323 - val_ACC: 0.9908\n",
      "Epoch 670/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0829 - ACC: 0.9800\n",
      "Epoch 00670: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0402 - ACC: 0.9877 - val_loss: 0.0242 - val_ACC: 0.9908\n",
      "Epoch 671/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0201 - ACC: 0.9940\n",
      "Epoch 00671: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0386 - ACC: 0.9892 - val_loss: 0.0560 - val_ACC: 0.9838\n",
      "Epoch 672/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0450 - ACC: 0.9860\n",
      "Epoch 00672: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0384 - ACC: 0.9898 - val_loss: 0.0314 - val_ACC: 0.9931\n",
      "Epoch 673/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0209 - ACC: 0.9920\n",
      "Epoch 00673: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0361 - ACC: 0.9904 - val_loss: 0.0433 - val_ACC: 0.9862\n",
      "Epoch 674/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0398 - ACC: 0.9920\n",
      "Epoch 00674: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0388 - ACC: 0.9896 - val_loss: 0.0370 - val_ACC: 0.9869\n",
      "Epoch 675/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0538 - ACC: 0.9880\n",
      "Epoch 00675: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0361 - ACC: 0.9906 - val_loss: 0.0233 - val_ACC: 0.9938\n",
      "Epoch 676/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0233 - ACC: 0.9940\n",
      "Epoch 00676: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0356 - ACC: 0.9910 - val_loss: 0.0260 - val_ACC: 0.9938\n",
      "Epoch 677/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0433 - ACC: 0.9900\n",
      "Epoch 00677: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0351 - ACC: 0.9902 - val_loss: 0.0283 - val_ACC: 0.9908\n",
      "Epoch 678/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0509 - ACC: 0.9880\n",
      "Epoch 00678: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0355 - ACC: 0.9915 - val_loss: 0.0244 - val_ACC: 0.9938\n",
      "Epoch 679/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0377 - ACC: 0.9900\n",
      "Epoch 00679: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0359 - ACC: 0.9902 - val_loss: 0.0205 - val_ACC: 0.9938\n",
      "Epoch 680/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0442 - ACC: 0.9900\n",
      "Epoch 00680: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0385 - ACC: 0.9888 - val_loss: 0.0386 - val_ACC: 0.9869\n",
      "Epoch 681/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0367 - ACC: 0.9920\n",
      "Epoch 00681: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0433 - ACC: 0.9881 - val_loss: 0.0661 - val_ACC: 0.9762\n",
      "Epoch 682/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0449 - ACC: 0.9880\n",
      "Epoch 00682: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0433 - ACC: 0.9883 - val_loss: 0.0329 - val_ACC: 0.9931\n",
      "Epoch 683/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0133 - ACC: 0.9960\n",
      "Epoch 00683: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0365 - ACC: 0.9902 - val_loss: 0.0405 - val_ACC: 0.9877\n",
      "Epoch 684/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0113 - ACC: 0.9980\n",
      "Epoch 00684: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0355 - ACC: 0.9913 - val_loss: 0.0479 - val_ACC: 0.9838\n",
      "Epoch 685/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0297 - ACC: 0.9860\n",
      "Epoch 00685: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0363 - ACC: 0.9908 - val_loss: 0.0319 - val_ACC: 0.9900\n",
      "Epoch 686/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0167 - ACC: 0.9960\n",
      "Epoch 00686: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0360 - ACC: 0.9908 - val_loss: 0.0404 - val_ACC: 0.9869\n",
      "Epoch 687/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0411 - ACC: 0.9900\n",
      "Epoch 00687: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0372 - ACC: 0.9900 - val_loss: 0.0213 - val_ACC: 0.9938\n",
      "Epoch 688/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0520 - ACC: 0.9880\n",
      "Epoch 00688: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9908 - val_loss: 0.0298 - val_ACC: 0.9938\n",
      "Epoch 689/2000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.0330 - ACC: 0.9920\n",
      "Epoch 00689: val_loss did not improve from 0.01052\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0367 - ACC: 0.9894 - val_loss: 0.0513 - val_ACC: 0.9846\n",
      "204/204 [==============================] - 0s 667us/step - loss: 0.0410 - ACC: 0.9885\n",
      "\n",
      " ACC : 0.9885\n"
     ]
    }
   ],
   "source": [
    "# early stopping & model save\n",
    "early_stopping_callback = EarlyStopping(monitor = \"val_loss\", patience = 100)\n",
    "\n",
    "model.fit(X, Y, validation_split = 0.2, epochs = 2000, batch_size = 500, \n",
    "         callbacks = [early_stopping_callback, checkpointer])\n",
    "\n",
    "print(\"\\n ACC : %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8c214f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.1835 - ACC: 0.6440\n",
      "Epoch 00001: val_loss improved from inf to 5.58841, saving model to ./pima_model\\01 - 5.5884.hdf5\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 5.1909 - ACC: 0.6450 - val_loss: 5.5884 - val_ACC: 0.5649\n",
      "Epoch 2/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.5996 - ACC: 0.6500\n",
      "Epoch 00002: val_loss improved from 5.58841 to 5.00745, saving model to ./pima_model\\02 - 5.0074.hdf5\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.5951 - ACC: 0.6531 - val_loss: 5.0074 - val_ACC: 0.5390\n",
      "Epoch 3/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.0522 - ACC: 0.6540\n",
      "Epoch 00003: val_loss improved from 5.00745 to 4.46276, saving model to ./pima_model\\03 - 4.4628.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.0928 - ACC: 0.6401 - val_loss: 4.4628 - val_ACC: 0.5130\n",
      "Epoch 4/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.1284 - ACC: 0.6040\n",
      "Epoch 00004: val_loss improved from 4.46276 to 3.99441, saving model to ./pima_model\\04 - 3.9944.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3.7126 - ACC: 0.6270 - val_loss: 3.9944 - val_ACC: 0.5130\n",
      "Epoch 5/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4600 - ACC: 0.5980\n",
      "Epoch 00005: val_loss improved from 3.99441 to 3.62353, saving model to ./pima_model\\05 - 3.6235.hdf5\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 3.3638 - ACC: 0.5977 - val_loss: 3.6235 - val_ACC: 0.5065\n",
      "Epoch 6/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1741 - ACC: 0.5680\n",
      "Epoch 00006: val_loss improved from 3.62353 to 3.30428, saving model to ./pima_model\\06 - 3.3043.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3.0922 - ACC: 0.5717 - val_loss: 3.3043 - val_ACC: 0.4935\n",
      "Epoch 7/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9801 - ACC: 0.5560\n",
      "Epoch 00007: val_loss improved from 3.30428 to 3.01113, saving model to ./pima_model\\07 - 3.0111.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.8508 - ACC: 0.5489 - val_loss: 3.0111 - val_ACC: 0.4870\n",
      "Epoch 8/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6772 - ACC: 0.5140\n",
      "Epoch 00008: val_loss improved from 3.01113 to 2.74499, saving model to ./pima_model\\08 - 2.7450.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.6163 - ACC: 0.5195 - val_loss: 2.7450 - val_ACC: 0.4675\n",
      "Epoch 9/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4185 - ACC: 0.5100\n",
      "Epoch 00009: val_loss improved from 2.74499 to 2.48311, saving model to ./pima_model\\09 - 2.4831.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.3867 - ACC: 0.5114 - val_loss: 2.4831 - val_ACC: 0.4545\n",
      "Epoch 10/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.2666 - ACC: 0.4740\n",
      "Epoch 00010: val_loss improved from 2.48311 to 2.23917, saving model to ./pima_model\\10 - 2.2392.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.1750 - ACC: 0.4870 - val_loss: 2.2392 - val_ACC: 0.4545\n",
      "Epoch 11/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.0200 - ACC: 0.4500\n",
      "Epoch 00011: val_loss improved from 2.23917 to 2.01420, saving model to ./pima_model\\11 - 2.0142.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.9678 - ACC: 0.4625 - val_loss: 2.0142 - val_ACC: 0.4351\n",
      "Epoch 12/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.8049 - ACC: 0.4480\n",
      "Epoch 00012: val_loss improved from 2.01420 to 1.80476, saving model to ./pima_model\\12 - 1.8048.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.7701 - ACC: 0.4528 - val_loss: 1.8048 - val_ACC: 0.4221\n",
      "Epoch 13/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.6480 - ACC: 0.4380\n",
      "Epoch 00013: val_loss improved from 1.80476 to 1.61552, saving model to ./pima_model\\13 - 1.6155.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.5847 - ACC: 0.4446 - val_loss: 1.6155 - val_ACC: 0.4221\n",
      "Epoch 14/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.4653 - ACC: 0.4300\n",
      "Epoch 00014: val_loss improved from 1.61552 to 1.44545, saving model to ./pima_model\\14 - 1.4455.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.4116 - ACC: 0.4365 - val_loss: 1.4455 - val_ACC: 0.3831\n",
      "Epoch 15/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3390 - ACC: 0.4100\n",
      "Epoch 00015: val_loss improved from 1.44545 to 1.27065, saving model to ./pima_model\\15 - 1.2706.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.2551 - ACC: 0.4202 - val_loss: 1.2706 - val_ACC: 0.3506\n",
      "Epoch 16/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0567 - ACC: 0.4060\n",
      "Epoch 00016: val_loss improved from 1.27065 to 1.11020, saving model to ./pima_model\\16 - 1.1102.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.0993 - ACC: 0.3909 - val_loss: 1.1102 - val_ACC: 0.3312\n",
      "Epoch 17/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0085 - ACC: 0.3720\n",
      "Epoch 00017: val_loss improved from 1.11020 to 0.98582, saving model to ./pima_model\\17 - 0.9858.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.9854 - ACC: 0.3876 - val_loss: 0.9858 - val_ACC: 0.3247\n",
      "Epoch 18/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8950 - ACC: 0.3820\n",
      "Epoch 00018: val_loss improved from 0.98582 to 0.91481, saving model to ./pima_model\\18 - 0.9148.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.8995 - ACC: 0.3844 - val_loss: 0.9148 - val_ACC: 0.3571\n",
      "Epoch 19/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8775 - ACC: 0.3780\n",
      "Epoch 00019: val_loss improved from 0.91481 to 0.86519, saving model to ./pima_model\\19 - 0.8652.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8602 - ACC: 0.4088 - val_loss: 0.8652 - val_ACC: 0.4675\n",
      "Epoch 20/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8474 - ACC: 0.5380\n",
      "Epoch 00020: val_loss improved from 0.86519 to 0.82029, saving model to ./pima_model\\20 - 0.8203.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.8386 - ACC: 0.5326 - val_loss: 0.8203 - val_ACC: 0.4740\n",
      "Epoch 21/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8111 - ACC: 0.5480\n",
      "Epoch 00021: val_loss improved from 0.82029 to 0.78084, saving model to ./pima_model\\21 - 0.7808.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.8099 - ACC: 0.5586 - val_loss: 0.7808 - val_ACC: 0.4870\n",
      "Epoch 22/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8030 - ACC: 0.5840\n",
      "Epoch 00022: val_loss improved from 0.78084 to 0.75188, saving model to ./pima_model\\22 - 0.7519.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7821 - ACC: 0.5879 - val_loss: 0.7519 - val_ACC: 0.5000\n",
      "Epoch 23/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7616 - ACC: 0.6140\n",
      "Epoch 00023: val_loss improved from 0.75188 to 0.73317, saving model to ./pima_model\\23 - 0.7332.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7587 - ACC: 0.6189 - val_loss: 0.7332 - val_ACC: 0.5455\n",
      "Epoch 24/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7473 - ACC: 0.6480\n",
      "Epoch 00024: val_loss improved from 0.73317 to 0.72006, saving model to ./pima_model\\24 - 0.7201.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7437 - ACC: 0.6466 - val_loss: 0.7201 - val_ACC: 0.5519\n",
      "Epoch 25/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7421 - ACC: 0.6580\n",
      "Epoch 00025: val_loss improved from 0.72006 to 0.71308, saving model to ./pima_model\\25 - 0.7131.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7343 - ACC: 0.6596 - val_loss: 0.7131 - val_ACC: 0.5519\n",
      "Epoch 26/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7332 - ACC: 0.6760\n",
      "Epoch 00026: val_loss improved from 0.71308 to 0.70834, saving model to ./pima_model\\26 - 0.7083.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7288 - ACC: 0.6678 - val_loss: 0.7083 - val_ACC: 0.5519\n",
      "Epoch 27/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7186 - ACC: 0.6540\n",
      "Epoch 00027: val_loss improved from 0.70834 to 0.70330, saving model to ./pima_model\\27 - 0.7033.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7256 - ACC: 0.6629 - val_loss: 0.7033 - val_ACC: 0.5844\n",
      "Epoch 28/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7245 - ACC: 0.6600\n",
      "Epoch 00028: val_loss improved from 0.70330 to 0.69966, saving model to ./pima_model\\28 - 0.6997.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7223 - ACC: 0.6580 - val_loss: 0.6997 - val_ACC: 0.6039\n",
      "Epoch 29/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7178 - ACC: 0.6440\n",
      "Epoch 00029: val_loss improved from 0.69966 to 0.69708, saving model to ./pima_model\\29 - 0.6971.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7192 - ACC: 0.6564 - val_loss: 0.6971 - val_ACC: 0.6039\n",
      "Epoch 30/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7156 - ACC: 0.6460\n",
      "Epoch 00030: val_loss improved from 0.69708 to 0.69479, saving model to ./pima_model\\30 - 0.6948.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7162 - ACC: 0.6564 - val_loss: 0.6948 - val_ACC: 0.6234\n",
      "Epoch 31/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7133 - ACC: 0.6520\n",
      "Epoch 00031: val_loss improved from 0.69479 to 0.69277, saving model to ./pima_model\\31 - 0.6928.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7136 - ACC: 0.6482 - val_loss: 0.6928 - val_ACC: 0.6364\n",
      "Epoch 32/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7040 - ACC: 0.6440\n",
      "Epoch 00032: val_loss improved from 0.69277 to 0.69120, saving model to ./pima_model\\32 - 0.6912.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7109 - ACC: 0.6482 - val_loss: 0.6912 - val_ACC: 0.6494\n",
      "Epoch 33/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7061 - ACC: 0.6580\n",
      "Epoch 00033: val_loss improved from 0.69120 to 0.68990, saving model to ./pima_model\\33 - 0.6899.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7081 - ACC: 0.6498 - val_loss: 0.6899 - val_ACC: 0.6558\n",
      "Epoch 34/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7059 - ACC: 0.6420\n",
      "Epoch 00034: val_loss improved from 0.68990 to 0.68861, saving model to ./pima_model\\34 - 0.6886.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7059 - ACC: 0.6466 - val_loss: 0.6886 - val_ACC: 0.6558\n",
      "Epoch 35/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7042 - ACC: 0.6460\n",
      "Epoch 00035: val_loss improved from 0.68861 to 0.68739, saving model to ./pima_model\\35 - 0.6874.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7036 - ACC: 0.6450 - val_loss: 0.6874 - val_ACC: 0.6558\n",
      "Epoch 36/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7055 - ACC: 0.6680\n",
      "Epoch 00036: val_loss improved from 0.68739 to 0.68602, saving model to ./pima_model\\36 - 0.6860.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7013 - ACC: 0.6531 - val_loss: 0.6860 - val_ACC: 0.6558\n",
      "Epoch 37/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6989 - ACC: 0.6720\n",
      "Epoch 00037: val_loss improved from 0.68602 to 0.68487, saving model to ./pima_model\\37 - 0.6849.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6998 - ACC: 0.6629 - val_loss: 0.6849 - val_ACC: 0.6558\n",
      "Epoch 38/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7026 - ACC: 0.6600\n",
      "Epoch 00038: val_loss improved from 0.68487 to 0.68421, saving model to ./pima_model\\38 - 0.6842.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6984 - ACC: 0.6629 - val_loss: 0.6842 - val_ACC: 0.6623\n",
      "Epoch 39/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6904 - ACC: 0.6620\n",
      "Epoch 00039: val_loss improved from 0.68421 to 0.68376, saving model to ./pima_model\\39 - 0.6838.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6971 - ACC: 0.6612 - val_loss: 0.6838 - val_ACC: 0.6558\n",
      "Epoch 40/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6979 - ACC: 0.6740\n",
      "Epoch 00040: val_loss improved from 0.68376 to 0.68344, saving model to ./pima_model\\40 - 0.6834.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6957 - ACC: 0.6645 - val_loss: 0.6834 - val_ACC: 0.6558\n",
      "Epoch 41/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6981 - ACC: 0.6780\n",
      "Epoch 00041: val_loss improved from 0.68344 to 0.68333, saving model to ./pima_model\\41 - 0.6833.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6943 - ACC: 0.6678 - val_loss: 0.6833 - val_ACC: 0.6623\n",
      "Epoch 42/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6942 - ACC: 0.6600\n",
      "Epoch 00042: val_loss did not improve from 0.68333\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6929 - ACC: 0.6678 - val_loss: 0.6834 - val_ACC: 0.6623\n",
      "Epoch 43/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6894 - ACC: 0.6720\n",
      "Epoch 00043: val_loss improved from 0.68333 to 0.68319, saving model to ./pima_model\\43 - 0.6832.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6918 - ACC: 0.6661 - val_loss: 0.6832 - val_ACC: 0.6623\n",
      "Epoch 44/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6850 - ACC: 0.6900\n",
      "Epoch 00044: val_loss improved from 0.68319 to 0.68269, saving model to ./pima_model\\44 - 0.6827.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6904 - ACC: 0.6694 - val_loss: 0.6827 - val_ACC: 0.6623\n",
      "Epoch 45/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6774 - ACC: 0.6740\n",
      "Epoch 00045: val_loss improved from 0.68269 to 0.68235, saving model to ./pima_model\\45 - 0.6823.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6895 - ACC: 0.6661 - val_loss: 0.6823 - val_ACC: 0.6623\n",
      "Epoch 46/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6869 - ACC: 0.6700\n",
      "Epoch 00046: val_loss improved from 0.68235 to 0.68215, saving model to ./pima_model\\46 - 0.6821.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6883 - ACC: 0.6645 - val_loss: 0.6821 - val_ACC: 0.6623\n",
      "Epoch 47/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6943 - ACC: 0.6560\n",
      "Epoch 00047: val_loss improved from 0.68215 to 0.68187, saving model to ./pima_model\\47 - 0.6819.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6872 - ACC: 0.6629 - val_loss: 0.6819 - val_ACC: 0.6623\n",
      "Epoch 48/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6884 - ACC: 0.6620\n",
      "Epoch 00048: val_loss improved from 0.68187 to 0.68138, saving model to ./pima_model\\48 - 0.6814.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6860 - ACC: 0.6612 - val_loss: 0.6814 - val_ACC: 0.6623\n",
      "Epoch 49/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6886 - ACC: 0.6540\n",
      "Epoch 00049: val_loss improved from 0.68138 to 0.68058, saving model to ./pima_model\\49 - 0.6806.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6848 - ACC: 0.6564 - val_loss: 0.6806 - val_ACC: 0.6623\n",
      "Epoch 50/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6763 - ACC: 0.6820\n",
      "Epoch 00050: val_loss improved from 0.68058 to 0.67935, saving model to ./pima_model\\50 - 0.6793.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6835 - ACC: 0.6580 - val_loss: 0.6793 - val_ACC: 0.6688\n",
      "Epoch 51/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6805 - ACC: 0.6500\n",
      "Epoch 00051: val_loss improved from 0.67935 to 0.67747, saving model to ./pima_model\\51 - 0.6775.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6819 - ACC: 0.6564 - val_loss: 0.6775 - val_ACC: 0.6688\n",
      "Epoch 52/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6807 - ACC: 0.6500\n",
      "Epoch 00052: val_loss improved from 0.67747 to 0.67518, saving model to ./pima_model\\52 - 0.6752.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6800 - ACC: 0.6531 - val_loss: 0.6752 - val_ACC: 0.6753\n",
      "Epoch 53/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6709 - ACC: 0.6680\n",
      "Epoch 00053: val_loss improved from 0.67518 to 0.67357, saving model to ./pima_model\\53 - 0.6736.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6777 - ACC: 0.6580 - val_loss: 0.6736 - val_ACC: 0.6753\n",
      "Epoch 54/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6728 - ACC: 0.6600\n",
      "Epoch 00054: val_loss improved from 0.67357 to 0.67142, saving model to ./pima_model\\54 - 0.6714.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6744 - ACC: 0.6531 - val_loss: 0.6714 - val_ACC: 0.6753\n",
      "Epoch 55/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6740 - ACC: 0.6640\n",
      "Epoch 00055: val_loss improved from 0.67142 to 0.66788, saving model to ./pima_model\\55 - 0.6679.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6695 - ACC: 0.6580 - val_loss: 0.6679 - val_ACC: 0.6753\n",
      "Epoch 56/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6596 - ACC: 0.6740\n",
      "Epoch 00056: val_loss improved from 0.66788 to 0.66750, saving model to ./pima_model\\56 - 0.6675.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6635 - ACC: 0.6629 - val_loss: 0.6675 - val_ACC: 0.6753\n",
      "Epoch 57/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6566 - ACC: 0.6680\n",
      "Epoch 00057: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6567 - ACC: 0.6694 - val_loss: 0.6687 - val_ACC: 0.6688\n",
      "Epoch 58/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6517 - ACC: 0.6820\n",
      "Epoch 00058: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6505 - ACC: 0.6726 - val_loss: 0.6698 - val_ACC: 0.6688\n",
      "Epoch 59/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6494 - ACC: 0.6720\n",
      "Epoch 00059: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6441 - ACC: 0.6775 - val_loss: 0.6713 - val_ACC: 0.6753\n",
      "Epoch 60/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6300 - ACC: 0.6840\n",
      "Epoch 00060: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6398 - ACC: 0.6792 - val_loss: 0.6751 - val_ACC: 0.6753\n",
      "Epoch 61/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6318 - ACC: 0.6860\n",
      "Epoch 00061: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6352 - ACC: 0.6792 - val_loss: 0.6776 - val_ACC: 0.6753\n",
      "Epoch 62/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6286 - ACC: 0.6880\n",
      "Epoch 00062: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6326 - ACC: 0.6808 - val_loss: 0.6763 - val_ACC: 0.6688\n",
      "Epoch 63/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6329 - ACC: 0.6820\n",
      "Epoch 00063: val_loss did not improve from 0.66750\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6291 - ACC: 0.6824 - val_loss: 0.6723 - val_ACC: 0.6623\n",
      "Epoch 64/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6098 - ACC: 0.6980\n",
      "Epoch 00064: val_loss improved from 0.66750 to 0.66604, saving model to ./pima_model\\64 - 0.6660.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6269 - ACC: 0.6808 - val_loss: 0.6660 - val_ACC: 0.6623\n",
      "Epoch 65/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6233 - ACC: 0.6640\n",
      "Epoch 00065: val_loss improved from 0.66604 to 0.66080, saving model to ./pima_model\\65 - 0.6608.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6243 - ACC: 0.6792 - val_loss: 0.6608 - val_ACC: 0.6688\n",
      "Epoch 66/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6127 - ACC: 0.6900\n",
      "Epoch 00066: val_loss improved from 0.66080 to 0.65724, saving model to ./pima_model\\66 - 0.6572.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6231 - ACC: 0.6792 - val_loss: 0.6572 - val_ACC: 0.6558\n",
      "Epoch 67/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6193 - ACC: 0.6740\n",
      "Epoch 00067: val_loss improved from 0.65724 to 0.65561, saving model to ./pima_model\\67 - 0.6556.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6205 - ACC: 0.6792 - val_loss: 0.6556 - val_ACC: 0.6494\n",
      "Epoch 68/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6099 - ACC: 0.6880\n",
      "Epoch 00068: val_loss improved from 0.65561 to 0.65430, saving model to ./pima_model\\68 - 0.6543.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6181 - ACC: 0.6824 - val_loss: 0.6543 - val_ACC: 0.6494\n",
      "Epoch 69/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6210 - ACC: 0.6720\n",
      "Epoch 00069: val_loss improved from 0.65430 to 0.65169, saving model to ./pima_model\\69 - 0.6517.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6152 - ACC: 0.6840 - val_loss: 0.6517 - val_ACC: 0.6558\n",
      "Epoch 70/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6275 - ACC: 0.6680\n",
      "Epoch 00070: val_loss improved from 0.65169 to 0.65100, saving model to ./pima_model\\70 - 0.6510.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6128 - ACC: 0.6808 - val_loss: 0.6510 - val_ACC: 0.6623\n",
      "Epoch 71/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6126 - ACC: 0.6840\n",
      "Epoch 00071: val_loss did not improve from 0.65100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6101 - ACC: 0.6840 - val_loss: 0.6531 - val_ACC: 0.6753\n",
      "Epoch 72/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6012 - ACC: 0.6840\n",
      "Epoch 00072: val_loss did not improve from 0.65100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6075 - ACC: 0.6906 - val_loss: 0.6561 - val_ACC: 0.6623\n",
      "Epoch 73/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6117 - ACC: 0.6920\n",
      "Epoch 00073: val_loss did not improve from 0.65100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6078 - ACC: 0.6971 - val_loss: 0.6581 - val_ACC: 0.6623\n",
      "Epoch 74/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6088 - ACC: 0.6980\n",
      "Epoch 00074: val_loss did not improve from 0.65100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6078 - ACC: 0.6938 - val_loss: 0.6573 - val_ACC: 0.6623\n",
      "Epoch 75/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5945 - ACC: 0.6920\n",
      "Epoch 00075: val_loss did not improve from 0.65100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6057 - ACC: 0.6938 - val_loss: 0.6529 - val_ACC: 0.6623\n",
      "Epoch 76/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5954 - ACC: 0.7060\n",
      "Epoch 00076: val_loss improved from 0.65100 to 0.64917, saving model to ./pima_model\\76 - 0.6492.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6022 - ACC: 0.6922 - val_loss: 0.6492 - val_ACC: 0.6558\n",
      "Epoch 77/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5875 - ACC: 0.7060\n",
      "Epoch 00077: val_loss improved from 0.64917 to 0.64690, saving model to ./pima_model\\77 - 0.6469.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6020 - ACC: 0.6840 - val_loss: 0.6469 - val_ACC: 0.6558\n",
      "Epoch 78/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5899 - ACC: 0.6940\n",
      "Epoch 00078: val_loss improved from 0.64690 to 0.64627, saving model to ./pima_model\\78 - 0.6463.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6020 - ACC: 0.6857 - val_loss: 0.6463 - val_ACC: 0.6558\n",
      "Epoch 79/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6074 - ACC: 0.6760\n",
      "Epoch 00079: val_loss did not improve from 0.64627\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6007 - ACC: 0.6889 - val_loss: 0.6466 - val_ACC: 0.6623\n",
      "Epoch 80/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6007 - ACC: 0.6900\n",
      "Epoch 00080: val_loss did not improve from 0.64627\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5992 - ACC: 0.6873 - val_loss: 0.6486 - val_ACC: 0.6623\n",
      "Epoch 81/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6055 - ACC: 0.6880\n",
      "Epoch 00081: val_loss did not improve from 0.64627\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5975 - ACC: 0.6954 - val_loss: 0.6494 - val_ACC: 0.6558\n",
      "Epoch 82/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5939 - ACC: 0.6980\n",
      "Epoch 00082: val_loss did not improve from 0.64627\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5972 - ACC: 0.6954 - val_loss: 0.6492 - val_ACC: 0.6558\n",
      "Epoch 83/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5881 - ACC: 0.7080\n",
      "Epoch 00083: val_loss did not improve from 0.64627\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5965 - ACC: 0.6954 - val_loss: 0.6476 - val_ACC: 0.6558\n",
      "Epoch 84/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5946 - ACC: 0.7040\n",
      "Epoch 00084: val_loss improved from 0.64627 to 0.64519, saving model to ./pima_model\\84 - 0.6452.hdf5\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5948 - ACC: 0.6938 - val_loss: 0.6452 - val_ACC: 0.6688\n",
      "Epoch 85/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5970 - ACC: 0.6920\n",
      "Epoch 00085: val_loss improved from 0.64519 to 0.64303, saving model to ./pima_model\\85 - 0.6430.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5940 - ACC: 0.6889 - val_loss: 0.6430 - val_ACC: 0.6688\n",
      "Epoch 86/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6025 - ACC: 0.6780\n",
      "Epoch 00086: val_loss improved from 0.64303 to 0.64253, saving model to ./pima_model\\86 - 0.6425.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5941 - ACC: 0.6873 - val_loss: 0.6425 - val_ACC: 0.6623\n",
      "Epoch 87/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5973 - ACC: 0.6720\n",
      "Epoch 00087: val_loss did not improve from 0.64253\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5933 - ACC: 0.6873 - val_loss: 0.6429 - val_ACC: 0.6753\n",
      "Epoch 88/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5974 - ACC: 0.6720\n",
      "Epoch 00088: val_loss did not improve from 0.64253\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5917 - ACC: 0.6857 - val_loss: 0.6451 - val_ACC: 0.6558\n",
      "Epoch 89/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5922 - ACC: 0.7020\n",
      "Epoch 00089: val_loss did not improve from 0.64253\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5913 - ACC: 0.6922 - val_loss: 0.6471 - val_ACC: 0.6558\n",
      "Epoch 90/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5904 - ACC: 0.6880\n",
      "Epoch 00090: val_loss did not improve from 0.64253\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5910 - ACC: 0.6938 - val_loss: 0.6475 - val_ACC: 0.6558\n",
      "Epoch 91/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5919 - ACC: 0.6860\n",
      "Epoch 00091: val_loss did not improve from 0.64253\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5908 - ACC: 0.6938 - val_loss: 0.6473 - val_ACC: 0.6558\n",
      "Epoch 92/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5920 - ACC: 0.6940\n",
      "Epoch 00092: val_loss did not improve from 0.64253\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5904 - ACC: 0.6938 - val_loss: 0.6449 - val_ACC: 0.6623\n",
      "Epoch 93/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5685 - ACC: 0.7160\n",
      "Epoch 00093: val_loss improved from 0.64253 to 0.64130, saving model to ./pima_model\\93 - 0.6413.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5886 - ACC: 0.6954 - val_loss: 0.6413 - val_ACC: 0.6688\n",
      "Epoch 94/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5849 - ACC: 0.6980\n",
      "Epoch 00094: val_loss improved from 0.64130 to 0.63766, saving model to ./pima_model\\94 - 0.6377.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5880 - ACC: 0.6906 - val_loss: 0.6377 - val_ACC: 0.6558\n",
      "Epoch 95/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6024 - ACC: 0.6740\n",
      "Epoch 00095: val_loss improved from 0.63766 to 0.63652, saving model to ./pima_model\\95 - 0.6365.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5889 - ACC: 0.6889 - val_loss: 0.6365 - val_ACC: 0.6494\n",
      "Epoch 96/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5842 - ACC: 0.6880\n",
      "Epoch 00096: val_loss improved from 0.63652 to 0.63605, saving model to ./pima_model\\96 - 0.6361.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5891 - ACC: 0.6906 - val_loss: 0.6361 - val_ACC: 0.6558\n",
      "Epoch 97/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6051 - ACC: 0.6740\n",
      "Epoch 00097: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5875 - ACC: 0.6922 - val_loss: 0.6361 - val_ACC: 0.6623\n",
      "Epoch 98/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5851 - ACC: 0.6960\n",
      "Epoch 00098: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5866 - ACC: 0.6922 - val_loss: 0.6364 - val_ACC: 0.6688\n",
      "Epoch 99/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5738 - ACC: 0.6920\n",
      "Epoch 00099: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5855 - ACC: 0.6906 - val_loss: 0.6366 - val_ACC: 0.6688\n",
      "Epoch 100/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6063 - ACC: 0.6780\n",
      "Epoch 00100: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5851 - ACC: 0.6906 - val_loss: 0.6373 - val_ACC: 0.6688\n",
      "Epoch 101/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5936 - ACC: 0.6900\n",
      "Epoch 00101: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5843 - ACC: 0.6971 - val_loss: 0.6392 - val_ACC: 0.6688\n",
      "Epoch 102/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5805 - ACC: 0.6940\n",
      "Epoch 00102: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5841 - ACC: 0.6954 - val_loss: 0.6408 - val_ACC: 0.6623\n",
      "Epoch 103/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5704 - ACC: 0.7100\n",
      "Epoch 00103: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5839 - ACC: 0.6987 - val_loss: 0.6393 - val_ACC: 0.6494\n",
      "Epoch 104/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5969 - ACC: 0.6920\n",
      "Epoch 00104: val_loss did not improve from 0.63605\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5833 - ACC: 0.6954 - val_loss: 0.6374 - val_ACC: 0.6623\n",
      "Epoch 105/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5717 - ACC: 0.7060\n",
      "Epoch 00105: val_loss improved from 0.63605 to 0.63595, saving model to ./pima_model\\105 - 0.6359.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5825 - ACC: 0.6954 - val_loss: 0.6359 - val_ACC: 0.6623\n",
      "Epoch 106/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5929 - ACC: 0.6860\n",
      "Epoch 00106: val_loss improved from 0.63595 to 0.63418, saving model to ./pima_model\\106 - 0.6342.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5818 - ACC: 0.6938 - val_loss: 0.6342 - val_ACC: 0.6753\n",
      "Epoch 107/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5821 - ACC: 0.6760\n",
      "Epoch 00107: val_loss improved from 0.63418 to 0.63323, saving model to ./pima_model\\107 - 0.6332.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5828 - ACC: 0.6906 - val_loss: 0.6332 - val_ACC: 0.6623\n",
      "Epoch 108/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5961 - ACC: 0.6800\n",
      "Epoch 00108: val_loss did not improve from 0.63323\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5827 - ACC: 0.6889 - val_loss: 0.6346 - val_ACC: 0.6753\n",
      "Epoch 109/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5824 - ACC: 0.6820\n",
      "Epoch 00109: val_loss did not improve from 0.63323\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5808 - ACC: 0.6857 - val_loss: 0.6380 - val_ACC: 0.6623\n",
      "Epoch 110/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5901 - ACC: 0.6880\n",
      "Epoch 00110: val_loss did not improve from 0.63323\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5809 - ACC: 0.6954 - val_loss: 0.6417 - val_ACC: 0.6753\n",
      "Epoch 111/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5978 - ACC: 0.6820\n",
      "Epoch 00111: val_loss did not improve from 0.63323\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5821 - ACC: 0.6987 - val_loss: 0.6432 - val_ACC: 0.6818\n",
      "Epoch 112/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5806 - ACC: 0.7020\n",
      "Epoch 00112: val_loss did not improve from 0.63323\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5826 - ACC: 0.6971 - val_loss: 0.6415 - val_ACC: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5944 - ACC: 0.6880\n",
      "Epoch 00113: val_loss did not improve from 0.63323\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5810 - ACC: 0.7003 - val_loss: 0.6373 - val_ACC: 0.6558\n",
      "Epoch 114/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5682 - ACC: 0.7060\n",
      "Epoch 00114: val_loss improved from 0.63323 to 0.63281, saving model to ./pima_model\\114 - 0.6328.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5797 - ACC: 0.6987 - val_loss: 0.6328 - val_ACC: 0.6753\n",
      "Epoch 115/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5656 - ACC: 0.6960\n",
      "Epoch 00115: val_loss improved from 0.63281 to 0.63037, saving model to ./pima_model\\115 - 0.6304.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5790 - ACC: 0.6938 - val_loss: 0.6304 - val_ACC: 0.6818\n",
      "Epoch 116/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5859 - ACC: 0.6780\n",
      "Epoch 00116: val_loss improved from 0.63037 to 0.62924, saving model to ./pima_model\\116 - 0.6292.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5797 - ACC: 0.6873 - val_loss: 0.6292 - val_ACC: 0.6753\n",
      "Epoch 117/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5904 - ACC: 0.6800\n",
      "Epoch 00117: val_loss did not improve from 0.62924\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5784 - ACC: 0.6889 - val_loss: 0.6312 - val_ACC: 0.6623\n",
      "Epoch 118/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5760 - ACC: 0.6960\n",
      "Epoch 00118: val_loss did not improve from 0.62924\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5765 - ACC: 0.7003 - val_loss: 0.6351 - val_ACC: 0.6623\n",
      "Epoch 119/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5858 - ACC: 0.6940\n",
      "Epoch 00119: val_loss did not improve from 0.62924\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5772 - ACC: 0.6987 - val_loss: 0.6353 - val_ACC: 0.6753\n",
      "Epoch 120/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5893 - ACC: 0.6980\n",
      "Epoch 00120: val_loss did not improve from 0.62924\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5776 - ACC: 0.6987 - val_loss: 0.6332 - val_ACC: 0.6623\n",
      "Epoch 121/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5754 - ACC: 0.7020\n",
      "Epoch 00121: val_loss did not improve from 0.62924\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5766 - ACC: 0.7003 - val_loss: 0.6302 - val_ACC: 0.6494\n",
      "Epoch 122/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5614 - ACC: 0.7100\n",
      "Epoch 00122: val_loss improved from 0.62924 to 0.62647, saving model to ./pima_model\\122 - 0.6265.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5755 - ACC: 0.7020 - val_loss: 0.6265 - val_ACC: 0.6753\n",
      "Epoch 123/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5598 - ACC: 0.7080\n",
      "Epoch 00123: val_loss improved from 0.62647 to 0.62525, saving model to ./pima_model\\123 - 0.6253.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5761 - ACC: 0.6922 - val_loss: 0.6253 - val_ACC: 0.6688\n",
      "Epoch 124/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5588 - ACC: 0.7120\n",
      "Epoch 00124: val_loss improved from 0.62525 to 0.62503, saving model to ./pima_model\\124 - 0.6250.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5783 - ACC: 0.6922 - val_loss: 0.6250 - val_ACC: 0.6688\n",
      "Epoch 125/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5741 - ACC: 0.6960\n",
      "Epoch 00125: val_loss did not improve from 0.62503\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5781 - ACC: 0.6906 - val_loss: 0.6251 - val_ACC: 0.6623\n",
      "Epoch 126/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5636 - ACC: 0.7020\n",
      "Epoch 00126: val_loss did not improve from 0.62503\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5756 - ACC: 0.6938 - val_loss: 0.6269 - val_ACC: 0.6623\n",
      "Epoch 127/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5843 - ACC: 0.6980\n",
      "Epoch 00127: val_loss did not improve from 0.62503\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5747 - ACC: 0.7003 - val_loss: 0.6304 - val_ACC: 0.6688\n",
      "Epoch 128/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5737 - ACC: 0.7040\n",
      "Epoch 00128: val_loss did not improve from 0.62503\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5749 - ACC: 0.7036 - val_loss: 0.6325 - val_ACC: 0.6818\n",
      "Epoch 129/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5787 - ACC: 0.7000\n",
      "Epoch 00129: val_loss did not improve from 0.62503\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5755 - ACC: 0.7020 - val_loss: 0.6285 - val_ACC: 0.6753\n",
      "Epoch 130/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5832 - ACC: 0.6920\n",
      "Epoch 00130: val_loss improved from 0.62503 to 0.62197, saving model to ./pima_model\\130 - 0.6220.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5736 - ACC: 0.6971 - val_loss: 0.6220 - val_ACC: 0.6753\n",
      "Epoch 131/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5790 - ACC: 0.6820\n",
      "Epoch 00131: val_loss improved from 0.62197 to 0.62071, saving model to ./pima_model\\131 - 0.6207.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5736 - ACC: 0.6938 - val_loss: 0.6207 - val_ACC: 0.6753\n",
      "Epoch 132/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5883 - ACC: 0.6720\n",
      "Epoch 00132: val_loss did not improve from 0.62071\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5739 - ACC: 0.6857 - val_loss: 0.6229 - val_ACC: 0.6688\n",
      "Epoch 133/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5727 - ACC: 0.6920\n",
      "Epoch 00133: val_loss did not improve from 0.62071\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5726 - ACC: 0.6954 - val_loss: 0.6295 - val_ACC: 0.6688\n",
      "Epoch 134/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5777 - ACC: 0.6860\n",
      "Epoch 00134: val_loss did not improve from 0.62071\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5731 - ACC: 0.6954 - val_loss: 0.6382 - val_ACC: 0.6818\n",
      "Epoch 135/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5704 - ACC: 0.7000\n",
      "Epoch 00135: val_loss did not improve from 0.62071\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5767 - ACC: 0.6922 - val_loss: 0.6426 - val_ACC: 0.6883\n",
      "Epoch 136/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5674 - ACC: 0.6920\n",
      "Epoch 00136: val_loss did not improve from 0.62071\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5782 - ACC: 0.6922 - val_loss: 0.6351 - val_ACC: 0.6818\n",
      "Epoch 137/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5779 - ACC: 0.6840\n",
      "Epoch 00137: val_loss did not improve from 0.62071\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5741 - ACC: 0.6938 - val_loss: 0.6235 - val_ACC: 0.6688\n",
      "Epoch 138/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5765 - ACC: 0.6960\n",
      "Epoch 00138: val_loss improved from 0.62071 to 0.62000, saving model to ./pima_model\\138 - 0.6200.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5706 - ACC: 0.6987 - val_loss: 0.6200 - val_ACC: 0.6753\n",
      "Epoch 139/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5720 - ACC: 0.6960\n",
      "Epoch 00139: val_loss improved from 0.62000 to 0.61972, saving model to ./pima_model\\139 - 0.6197.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5732 - ACC: 0.6840 - val_loss: 0.6197 - val_ACC: 0.6753\n",
      "Epoch 140/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5714 - ACC: 0.6760\n",
      "Epoch 00140: val_loss did not improve from 0.61972\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5725 - ACC: 0.6824 - val_loss: 0.6226 - val_ACC: 0.6688\n",
      "Epoch 141/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5721 - ACC: 0.6980\n",
      "Epoch 00141: val_loss did not improve from 0.61972\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5698 - ACC: 0.7003 - val_loss: 0.6337 - val_ACC: 0.6753\n",
      "Epoch 142/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5726 - ACC: 0.6960\n",
      "Epoch 00142: val_loss did not improve from 0.61972\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5733 - ACC: 0.6906 - val_loss: 0.6458 - val_ACC: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5769 - ACC: 0.6940\n",
      "Epoch 00143: val_loss did not improve from 0.61972\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5789 - ACC: 0.6987 - val_loss: 0.6424 - val_ACC: 0.6818\n",
      "Epoch 144/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5819 - ACC: 0.6960\n",
      "Epoch 00144: val_loss did not improve from 0.61972\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5765 - ACC: 0.6954 - val_loss: 0.6299 - val_ACC: 0.6753\n",
      "Epoch 145/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5964 - ACC: 0.6820\n",
      "Epoch 00145: val_loss did not improve from 0.61972\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5699 - ACC: 0.6971 - val_loss: 0.6218 - val_ACC: 0.6623\n",
      "Epoch 146/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5641 - ACC: 0.7000\n",
      "Epoch 00146: val_loss improved from 0.61972 to 0.61955, saving model to ./pima_model\\146 - 0.6196.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5695 - ACC: 0.6922 - val_loss: 0.6196 - val_ACC: 0.6753\n",
      "Epoch 147/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5838 - ACC: 0.6680\n",
      "Epoch 00147: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5702 - ACC: 0.6857 - val_loss: 0.6205 - val_ACC: 0.6623\n",
      "Epoch 148/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5621 - ACC: 0.7000\n",
      "Epoch 00148: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5698 - ACC: 0.6954 - val_loss: 0.6240 - val_ACC: 0.6623\n",
      "Epoch 149/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5644 - ACC: 0.6980\n",
      "Epoch 00149: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5687 - ACC: 0.6954 - val_loss: 0.6256 - val_ACC: 0.6494\n",
      "Epoch 150/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5626 - ACC: 0.7060\n",
      "Epoch 00150: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5685 - ACC: 0.6971 - val_loss: 0.6254 - val_ACC: 0.6494\n",
      "Epoch 151/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5697 - ACC: 0.6960\n",
      "Epoch 00151: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5678 - ACC: 0.6954 - val_loss: 0.6234 - val_ACC: 0.6558\n",
      "Epoch 152/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5726 - ACC: 0.6920\n",
      "Epoch 00152: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5670 - ACC: 0.6938 - val_loss: 0.6225 - val_ACC: 0.6623\n",
      "Epoch 153/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5650 - ACC: 0.6940\n",
      "Epoch 00153: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5668 - ACC: 0.6938 - val_loss: 0.6240 - val_ACC: 0.6558\n",
      "Epoch 154/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5702 - ACC: 0.6980\n",
      "Epoch 00154: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5666 - ACC: 0.6971 - val_loss: 0.6276 - val_ACC: 0.6688\n",
      "Epoch 155/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5615 - ACC: 0.6980\n",
      "Epoch 00155: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5672 - ACC: 0.7036 - val_loss: 0.6268 - val_ACC: 0.6688\n",
      "Epoch 156/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5594 - ACC: 0.7000\n",
      "Epoch 00156: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5668 - ACC: 0.6987 - val_loss: 0.6227 - val_ACC: 0.6623\n",
      "Epoch 157/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5746 - ACC: 0.6880\n",
      "Epoch 00157: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5660 - ACC: 0.6987 - val_loss: 0.6205 - val_ACC: 0.6688\n",
      "Epoch 158/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5724 - ACC: 0.6880\n",
      "Epoch 00158: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5658 - ACC: 0.6938 - val_loss: 0.6210 - val_ACC: 0.6623\n",
      "Epoch 159/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5575 - ACC: 0.6840\n",
      "Epoch 00159: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5652 - ACC: 0.6971 - val_loss: 0.6246 - val_ACC: 0.6753\n",
      "Epoch 160/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5710 - ACC: 0.6940\n",
      "Epoch 00160: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5657 - ACC: 0.6971 - val_loss: 0.6285 - val_ACC: 0.6753\n",
      "Epoch 161/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5591 - ACC: 0.6960\n",
      "Epoch 00161: val_loss did not improve from 0.61955\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5667 - ACC: 0.6954 - val_loss: 0.6234 - val_ACC: 0.6558\n",
      "Epoch 162/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5683 - ACC: 0.6920\n",
      "Epoch 00162: val_loss improved from 0.61955 to 0.61783, saving model to ./pima_model\\162 - 0.6178.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5642 - ACC: 0.6954 - val_loss: 0.6178 - val_ACC: 0.6753\n",
      "Epoch 163/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5828 - ACC: 0.6780\n",
      "Epoch 00163: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5678 - ACC: 0.6873 - val_loss: 0.6187 - val_ACC: 0.6623\n",
      "Epoch 164/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5545 - ACC: 0.6980\n",
      "Epoch 00164: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5699 - ACC: 0.6889 - val_loss: 0.6196 - val_ACC: 0.6753\n",
      "Epoch 165/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5503 - ACC: 0.7020\n",
      "Epoch 00165: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5653 - ACC: 0.6938 - val_loss: 0.6268 - val_ACC: 0.6688\n",
      "Epoch 166/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5606 - ACC: 0.7000\n",
      "Epoch 00166: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5641 - ACC: 0.7003 - val_loss: 0.6387 - val_ACC: 0.6753\n",
      "Epoch 167/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5702 - ACC: 0.7020\n",
      "Epoch 00167: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5690 - ACC: 0.6954 - val_loss: 0.6397 - val_ACC: 0.6753\n",
      "Epoch 168/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5852 - ACC: 0.6840\n",
      "Epoch 00168: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5681 - ACC: 0.6938 - val_loss: 0.6284 - val_ACC: 0.6558\n",
      "Epoch 169/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5634 - ACC: 0.6940\n",
      "Epoch 00169: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5644 - ACC: 0.6987 - val_loss: 0.6224 - val_ACC: 0.6623\n",
      "Epoch 170/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5660 - ACC: 0.6940\n",
      "Epoch 00170: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5633 - ACC: 0.6938 - val_loss: 0.6201 - val_ACC: 0.6688\n",
      "Epoch 171/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5721 - ACC: 0.6920\n",
      "Epoch 00171: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5635 - ACC: 0.6954 - val_loss: 0.6204 - val_ACC: 0.6623\n",
      "Epoch 172/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5741 - ACC: 0.6840\n",
      "Epoch 00172: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5624 - ACC: 0.6954 - val_loss: 0.6254 - val_ACC: 0.6753\n",
      "Epoch 173/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5755 - ACC: 0.6880\n",
      "Epoch 00173: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5626 - ACC: 0.6987 - val_loss: 0.6345 - val_ACC: 0.6753\n",
      "Epoch 174/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5745 - ACC: 0.6820\n",
      "Epoch 00174: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5661 - ACC: 0.6971 - val_loss: 0.6398 - val_ACC: 0.6753\n",
      "Epoch 175/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5898 - ACC: 0.6700\n",
      "Epoch 00175: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5677 - ACC: 0.6938 - val_loss: 0.6314 - val_ACC: 0.6753\n",
      "Epoch 176/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5580 - ACC: 0.7000\n",
      "Epoch 00176: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5635 - ACC: 0.6954 - val_loss: 0.6208 - val_ACC: 0.6623\n",
      "Epoch 177/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5669 - ACC: 0.6960\n",
      "Epoch 00177: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5620 - ACC: 0.6938 - val_loss: 0.6182 - val_ACC: 0.6688\n",
      "Epoch 178/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5632 - ACC: 0.6960\n",
      "Epoch 00178: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5652 - ACC: 0.6889 - val_loss: 0.6180 - val_ACC: 0.6688\n",
      "Epoch 179/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5581 - ACC: 0.6920\n",
      "Epoch 00179: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5643 - ACC: 0.6889 - val_loss: 0.6200 - val_ACC: 0.6623\n",
      "Epoch 180/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5836 - ACC: 0.6780\n",
      "Epoch 00180: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5612 - ACC: 0.6971 - val_loss: 0.6277 - val_ACC: 0.6688\n",
      "Epoch 181/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5703 - ACC: 0.6820\n",
      "Epoch 00181: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5628 - ACC: 0.6987 - val_loss: 0.6310 - val_ACC: 0.6753\n",
      "Epoch 182/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5691 - ACC: 0.6960\n",
      "Epoch 00182: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5630 - ACC: 0.6954 - val_loss: 0.6245 - val_ACC: 0.6623\n",
      "Epoch 183/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5536 - ACC: 0.7020\n",
      "Epoch 00183: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5605 - ACC: 0.7020 - val_loss: 0.6188 - val_ACC: 0.6753\n",
      "Epoch 184/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5697 - ACC: 0.6840\n",
      "Epoch 00184: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5634 - ACC: 0.6857 - val_loss: 0.6185 - val_ACC: 0.6558\n",
      "Epoch 185/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5621 - ACC: 0.7040\n",
      "Epoch 00185: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5661 - ACC: 0.6906 - val_loss: 0.6182 - val_ACC: 0.6688\n",
      "Epoch 186/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5690 - ACC: 0.6760\n",
      "Epoch 00186: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5634 - ACC: 0.6857 - val_loss: 0.6197 - val_ACC: 0.6753\n",
      "Epoch 187/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5522 - ACC: 0.7060\n",
      "Epoch 00187: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5610 - ACC: 0.6971 - val_loss: 0.6213 - val_ACC: 0.6623\n",
      "Epoch 188/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5638 - ACC: 0.6940\n",
      "Epoch 00188: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5600 - ACC: 0.6971 - val_loss: 0.6214 - val_ACC: 0.6558\n",
      "Epoch 189/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5517 - ACC: 0.7020\n",
      "Epoch 00189: val_loss did not improve from 0.61783\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5598 - ACC: 0.7003 - val_loss: 0.6199 - val_ACC: 0.6623\n",
      "Epoch 190/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5597 - ACC: 0.6920\n",
      "Epoch 00190: val_loss improved from 0.61783 to 0.61730, saving model to ./pima_model\\190 - 0.6173.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5595 - ACC: 0.6971 - val_loss: 0.6173 - val_ACC: 0.6818\n",
      "Epoch 191/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5622 - ACC: 0.6840\n",
      "Epoch 00191: val_loss improved from 0.61730 to 0.61673, saving model to ./pima_model\\191 - 0.6167.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5607 - ACC: 0.6889 - val_loss: 0.6167 - val_ACC: 0.6753\n",
      "Epoch 192/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5486 - ACC: 0.6940\n",
      "Epoch 00192: val_loss did not improve from 0.61673\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5608 - ACC: 0.6873 - val_loss: 0.6191 - val_ACC: 0.6688\n",
      "Epoch 193/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5612 - ACC: 0.6880\n",
      "Epoch 00193: val_loss did not improve from 0.61673\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5592 - ACC: 0.6971 - val_loss: 0.6256 - val_ACC: 0.6753\n",
      "Epoch 194/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5696 - ACC: 0.6880\n",
      "Epoch 00194: val_loss did not improve from 0.61673\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5604 - ACC: 0.7003 - val_loss: 0.6292 - val_ACC: 0.6753\n",
      "Epoch 195/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5576 - ACC: 0.7000\n",
      "Epoch 00195: val_loss did not improve from 0.61673\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5619 - ACC: 0.6954 - val_loss: 0.6274 - val_ACC: 0.6753\n",
      "Epoch 196/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5529 - ACC: 0.7080\n",
      "Epoch 00196: val_loss did not improve from 0.61673\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5609 - ACC: 0.6971 - val_loss: 0.6212 - val_ACC: 0.6753\n",
      "Epoch 197/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5648 - ACC: 0.6980\n",
      "Epoch 00197: val_loss improved from 0.61673 to 0.61541, saving model to ./pima_model\\197 - 0.6154.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5583 - ACC: 0.7036 - val_loss: 0.6154 - val_ACC: 0.6753\n",
      "Epoch 198/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5602 - ACC: 0.7020\n",
      "Epoch 00198: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5597 - ACC: 0.6889 - val_loss: 0.6160 - val_ACC: 0.6688\n",
      "Epoch 199/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5708 - ACC: 0.6840\n",
      "Epoch 00199: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5650 - ACC: 0.6906 - val_loss: 0.6160 - val_ACC: 0.6688\n",
      "Epoch 200/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5557 - ACC: 0.7000\n",
      "Epoch 00200: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5628 - ACC: 0.6971 - val_loss: 0.6189 - val_ACC: 0.6494\n",
      "Epoch 201/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5680 - ACC: 0.6960\n",
      "Epoch 00201: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5587 - ACC: 0.7036 - val_loss: 0.6251 - val_ACC: 0.6753\n",
      "Epoch 202/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5576 - ACC: 0.6920\n",
      "Epoch 00202: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5598 - ACC: 0.7003 - val_loss: 0.6264 - val_ACC: 0.6753\n",
      "Epoch 203/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5635 - ACC: 0.7060\n",
      "Epoch 00203: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5603 - ACC: 0.7020 - val_loss: 0.6235 - val_ACC: 0.6753\n",
      "Epoch 204/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5670 - ACC: 0.7000\n",
      "Epoch 00204: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5598 - ACC: 0.7036 - val_loss: 0.6233 - val_ACC: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5485 - ACC: 0.7120\n",
      "Epoch 00205: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5589 - ACC: 0.7052 - val_loss: 0.6256 - val_ACC: 0.6753\n",
      "Epoch 206/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5525 - ACC: 0.7020\n",
      "Epoch 00206: val_loss did not improve from 0.61541\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5598 - ACC: 0.7003 - val_loss: 0.6220 - val_ACC: 0.6818\n",
      "Epoch 207/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5467 - ACC: 0.7220\n",
      "Epoch 00207: val_loss improved from 0.61541 to 0.61377, saving model to ./pima_model\\207 - 0.6138.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5581 - ACC: 0.7052 - val_loss: 0.6138 - val_ACC: 0.6558\n",
      "Epoch 208/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5418 - ACC: 0.7120\n",
      "Epoch 00208: val_loss improved from 0.61377 to 0.61152, saving model to ./pima_model\\208 - 0.6115.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5583 - ACC: 0.6971 - val_loss: 0.6115 - val_ACC: 0.6753\n",
      "Epoch 209/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5534 - ACC: 0.6980\n",
      "Epoch 00209: val_loss improved from 0.61152 to 0.61150, saving model to ./pima_model\\209 - 0.6115.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5627 - ACC: 0.6840 - val_loss: 0.6115 - val_ACC: 0.6753\n",
      "Epoch 210/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5534 - ACC: 0.7020\n",
      "Epoch 00210: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5608 - ACC: 0.6922 - val_loss: 0.6148 - val_ACC: 0.6558\n",
      "Epoch 211/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5628 - ACC: 0.6980\n",
      "Epoch 00211: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5568 - ACC: 0.7036 - val_loss: 0.6247 - val_ACC: 0.6753\n",
      "Epoch 212/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5634 - ACC: 0.6960\n",
      "Epoch 00212: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5608 - ACC: 0.7020 - val_loss: 0.6317 - val_ACC: 0.6753\n",
      "Epoch 213/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5641 - ACC: 0.6980\n",
      "Epoch 00213: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5622 - ACC: 0.7020 - val_loss: 0.6227 - val_ACC: 0.6818\n",
      "Epoch 214/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5497 - ACC: 0.7040\n",
      "Epoch 00214: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5582 - ACC: 0.7003 - val_loss: 0.6154 - val_ACC: 0.6558\n",
      "Epoch 215/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5611 - ACC: 0.6860\n",
      "Epoch 00215: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5573 - ACC: 0.6971 - val_loss: 0.6138 - val_ACC: 0.6753\n",
      "Epoch 216/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5676 - ACC: 0.6900\n",
      "Epoch 00216: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5588 - ACC: 0.6938 - val_loss: 0.6151 - val_ACC: 0.6688\n",
      "Epoch 217/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5606 - ACC: 0.7080\n",
      "Epoch 00217: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5567 - ACC: 0.7020 - val_loss: 0.6225 - val_ACC: 0.6753\n",
      "Epoch 218/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5650 - ACC: 0.6840\n",
      "Epoch 00218: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5573 - ACC: 0.7003 - val_loss: 0.6293 - val_ACC: 0.6753\n",
      "Epoch 219/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5472 - ACC: 0.7000\n",
      "Epoch 00219: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5595 - ACC: 0.6971 - val_loss: 0.6248 - val_ACC: 0.6753\n",
      "Epoch 220/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5460 - ACC: 0.6980\n",
      "Epoch 00220: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5574 - ACC: 0.6954 - val_loss: 0.6157 - val_ACC: 0.6623\n",
      "Epoch 221/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5676 - ACC: 0.6880\n",
      "Epoch 00221: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5569 - ACC: 0.6971 - val_loss: 0.6132 - val_ACC: 0.6753\n",
      "Epoch 222/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5559 - ACC: 0.6840\n",
      "Epoch 00222: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5581 - ACC: 0.6906 - val_loss: 0.6134 - val_ACC: 0.6688\n",
      "Epoch 223/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5543 - ACC: 0.6960\n",
      "Epoch 00223: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5577 - ACC: 0.6922 - val_loss: 0.6149 - val_ACC: 0.6753\n",
      "Epoch 224/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5577 - ACC: 0.6880\n",
      "Epoch 00224: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5563 - ACC: 0.6954 - val_loss: 0.6203 - val_ACC: 0.6753\n",
      "Epoch 225/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5508 - ACC: 0.6940\n",
      "Epoch 00225: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5570 - ACC: 0.6987 - val_loss: 0.6239 - val_ACC: 0.6753\n",
      "Epoch 226/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5570 - ACC: 0.6840\n",
      "Epoch 00226: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5573 - ACC: 0.6987 - val_loss: 0.6198 - val_ACC: 0.6753\n",
      "Epoch 227/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5575 - ACC: 0.7100\n",
      "Epoch 00227: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5560 - ACC: 0.6987 - val_loss: 0.6149 - val_ACC: 0.6688\n",
      "Epoch 228/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5649 - ACC: 0.6920\n",
      "Epoch 00228: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5566 - ACC: 0.6971 - val_loss: 0.6135 - val_ACC: 0.6753\n",
      "Epoch 229/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5704 - ACC: 0.6840\n",
      "Epoch 00229: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5557 - ACC: 0.6971 - val_loss: 0.6151 - val_ACC: 0.6753\n",
      "Epoch 230/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5509 - ACC: 0.7120\n",
      "Epoch 00230: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5549 - ACC: 0.7036 - val_loss: 0.6169 - val_ACC: 0.6753\n",
      "Epoch 231/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5697 - ACC: 0.6900\n",
      "Epoch 00231: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5553 - ACC: 0.7003 - val_loss: 0.6182 - val_ACC: 0.6818\n",
      "Epoch 232/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5706 - ACC: 0.6920\n",
      "Epoch 00232: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5558 - ACC: 0.7003 - val_loss: 0.6194 - val_ACC: 0.6818\n",
      "Epoch 233/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5506 - ACC: 0.7060\n",
      "Epoch 00233: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5561 - ACC: 0.7020 - val_loss: 0.6189 - val_ACC: 0.6753\n",
      "Epoch 234/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5528 - ACC: 0.7140\n",
      "Epoch 00234: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5553 - ACC: 0.7003 - val_loss: 0.6135 - val_ACC: 0.6753\n",
      "Epoch 235/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5542 - ACC: 0.6980\n",
      "Epoch 00235: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5547 - ACC: 0.7003 - val_loss: 0.6115 - val_ACC: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5469 - ACC: 0.6900\n",
      "Epoch 00236: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5558 - ACC: 0.6889 - val_loss: 0.6122 - val_ACC: 0.6753\n",
      "Epoch 237/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5528 - ACC: 0.6860\n",
      "Epoch 00237: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5551 - ACC: 0.6906 - val_loss: 0.6155 - val_ACC: 0.6753\n",
      "Epoch 238/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5620 - ACC: 0.6980\n",
      "Epoch 00238: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5552 - ACC: 0.7020 - val_loss: 0.6188 - val_ACC: 0.6753\n",
      "Epoch 239/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5628 - ACC: 0.7000\n",
      "Epoch 00239: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5542 - ACC: 0.7020 - val_loss: 0.6167 - val_ACC: 0.6753\n",
      "Epoch 240/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5617 - ACC: 0.6980\n",
      "Epoch 00240: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5539 - ACC: 0.7003 - val_loss: 0.6158 - val_ACC: 0.6818\n",
      "Epoch 241/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5649 - ACC: 0.6980\n",
      "Epoch 00241: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5543 - ACC: 0.7036 - val_loss: 0.6171 - val_ACC: 0.6753\n",
      "Epoch 242/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5601 - ACC: 0.6940\n",
      "Epoch 00242: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5539 - ACC: 0.7003 - val_loss: 0.6208 - val_ACC: 0.6688\n",
      "Epoch 243/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5537 - ACC: 0.6960\n",
      "Epoch 00243: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5545 - ACC: 0.7003 - val_loss: 0.6246 - val_ACC: 0.6688\n",
      "Epoch 244/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5579 - ACC: 0.6960\n",
      "Epoch 00244: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5556 - ACC: 0.7003 - val_loss: 0.6264 - val_ACC: 0.6688\n",
      "Epoch 245/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5654 - ACC: 0.6980\n",
      "Epoch 00245: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5559 - ACC: 0.7020 - val_loss: 0.6207 - val_ACC: 0.6688\n",
      "Epoch 246/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5654 - ACC: 0.6880\n",
      "Epoch 00246: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5537 - ACC: 0.7003 - val_loss: 0.6153 - val_ACC: 0.6883\n",
      "Epoch 247/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5545 - ACC: 0.7020\n",
      "Epoch 00247: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5539 - ACC: 0.7020 - val_loss: 0.6144 - val_ACC: 0.6818\n",
      "Epoch 248/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5663 - ACC: 0.6960\n",
      "Epoch 00248: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5556 - ACC: 0.6922 - val_loss: 0.6154 - val_ACC: 0.6818\n",
      "Epoch 249/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5492 - ACC: 0.7040\n",
      "Epoch 00249: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5541 - ACC: 0.6987 - val_loss: 0.6200 - val_ACC: 0.6818\n",
      "Epoch 250/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5459 - ACC: 0.7100\n",
      "Epoch 00250: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5534 - ACC: 0.7036 - val_loss: 0.6243 - val_ACC: 0.6688\n",
      "Epoch 251/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5565 - ACC: 0.7000\n",
      "Epoch 00251: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5553 - ACC: 0.7036 - val_loss: 0.6225 - val_ACC: 0.6753\n",
      "Epoch 252/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5350 - ACC: 0.7140\n",
      "Epoch 00252: val_loss did not improve from 0.61150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5543 - ACC: 0.7020 - val_loss: 0.6141 - val_ACC: 0.6818\n",
      "Epoch 253/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5551 - ACC: 0.7020\n",
      "Epoch 00253: val_loss improved from 0.61150 to 0.60995, saving model to ./pima_model\\253 - 0.6099.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5541 - ACC: 0.6971 - val_loss: 0.6099 - val_ACC: 0.6753\n",
      "Epoch 254/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5544 - ACC: 0.6960\n",
      "Epoch 00254: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5538 - ACC: 0.6971 - val_loss: 0.6106 - val_ACC: 0.6948\n",
      "Epoch 255/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5508 - ACC: 0.7120\n",
      "Epoch 00255: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5529 - ACC: 0.7020 - val_loss: 0.6143 - val_ACC: 0.6818\n",
      "Epoch 256/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5550 - ACC: 0.6900\n",
      "Epoch 00256: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5524 - ACC: 0.7003 - val_loss: 0.6185 - val_ACC: 0.6623\n",
      "Epoch 257/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5400 - ACC: 0.7140\n",
      "Epoch 00257: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5532 - ACC: 0.7020 - val_loss: 0.6195 - val_ACC: 0.6623\n",
      "Epoch 258/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5594 - ACC: 0.6920\n",
      "Epoch 00258: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5537 - ACC: 0.7003 - val_loss: 0.6185 - val_ACC: 0.6623\n",
      "Epoch 259/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5478 - ACC: 0.7140\n",
      "Epoch 00259: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5532 - ACC: 0.7036 - val_loss: 0.6160 - val_ACC: 0.6818\n",
      "Epoch 260/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5514 - ACC: 0.7080\n",
      "Epoch 00260: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5523 - ACC: 0.7052 - val_loss: 0.6118 - val_ACC: 0.6818\n",
      "Epoch 261/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5578 - ACC: 0.6840\n",
      "Epoch 00261: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5529 - ACC: 0.6987 - val_loss: 0.6111 - val_ACC: 0.6753\n",
      "Epoch 262/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5589 - ACC: 0.6820\n",
      "Epoch 00262: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5527 - ACC: 0.7003 - val_loss: 0.6145 - val_ACC: 0.6818\n",
      "Epoch 263/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5523 - ACC: 0.7100\n",
      "Epoch 00263: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5515 - ACC: 0.7068 - val_loss: 0.6257 - val_ACC: 0.6818\n",
      "Epoch 264/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5361 - ACC: 0.7140\n",
      "Epoch 00264: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5570 - ACC: 0.7036 - val_loss: 0.6232 - val_ACC: 0.6818\n",
      "Epoch 265/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5453 - ACC: 0.7100\n",
      "Epoch 00265: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5526 - ACC: 0.7036 - val_loss: 0.6102 - val_ACC: 0.6818\n",
      "Epoch 266/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5553 - ACC: 0.7020\n",
      "Epoch 00266: val_loss did not improve from 0.60995\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5533 - ACC: 0.6938 - val_loss: 0.6109 - val_ACC: 0.6818\n",
      "Epoch 267/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5727 - ACC: 0.6840\n",
      "Epoch 00267: val_loss improved from 0.60995 to 0.60935, saving model to ./pima_model\\267 - 0.6094.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5596 - ACC: 0.6954 - val_loss: 0.6094 - val_ACC: 0.6883\n",
      "Epoch 268/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5460 - ACC: 0.7020\n",
      "Epoch 00268: val_loss did not improve from 0.60935\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5572 - ACC: 0.6987 - val_loss: 0.6094 - val_ACC: 0.6753\n",
      "Epoch 269/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5464 - ACC: 0.7080\n",
      "Epoch 00269: val_loss did not improve from 0.60935\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5516 - ACC: 0.6971 - val_loss: 0.6128 - val_ACC: 0.6753\n",
      "Epoch 270/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5485 - ACC: 0.7080\n",
      "Epoch 00270: val_loss did not improve from 0.60935\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5509 - ACC: 0.7020 - val_loss: 0.6163 - val_ACC: 0.6818\n",
      "Epoch 271/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5576 - ACC: 0.7000\n",
      "Epoch 00271: val_loss did not improve from 0.60935\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5519 - ACC: 0.7020 - val_loss: 0.6180 - val_ACC: 0.6883\n",
      "Epoch 272/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5503 - ACC: 0.7100\n",
      "Epoch 00272: val_loss did not improve from 0.60935\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5524 - ACC: 0.7036 - val_loss: 0.6174 - val_ACC: 0.6818\n",
      "Epoch 273/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5407 - ACC: 0.7160\n",
      "Epoch 00273: val_loss did not improve from 0.60935\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5521 - ACC: 0.7036 - val_loss: 0.6109 - val_ACC: 0.6753\n",
      "Epoch 274/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5496 - ACC: 0.7000\n",
      "Epoch 00274: val_loss improved from 0.60935 to 0.60925, saving model to ./pima_model\\274 - 0.6092.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5521 - ACC: 0.6954 - val_loss: 0.6092 - val_ACC: 0.6883\n",
      "Epoch 275/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5546 - ACC: 0.6880\n",
      "Epoch 00275: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5547 - ACC: 0.6906 - val_loss: 0.6097 - val_ACC: 0.6818\n",
      "Epoch 276/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5486 - ACC: 0.6940\n",
      "Epoch 00276: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5531 - ACC: 0.6922 - val_loss: 0.6113 - val_ACC: 0.6753\n",
      "Epoch 277/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5499 - ACC: 0.6960\n",
      "Epoch 00277: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5506 - ACC: 0.7003 - val_loss: 0.6159 - val_ACC: 0.6753\n",
      "Epoch 278/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5395 - ACC: 0.7080\n",
      "Epoch 00278: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5507 - ACC: 0.7003 - val_loss: 0.6172 - val_ACC: 0.6688\n",
      "Epoch 279/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5563 - ACC: 0.7020\n",
      "Epoch 00279: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5507 - ACC: 0.7003 - val_loss: 0.6134 - val_ACC: 0.6818\n",
      "Epoch 280/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5419 - ACC: 0.7060\n",
      "Epoch 00280: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5500 - ACC: 0.7036 - val_loss: 0.6099 - val_ACC: 0.6883\n",
      "Epoch 281/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5535 - ACC: 0.6960\n",
      "Epoch 00281: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5508 - ACC: 0.7020 - val_loss: 0.6093 - val_ACC: 0.6883\n",
      "Epoch 282/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5472 - ACC: 0.7060\n",
      "Epoch 00282: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5511 - ACC: 0.7003 - val_loss: 0.6110 - val_ACC: 0.6818\n",
      "Epoch 283/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5495 - ACC: 0.6980\n",
      "Epoch 00283: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5498 - ACC: 0.7036 - val_loss: 0.6166 - val_ACC: 0.6688\n",
      "Epoch 284/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5655 - ACC: 0.6980\n",
      "Epoch 00284: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5503 - ACC: 0.7052 - val_loss: 0.6235 - val_ACC: 0.6818\n",
      "Epoch 285/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5449 - ACC: 0.7060\n",
      "Epoch 00285: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5529 - ACC: 0.7036 - val_loss: 0.6225 - val_ACC: 0.6818\n",
      "Epoch 286/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5612 - ACC: 0.7080\n",
      "Epoch 00286: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5524 - ACC: 0.7052 - val_loss: 0.6164 - val_ACC: 0.6688\n",
      "Epoch 287/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5533 - ACC: 0.6960\n",
      "Epoch 00287: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5503 - ACC: 0.7068 - val_loss: 0.6131 - val_ACC: 0.6818\n",
      "Epoch 288/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5624 - ACC: 0.6820\n",
      "Epoch 00288: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5494 - ACC: 0.7003 - val_loss: 0.6125 - val_ACC: 0.6818\n",
      "Epoch 289/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5559 - ACC: 0.6920\n",
      "Epoch 00289: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5493 - ACC: 0.6987 - val_loss: 0.6119 - val_ACC: 0.6883\n",
      "Epoch 290/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5395 - ACC: 0.7060\n",
      "Epoch 00290: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5491 - ACC: 0.7020 - val_loss: 0.6108 - val_ACC: 0.6948\n",
      "Epoch 291/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5341 - ACC: 0.7200\n",
      "Epoch 00291: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5494 - ACC: 0.7052 - val_loss: 0.6106 - val_ACC: 0.6948\n",
      "Epoch 292/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5604 - ACC: 0.6940\n",
      "Epoch 00292: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5494 - ACC: 0.7020 - val_loss: 0.6110 - val_ACC: 0.6818\n",
      "Epoch 293/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5518 - ACC: 0.7060\n",
      "Epoch 00293: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5489 - ACC: 0.7020 - val_loss: 0.6133 - val_ACC: 0.6818\n",
      "Epoch 294/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5583 - ACC: 0.6960\n",
      "Epoch 00294: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5485 - ACC: 0.7036 - val_loss: 0.6169 - val_ACC: 0.6753\n",
      "Epoch 295/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5441 - ACC: 0.7060\n",
      "Epoch 00295: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5493 - ACC: 0.7036 - val_loss: 0.6165 - val_ACC: 0.6753\n",
      "Epoch 296/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5402 - ACC: 0.7120\n",
      "Epoch 00296: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5493 - ACC: 0.7020 - val_loss: 0.6141 - val_ACC: 0.6753\n",
      "Epoch 297/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5407 - ACC: 0.7140\n",
      "Epoch 00297: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5482 - ACC: 0.7052 - val_loss: 0.6135 - val_ACC: 0.6818\n",
      "Epoch 298/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5589 - ACC: 0.6940\n",
      "Epoch 00298: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5482 - ACC: 0.7020 - val_loss: 0.6134 - val_ACC: 0.6753\n",
      "Epoch 299/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5609 - ACC: 0.6960\n",
      "Epoch 00299: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5482 - ACC: 0.7020 - val_loss: 0.6135 - val_ACC: 0.6753\n",
      "Epoch 300/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5538 - ACC: 0.7000\n",
      "Epoch 00300: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5481 - ACC: 0.7036 - val_loss: 0.6138 - val_ACC: 0.6883\n",
      "Epoch 301/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5562 - ACC: 0.6960\n",
      "Epoch 00301: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5485 - ACC: 0.7052 - val_loss: 0.6163 - val_ACC: 0.6883\n",
      "Epoch 302/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5610 - ACC: 0.6940\n",
      "Epoch 00302: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5484 - ACC: 0.7068 - val_loss: 0.6211 - val_ACC: 0.6688\n",
      "Epoch 303/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5451 - ACC: 0.7060\n",
      "Epoch 00303: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5489 - ACC: 0.7085 - val_loss: 0.6217 - val_ACC: 0.6688\n",
      "Epoch 304/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5482 - ACC: 0.7040\n",
      "Epoch 00304: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5494 - ACC: 0.7068 - val_loss: 0.6203 - val_ACC: 0.6688\n",
      "Epoch 305/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5526 - ACC: 0.7140\n",
      "Epoch 00305: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5488 - ACC: 0.7068 - val_loss: 0.6177 - val_ACC: 0.6818\n",
      "Epoch 306/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5588 - ACC: 0.6960\n",
      "Epoch 00306: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5489 - ACC: 0.7068 - val_loss: 0.6150 - val_ACC: 0.6753\n",
      "Epoch 307/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5370 - ACC: 0.7160\n",
      "Epoch 00307: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5476 - ACC: 0.7052 - val_loss: 0.6170 - val_ACC: 0.6753\n",
      "Epoch 308/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5485 - ACC: 0.7100\n",
      "Epoch 00308: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5478 - ACC: 0.7085 - val_loss: 0.6192 - val_ACC: 0.6753\n",
      "Epoch 309/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5528 - ACC: 0.7080\n",
      "Epoch 00309: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5486 - ACC: 0.7101 - val_loss: 0.6169 - val_ACC: 0.6688\n",
      "Epoch 310/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5557 - ACC: 0.7020\n",
      "Epoch 00310: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5480 - ACC: 0.7101 - val_loss: 0.6122 - val_ACC: 0.6753\n",
      "Epoch 311/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5424 - ACC: 0.7000\n",
      "Epoch 00311: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5480 - ACC: 0.7020 - val_loss: 0.6115 - val_ACC: 0.6753\n",
      "Epoch 312/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5360 - ACC: 0.7100\n",
      "Epoch 00312: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5474 - ACC: 0.7003 - val_loss: 0.6144 - val_ACC: 0.6688\n",
      "Epoch 313/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5571 - ACC: 0.6980\n",
      "Epoch 00313: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5479 - ACC: 0.7101 - val_loss: 0.6139 - val_ACC: 0.6688\n",
      "Epoch 314/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5475 - ACC: 0.6960\n",
      "Epoch 00314: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5472 - ACC: 0.7085 - val_loss: 0.6115 - val_ACC: 0.6818\n",
      "Epoch 315/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5420 - ACC: 0.7060\n",
      "Epoch 00315: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5468 - ACC: 0.7068 - val_loss: 0.6108 - val_ACC: 0.6883\n",
      "Epoch 316/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5478 - ACC: 0.7040\n",
      "Epoch 00316: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5485 - ACC: 0.7003 - val_loss: 0.6116 - val_ACC: 0.6883\n",
      "Epoch 317/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5529 - ACC: 0.7000\n",
      "Epoch 00317: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5481 - ACC: 0.7036 - val_loss: 0.6148 - val_ACC: 0.6818\n",
      "Epoch 318/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5516 - ACC: 0.7040\n",
      "Epoch 00318: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5471 - ACC: 0.7052 - val_loss: 0.6220 - val_ACC: 0.6688\n",
      "Epoch 319/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5500 - ACC: 0.7180\n",
      "Epoch 00319: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5483 - ACC: 0.7085 - val_loss: 0.6235 - val_ACC: 0.6753\n",
      "Epoch 320/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5491 - ACC: 0.7000\n",
      "Epoch 00320: val_loss did not improve from 0.60925\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5491 - ACC: 0.7052 - val_loss: 0.6174 - val_ACC: 0.6753\n",
      "Epoch 321/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5543 - ACC: 0.7000\n",
      "Epoch 00321: val_loss improved from 0.60925 to 0.60805, saving model to ./pima_model\\321 - 0.6081.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5469 - ACC: 0.7101 - val_loss: 0.6081 - val_ACC: 0.6883\n",
      "Epoch 322/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5438 - ACC: 0.7020\n",
      "Epoch 00322: val_loss improved from 0.60805 to 0.60713, saving model to ./pima_model\\322 - 0.6071.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5471 - ACC: 0.6987 - val_loss: 0.6071 - val_ACC: 0.6883\n",
      "Epoch 323/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5475 - ACC: 0.7040\n",
      "Epoch 00323: val_loss improved from 0.60713 to 0.60692, saving model to ./pima_model\\323 - 0.6069.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5525 - ACC: 0.6971 - val_loss: 0.6069 - val_ACC: 0.7013\n",
      "Epoch 324/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5525 - ACC: 0.6920\n",
      "Epoch 00324: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5468 - ACC: 0.7003 - val_loss: 0.6192 - val_ACC: 0.6818\n",
      "Epoch 325/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5613 - ACC: 0.7060\n",
      "Epoch 00325: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5493 - ACC: 0.7085 - val_loss: 0.6357 - val_ACC: 0.6688\n",
      "Epoch 326/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5520 - ACC: 0.7080\n",
      "Epoch 00326: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5563 - ACC: 0.7003 - val_loss: 0.6245 - val_ACC: 0.6818\n",
      "Epoch 327/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5657 - ACC: 0.6980\n",
      "Epoch 00327: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5502 - ACC: 0.7036 - val_loss: 0.6104 - val_ACC: 0.6948\n",
      "Epoch 328/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5357 - ACC: 0.7200\n",
      "Epoch 00328: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5463 - ACC: 0.7068 - val_loss: 0.6098 - val_ACC: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5470 - ACC: 0.7160\n",
      "Epoch 00329: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5508 - ACC: 0.7036 - val_loss: 0.6096 - val_ACC: 0.6818\n",
      "Epoch 330/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5409 - ACC: 0.7140\n",
      "Epoch 00330: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5509 - ACC: 0.7068 - val_loss: 0.6090 - val_ACC: 0.7013\n",
      "Epoch 331/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5439 - ACC: 0.7000\n",
      "Epoch 00331: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5464 - ACC: 0.7020 - val_loss: 0.6173 - val_ACC: 0.6688\n",
      "Epoch 332/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5467 - ACC: 0.6980\n",
      "Epoch 00332: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5469 - ACC: 0.7052 - val_loss: 0.6264 - val_ACC: 0.6818\n",
      "Epoch 333/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5498 - ACC: 0.7080\n",
      "Epoch 00333: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5511 - ACC: 0.7036 - val_loss: 0.6210 - val_ACC: 0.6818\n",
      "Epoch 334/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5513 - ACC: 0.7080\n",
      "Epoch 00334: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5488 - ACC: 0.7085 - val_loss: 0.6103 - val_ACC: 0.6818\n",
      "Epoch 335/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5504 - ACC: 0.7000\n",
      "Epoch 00335: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5467 - ACC: 0.7052 - val_loss: 0.6084 - val_ACC: 0.6948\n",
      "Epoch 336/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5584 - ACC: 0.6880\n",
      "Epoch 00336: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5459 - ACC: 0.7020 - val_loss: 0.6119 - val_ACC: 0.6818\n",
      "Epoch 337/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5481 - ACC: 0.7020\n",
      "Epoch 00337: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5454 - ACC: 0.7068 - val_loss: 0.6170 - val_ACC: 0.6688\n",
      "Epoch 338/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5416 - ACC: 0.7180\n",
      "Epoch 00338: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5465 - ACC: 0.7085 - val_loss: 0.6176 - val_ACC: 0.6818\n",
      "Epoch 339/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5437 - ACC: 0.7100\n",
      "Epoch 00339: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5464 - ACC: 0.7085 - val_loss: 0.6132 - val_ACC: 0.6753\n",
      "Epoch 340/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5520 - ACC: 0.6960\n",
      "Epoch 00340: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5456 - ACC: 0.7085 - val_loss: 0.6109 - val_ACC: 0.6753\n",
      "Epoch 341/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5530 - ACC: 0.7120\n",
      "Epoch 00341: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5449 - ACC: 0.7085 - val_loss: 0.6122 - val_ACC: 0.6818\n",
      "Epoch 342/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5397 - ACC: 0.7040\n",
      "Epoch 00342: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5451 - ACC: 0.7101 - val_loss: 0.6130 - val_ACC: 0.6818\n",
      "Epoch 343/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5443 - ACC: 0.7100\n",
      "Epoch 00343: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5446 - ACC: 0.7101 - val_loss: 0.6116 - val_ACC: 0.6883\n",
      "Epoch 344/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5502 - ACC: 0.6880\n",
      "Epoch 00344: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5451 - ACC: 0.7052 - val_loss: 0.6119 - val_ACC: 0.6818\n",
      "Epoch 345/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5363 - ACC: 0.7080\n",
      "Epoch 00345: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5448 - ACC: 0.7085 - val_loss: 0.6137 - val_ACC: 0.6753\n",
      "Epoch 346/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5263 - ACC: 0.7240\n",
      "Epoch 00346: val_loss did not improve from 0.60692\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5440 - ACC: 0.7101 - val_loss: 0.6093 - val_ACC: 0.6753\n",
      "Epoch 347/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5343 - ACC: 0.7220\n",
      "Epoch 00347: val_loss improved from 0.60692 to 0.60425, saving model to ./pima_model\\347 - 0.6043.hdf5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5433 - ACC: 0.7101 - val_loss: 0.6043 - val_ACC: 0.6948\n",
      "Epoch 348/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5451 - ACC: 0.7140\n",
      "Epoch 00348: val_loss improved from 0.60425 to 0.60267, saving model to ./pima_model\\348 - 0.6027.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5446 - ACC: 0.7068 - val_loss: 0.6027 - val_ACC: 0.6883\n",
      "Epoch 349/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5462 - ACC: 0.7000\n",
      "Epoch 00349: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5458 - ACC: 0.6987 - val_loss: 0.6036 - val_ACC: 0.6948\n",
      "Epoch 350/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5525 - ACC: 0.6980\n",
      "Epoch 00350: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5435 - ACC: 0.7068 - val_loss: 0.6109 - val_ACC: 0.6753\n",
      "Epoch 351/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5291 - ACC: 0.7320\n",
      "Epoch 00351: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5440 - ACC: 0.7085 - val_loss: 0.6185 - val_ACC: 0.6818\n",
      "Epoch 352/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5440 - ACC: 0.7020\n",
      "Epoch 00352: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5449 - ACC: 0.7052 - val_loss: 0.6139 - val_ACC: 0.6818\n",
      "Epoch 353/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5408 - ACC: 0.7020\n",
      "Epoch 00353: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5424 - ACC: 0.7068 - val_loss: 0.6091 - val_ACC: 0.6883\n",
      "Epoch 354/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5505 - ACC: 0.6980\n",
      "Epoch 00354: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5421 - ACC: 0.7101 - val_loss: 0.6090 - val_ACC: 0.6883\n",
      "Epoch 355/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5511 - ACC: 0.6980\n",
      "Epoch 00355: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5417 - ACC: 0.7036 - val_loss: 0.6096 - val_ACC: 0.6948\n",
      "Epoch 356/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5315 - ACC: 0.7220\n",
      "Epoch 00356: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5411 - ACC: 0.7117 - val_loss: 0.6086 - val_ACC: 0.6948\n",
      "Epoch 357/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5289 - ACC: 0.7160\n",
      "Epoch 00357: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5397 - ACC: 0.7134 - val_loss: 0.6054 - val_ACC: 0.6883\n",
      "Epoch 358/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5458 - ACC: 0.7040\n",
      "Epoch 00358: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5411 - ACC: 0.7117 - val_loss: 0.6059 - val_ACC: 0.6948\n",
      "Epoch 359/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5411 - ACC: 0.7060\n",
      "Epoch 00359: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5414 - ACC: 0.7101 - val_loss: 0.6118 - val_ACC: 0.6883\n",
      "Epoch 360/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5425 - ACC: 0.7100\n",
      "Epoch 00360: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5427 - ACC: 0.7085 - val_loss: 0.6095 - val_ACC: 0.6818\n",
      "Epoch 361/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5349 - ACC: 0.7020\n",
      "Epoch 00361: val_loss did not improve from 0.60267\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5414 - ACC: 0.7085 - val_loss: 0.6051 - val_ACC: 0.6883\n",
      "Epoch 362/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5528 - ACC: 0.7060\n",
      "Epoch 00362: val_loss improved from 0.60267 to 0.60242, saving model to ./pima_model\\362 - 0.6024.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5396 - ACC: 0.7085 - val_loss: 0.6024 - val_ACC: 0.6883\n",
      "Epoch 363/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5440 - ACC: 0.7040\n",
      "Epoch 00363: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5405 - ACC: 0.7085 - val_loss: 0.6041 - val_ACC: 0.6818\n",
      "Epoch 364/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5435 - ACC: 0.6960\n",
      "Epoch 00364: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5419 - ACC: 0.7101 - val_loss: 0.6076 - val_ACC: 0.6753\n",
      "Epoch 365/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5499 - ACC: 0.6860\n",
      "Epoch 00365: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5399 - ACC: 0.7085 - val_loss: 0.6144 - val_ACC: 0.6883\n",
      "Epoch 366/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5533 - ACC: 0.6960\n",
      "Epoch 00366: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5387 - ACC: 0.7068 - val_loss: 0.6192 - val_ACC: 0.6558\n",
      "Epoch 367/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5417 - ACC: 0.7000\n",
      "Epoch 00367: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5400 - ACC: 0.7020 - val_loss: 0.6204 - val_ACC: 0.6558\n",
      "Epoch 368/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5371 - ACC: 0.7120\n",
      "Epoch 00368: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5406 - ACC: 0.7068 - val_loss: 0.6168 - val_ACC: 0.6688\n",
      "Epoch 369/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5466 - ACC: 0.6920\n",
      "Epoch 00369: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5392 - ACC: 0.7036 - val_loss: 0.6110 - val_ACC: 0.6883\n",
      "Epoch 370/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5326 - ACC: 0.7140\n",
      "Epoch 00370: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5382 - ACC: 0.7199 - val_loss: 0.6087 - val_ACC: 0.6818\n",
      "Epoch 371/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5333 - ACC: 0.7220\n",
      "Epoch 00371: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5390 - ACC: 0.7166 - val_loss: 0.6075 - val_ACC: 0.6818\n",
      "Epoch 372/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5434 - ACC: 0.7060\n",
      "Epoch 00372: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5396 - ACC: 0.7101 - val_loss: 0.6070 - val_ACC: 0.6948\n",
      "Epoch 373/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5469 - ACC: 0.7000\n",
      "Epoch 00373: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5384 - ACC: 0.7117 - val_loss: 0.6102 - val_ACC: 0.6948\n",
      "Epoch 374/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5222 - ACC: 0.7200\n",
      "Epoch 00374: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5372 - ACC: 0.7068 - val_loss: 0.6119 - val_ACC: 0.6753\n",
      "Epoch 375/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5321 - ACC: 0.7080\n",
      "Epoch 00375: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5371 - ACC: 0.7068 - val_loss: 0.6093 - val_ACC: 0.6948\n",
      "Epoch 376/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5396 - ACC: 0.7080\n",
      "Epoch 00376: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5361 - ACC: 0.7134 - val_loss: 0.6075 - val_ACC: 0.6753\n",
      "Epoch 377/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5426 - ACC: 0.7020\n",
      "Epoch 00377: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5389 - ACC: 0.7134 - val_loss: 0.6079 - val_ACC: 0.6753\n",
      "Epoch 378/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5409 - ACC: 0.7120\n",
      "Epoch 00378: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5387 - ACC: 0.7166 - val_loss: 0.6101 - val_ACC: 0.6883\n",
      "Epoch 379/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5414 - ACC: 0.7180\n",
      "Epoch 00379: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5366 - ACC: 0.7134 - val_loss: 0.6116 - val_ACC: 0.6948\n",
      "Epoch 380/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5409 - ACC: 0.7020\n",
      "Epoch 00380: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5366 - ACC: 0.7068 - val_loss: 0.6123 - val_ACC: 0.6818\n",
      "Epoch 381/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5359 - ACC: 0.7060\n",
      "Epoch 00381: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5367 - ACC: 0.7068 - val_loss: 0.6124 - val_ACC: 0.6753\n",
      "Epoch 382/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5331 - ACC: 0.7060\n",
      "Epoch 00382: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5366 - ACC: 0.7068 - val_loss: 0.6086 - val_ACC: 0.7013\n",
      "Epoch 383/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5456 - ACC: 0.6920\n",
      "Epoch 00383: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5363 - ACC: 0.7085 - val_loss: 0.6060 - val_ACC: 0.6948\n",
      "Epoch 384/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5318 - ACC: 0.7200\n",
      "Epoch 00384: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5357 - ACC: 0.7134 - val_loss: 0.6077 - val_ACC: 0.7013\n",
      "Epoch 385/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5288 - ACC: 0.7220\n",
      "Epoch 00385: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5368 - ACC: 0.7052 - val_loss: 0.6090 - val_ACC: 0.6818\n",
      "Epoch 386/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5308 - ACC: 0.7160\n",
      "Epoch 00386: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5360 - ACC: 0.7052 - val_loss: 0.6064 - val_ACC: 0.6948\n",
      "Epoch 387/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5308 - ACC: 0.7180\n",
      "Epoch 00387: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5352 - ACC: 0.7117 - val_loss: 0.6060 - val_ACC: 0.6818\n",
      "Epoch 388/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5393 - ACC: 0.7140\n",
      "Epoch 00388: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5385 - ACC: 0.7101 - val_loss: 0.6073 - val_ACC: 0.6818\n",
      "Epoch 389/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5344 - ACC: 0.7060\n",
      "Epoch 00389: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5398 - ACC: 0.7101 - val_loss: 0.6077 - val_ACC: 0.6883\n",
      "Epoch 390/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5461 - ACC: 0.7160\n",
      "Epoch 00390: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5347 - ACC: 0.7231 - val_loss: 0.6168 - val_ACC: 0.6753\n",
      "Epoch 391/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5314 - ACC: 0.7220\n",
      "Epoch 00391: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5365 - ACC: 0.7085 - val_loss: 0.6317 - val_ACC: 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5547 - ACC: 0.6940\n",
      "Epoch 00392: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5444 - ACC: 0.7020 - val_loss: 0.6318 - val_ACC: 0.6688\n",
      "Epoch 393/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5490 - ACC: 0.7000\n",
      "Epoch 00393: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5440 - ACC: 0.7003 - val_loss: 0.6151 - val_ACC: 0.6753\n",
      "Epoch 394/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5274 - ACC: 0.7180\n",
      "Epoch 00394: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5358 - ACC: 0.7085 - val_loss: 0.6045 - val_ACC: 0.6883\n",
      "Epoch 395/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5170 - ACC: 0.7360\n",
      "Epoch 00395: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5355 - ACC: 0.7166 - val_loss: 0.6044 - val_ACC: 0.6753\n",
      "Epoch 396/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5445 - ACC: 0.7040\n",
      "Epoch 00396: val_loss did not improve from 0.60242\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5414 - ACC: 0.7036 - val_loss: 0.6027 - val_ACC: 0.6818\n",
      "Epoch 397/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5443 - ACC: 0.7140\n",
      "Epoch 00397: val_loss improved from 0.60242 to 0.60213, saving model to ./pima_model\\397 - 0.6021.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5397 - ACC: 0.7117 - val_loss: 0.6021 - val_ACC: 0.6883\n",
      "Epoch 398/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5360 - ACC: 0.7180\n",
      "Epoch 00398: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5348 - ACC: 0.7182 - val_loss: 0.6113 - val_ACC: 0.6818\n",
      "Epoch 399/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5403 - ACC: 0.7200\n",
      "Epoch 00399: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5370 - ACC: 0.7150 - val_loss: 0.6191 - val_ACC: 0.6883\n",
      "Epoch 400/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5445 - ACC: 0.7100\n",
      "Epoch 00400: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5397 - ACC: 0.7085 - val_loss: 0.6129 - val_ACC: 0.6883\n",
      "Epoch 401/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5451 - ACC: 0.7080\n",
      "Epoch 00401: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5356 - ACC: 0.7150 - val_loss: 0.6047 - val_ACC: 0.6883\n",
      "Epoch 402/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5371 - ACC: 0.7180\n",
      "Epoch 00402: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5342 - ACC: 0.7199 - val_loss: 0.6050 - val_ACC: 0.6753\n",
      "Epoch 403/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5393 - ACC: 0.7160\n",
      "Epoch 00403: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5372 - ACC: 0.7166 - val_loss: 0.6055 - val_ACC: 0.6818\n",
      "Epoch 404/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5422 - ACC: 0.7200\n",
      "Epoch 00404: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5344 - ACC: 0.7231 - val_loss: 0.6116 - val_ACC: 0.6753\n",
      "Epoch 405/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5405 - ACC: 0.7060\n",
      "Epoch 00405: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5344 - ACC: 0.7101 - val_loss: 0.6211 - val_ACC: 0.6688\n",
      "Epoch 406/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5365 - ACC: 0.7100\n",
      "Epoch 00406: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5381 - ACC: 0.7101 - val_loss: 0.6150 - val_ACC: 0.6753\n",
      "Epoch 407/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5430 - ACC: 0.7060\n",
      "Epoch 00407: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5347 - ACC: 0.7150 - val_loss: 0.6056 - val_ACC: 0.6883\n",
      "Epoch 408/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5220 - ACC: 0.7340\n",
      "Epoch 00408: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5336 - ACC: 0.7215 - val_loss: 0.6040 - val_ACC: 0.6753\n",
      "Epoch 409/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5329 - ACC: 0.7160\n",
      "Epoch 00409: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5357 - ACC: 0.7166 - val_loss: 0.6031 - val_ACC: 0.6883\n",
      "Epoch 410/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5377 - ACC: 0.7240\n",
      "Epoch 00410: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5336 - ACC: 0.7231 - val_loss: 0.6061 - val_ACC: 0.6883\n",
      "Epoch 411/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5221 - ACC: 0.7160\n",
      "Epoch 00411: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5324 - ACC: 0.7134 - val_loss: 0.6148 - val_ACC: 0.6818\n",
      "Epoch 412/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5266 - ACC: 0.7180\n",
      "Epoch 00412: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5357 - ACC: 0.7134 - val_loss: 0.6158 - val_ACC: 0.6883\n",
      "Epoch 413/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5386 - ACC: 0.7140\n",
      "Epoch 00413: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5364 - ACC: 0.7117 - val_loss: 0.6088 - val_ACC: 0.6753\n",
      "Epoch 414/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5249 - ACC: 0.7160\n",
      "Epoch 00414: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5340 - ACC: 0.7085 - val_loss: 0.6046 - val_ACC: 0.6883\n",
      "Epoch 415/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5274 - ACC: 0.7080\n",
      "Epoch 00415: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5327 - ACC: 0.7085 - val_loss: 0.6049 - val_ACC: 0.6883\n",
      "Epoch 416/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5353 - ACC: 0.7040\n",
      "Epoch 00416: val_loss did not improve from 0.60213\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5333 - ACC: 0.7068 - val_loss: 0.6049 - val_ACC: 0.6818\n",
      "Epoch 417/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5240 - ACC: 0.7200\n",
      "Epoch 00417: val_loss improved from 0.60213 to 0.60032, saving model to ./pima_model\\417 - 0.6003.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5321 - ACC: 0.7085 - val_loss: 0.6003 - val_ACC: 0.6883\n",
      "Epoch 418/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5350 - ACC: 0.7140\n",
      "Epoch 00418: val_loss improved from 0.60032 to 0.60012, saving model to ./pima_model\\418 - 0.6001.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5335 - ACC: 0.7199 - val_loss: 0.6001 - val_ACC: 0.6948\n",
      "Epoch 419/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5309 - ACC: 0.7200\n",
      "Epoch 00419: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5343 - ACC: 0.7215 - val_loss: 0.6020 - val_ACC: 0.6818\n",
      "Epoch 420/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5161 - ACC: 0.7380\n",
      "Epoch 00420: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5321 - ACC: 0.7150 - val_loss: 0.6084 - val_ACC: 0.6818\n",
      "Epoch 421/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5427 - ACC: 0.7000\n",
      "Epoch 00421: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5326 - ACC: 0.7117 - val_loss: 0.6177 - val_ACC: 0.6883\n",
      "Epoch 422/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5563 - ACC: 0.6960\n",
      "Epoch 00422: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5366 - ACC: 0.7101 - val_loss: 0.6266 - val_ACC: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5236 - ACC: 0.7200\n",
      "Epoch 00423: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5414 - ACC: 0.7052 - val_loss: 0.6178 - val_ACC: 0.6818\n",
      "Epoch 424/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5538 - ACC: 0.7080\n",
      "Epoch 00424: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5362 - ACC: 0.7117 - val_loss: 0.6045 - val_ACC: 0.6818\n",
      "Epoch 425/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5349 - ACC: 0.7160\n",
      "Epoch 00425: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5320 - ACC: 0.7199 - val_loss: 0.6037 - val_ACC: 0.6753\n",
      "Epoch 426/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5537 - ACC: 0.7020\n",
      "Epoch 00426: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5345 - ACC: 0.7199 - val_loss: 0.6032 - val_ACC: 0.6818\n",
      "Epoch 427/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5315 - ACC: 0.7340\n",
      "Epoch 00427: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5322 - ACC: 0.7215 - val_loss: 0.6065 - val_ACC: 0.6883\n",
      "Epoch 428/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5465 - ACC: 0.6980\n",
      "Epoch 00428: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5312 - ACC: 0.7101 - val_loss: 0.6133 - val_ACC: 0.6883\n",
      "Epoch 429/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5280 - ACC: 0.7100\n",
      "Epoch 00429: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5350 - ACC: 0.7117 - val_loss: 0.6140 - val_ACC: 0.6883\n",
      "Epoch 430/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5465 - ACC: 0.7200\n",
      "Epoch 00430: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5344 - ACC: 0.7117 - val_loss: 0.6046 - val_ACC: 0.7013\n",
      "Epoch 431/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5219 - ACC: 0.7300\n",
      "Epoch 00431: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5318 - ACC: 0.7199 - val_loss: 0.6033 - val_ACC: 0.6753\n",
      "Epoch 432/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5320 - ACC: 0.7120\n",
      "Epoch 00432: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5352 - ACC: 0.7182 - val_loss: 0.6048 - val_ACC: 0.6818\n",
      "Epoch 433/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5197 - ACC: 0.7220\n",
      "Epoch 00433: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5372 - ACC: 0.7068 - val_loss: 0.6036 - val_ACC: 0.6818\n",
      "Epoch 434/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5246 - ACC: 0.7380\n",
      "Epoch 00434: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5327 - ACC: 0.7215 - val_loss: 0.6085 - val_ACC: 0.7013\n",
      "Epoch 435/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5415 - ACC: 0.7020\n",
      "Epoch 00435: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5317 - ACC: 0.7101 - val_loss: 0.6241 - val_ACC: 0.6753\n",
      "Epoch 436/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5397 - ACC: 0.7000\n",
      "Epoch 00436: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5373 - ACC: 0.7020 - val_loss: 0.6225 - val_ACC: 0.6753\n",
      "Epoch 437/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5413 - ACC: 0.6960\n",
      "Epoch 00437: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5358 - ACC: 0.7052 - val_loss: 0.6100 - val_ACC: 0.6883\n",
      "Epoch 438/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5276 - ACC: 0.7200\n",
      "Epoch 00438: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5301 - ACC: 0.7134 - val_loss: 0.6039 - val_ACC: 0.6883\n",
      "Epoch 439/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5238 - ACC: 0.7280\n",
      "Epoch 00439: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5317 - ACC: 0.7215 - val_loss: 0.6032 - val_ACC: 0.6688\n",
      "Epoch 440/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5290 - ACC: 0.7300\n",
      "Epoch 00440: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5343 - ACC: 0.7182 - val_loss: 0.6014 - val_ACC: 0.6883\n",
      "Epoch 441/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5366 - ACC: 0.7200\n",
      "Epoch 00441: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5316 - ACC: 0.7215 - val_loss: 0.6032 - val_ACC: 0.6818\n",
      "Epoch 442/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5361 - ACC: 0.7160\n",
      "Epoch 00442: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5306 - ACC: 0.7117 - val_loss: 0.6092 - val_ACC: 0.6818\n",
      "Epoch 443/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5414 - ACC: 0.7060\n",
      "Epoch 00443: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5325 - ACC: 0.7150 - val_loss: 0.6103 - val_ACC: 0.6883\n",
      "Epoch 444/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5310 - ACC: 0.7120\n",
      "Epoch 00444: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5325 - ACC: 0.7150 - val_loss: 0.6074 - val_ACC: 0.6818\n",
      "Epoch 445/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5352 - ACC: 0.7140\n",
      "Epoch 00445: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5314 - ACC: 0.7085 - val_loss: 0.6041 - val_ACC: 0.6883\n",
      "Epoch 446/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5363 - ACC: 0.7060\n",
      "Epoch 00446: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5299 - ACC: 0.7117 - val_loss: 0.6033 - val_ACC: 0.6818\n",
      "Epoch 447/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5219 - ACC: 0.7360\n",
      "Epoch 00447: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5301 - ACC: 0.7215 - val_loss: 0.6043 - val_ACC: 0.6883\n",
      "Epoch 448/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5408 - ACC: 0.7120\n",
      "Epoch 00448: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5309 - ACC: 0.7248 - val_loss: 0.6059 - val_ACC: 0.6883\n",
      "Epoch 449/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5321 - ACC: 0.7300\n",
      "Epoch 00449: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5296 - ACC: 0.7264 - val_loss: 0.6084 - val_ACC: 0.6883\n",
      "Epoch 450/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5262 - ACC: 0.7120\n",
      "Epoch 00450: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5295 - ACC: 0.7117 - val_loss: 0.6108 - val_ACC: 0.6818\n",
      "Epoch 451/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5355 - ACC: 0.6980\n",
      "Epoch 00451: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5302 - ACC: 0.7101 - val_loss: 0.6070 - val_ACC: 0.6883\n",
      "Epoch 452/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5273 - ACC: 0.7160\n",
      "Epoch 00452: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5291 - ACC: 0.7101 - val_loss: 0.6035 - val_ACC: 0.6818\n",
      "Epoch 453/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5336 - ACC: 0.7220\n",
      "Epoch 00453: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5294 - ACC: 0.7248 - val_loss: 0.6031 - val_ACC: 0.6818\n",
      "Epoch 454/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5466 - ACC: 0.7080\n",
      "Epoch 00454: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5310 - ACC: 0.7248 - val_loss: 0.6034 - val_ACC: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5335 - ACC: 0.7240\n",
      "Epoch 00455: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5301 - ACC: 0.7166 - val_loss: 0.6082 - val_ACC: 0.6818\n",
      "Epoch 456/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5377 - ACC: 0.7080\n",
      "Epoch 00456: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5305 - ACC: 0.7101 - val_loss: 0.6072 - val_ACC: 0.6818\n",
      "Epoch 457/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5388 - ACC: 0.7120\n",
      "Epoch 00457: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5306 - ACC: 0.7101 - val_loss: 0.6034 - val_ACC: 0.6753\n",
      "Epoch 458/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5281 - ACC: 0.7140\n",
      "Epoch 00458: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5294 - ACC: 0.7182 - val_loss: 0.6016 - val_ACC: 0.6883\n",
      "Epoch 459/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5292 - ACC: 0.7260\n",
      "Epoch 00459: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5300 - ACC: 0.7231 - val_loss: 0.6032 - val_ACC: 0.6753\n",
      "Epoch 460/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5333 - ACC: 0.7200\n",
      "Epoch 00460: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5332 - ACC: 0.7166 - val_loss: 0.6042 - val_ACC: 0.6818\n",
      "Epoch 461/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5292 - ACC: 0.7180\n",
      "Epoch 00461: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5323 - ACC: 0.7166 - val_loss: 0.6045 - val_ACC: 0.6883\n",
      "Epoch 462/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5155 - ACC: 0.7280\n",
      "Epoch 00462: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5296 - ACC: 0.7215 - val_loss: 0.6105 - val_ACC: 0.6818\n",
      "Epoch 463/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5197 - ACC: 0.7120\n",
      "Epoch 00463: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5296 - ACC: 0.7052 - val_loss: 0.6115 - val_ACC: 0.6818\n",
      "Epoch 464/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5265 - ACC: 0.7080\n",
      "Epoch 00464: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5299 - ACC: 0.7052 - val_loss: 0.6087 - val_ACC: 0.6753\n",
      "Epoch 465/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5253 - ACC: 0.7240\n",
      "Epoch 00465: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5288 - ACC: 0.7150 - val_loss: 0.6078 - val_ACC: 0.6818\n",
      "Epoch 466/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5330 - ACC: 0.7200\n",
      "Epoch 00466: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5284 - ACC: 0.7166 - val_loss: 0.6094 - val_ACC: 0.6753\n",
      "Epoch 467/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5286 - ACC: 0.7040\n",
      "Epoch 00467: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5285 - ACC: 0.7052 - val_loss: 0.6118 - val_ACC: 0.6818\n",
      "Epoch 468/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5418 - ACC: 0.6980\n",
      "Epoch 00468: val_loss did not improve from 0.60012\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5297 - ACC: 0.7101 - val_loss: 0.6085 - val_ACC: 0.6753\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5449 - ACC: 0.7031\n",
      "\n",
      " ACC : 0.7031\n"
     ]
    }
   ],
   "source": [
    "# pima indian의 자료 중에서 30% 만 샘플로 추출\n",
    "# monitor는 에러(손실) 값\n",
    "# 기다리는 횟수는 50회\n",
    "# 저장은 pima_model dir에\n",
    "# 마지막 저장된 model을 가져와서 정확도의 값을 출력\n",
    "model_dir = \"./pima_model\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "modelpath = model_dir + \"/{epoch:02d} - {val_loss:.4f}.hdf5\"\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "df = pd.read_csv(\"./Dataset/pima-indians-diabetes.csv\", header = None)\n",
    "\n",
    "df.sample(frac=0.3)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = X.shape[1], activation = \"relu\"))\n",
    "model.add(Dense(12, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# model compile\n",
    "model.compile(loss = \"BCE\", optimizer = \"adam\", metrics=[\"ACC\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(modelpath, monitor = 'val_loss', # ACC 넣으면 정확도\n",
    "                               verbose = 1, save_best_only = True) \n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor = \"val_loss\", \n",
    "                                        patience = 50)\n",
    "\n",
    "model.fit(X, Y, validation_split = 0.2, epochs = 2000, batch_size = 500, \n",
    "         callbacks = [early_stopping_callback, checkpointer])\n",
    "\n",
    "print(\"\\n ACC : %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3ce7f",
   "metadata": {},
   "source": [
    "### 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33a148e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    dataset = df.values\n",
    "    X = dataset[:, :-1]\n",
    "    Y = dataset[:,-1]\n",
    "    return (X,Y)\n",
    "\n",
    "def create_model(X,Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30,  input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def create_dir(dir_name):\n",
    "    # 모델 저장 폴더 만들기\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f958bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = pd.read_csv('../dataset/pima-indians-diabetes.csv', header=None)\n",
    "df = df_pre.sample(frac=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y=create_dataset(df)\n",
    "model = create_model(X, Y)\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "create_dir('pima_model')\n",
    "modelpath = \"./DL/pima_model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "# 모델 업데이트 및 저장\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "model.fit(X, Y, validation_split=0.2, epochs=3500, \n",
    "          batch_size=500, verbose=0, \n",
    "          callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddeaf6",
   "metadata": {},
   "source": [
    "### 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cdc2244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 877us/step - loss: 0.5972 - ACC: 0.6849\n",
      "0.6848958134651184\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 30)                270       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 12)                372       \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 655\n",
      "Trainable params: 655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_1 = load_model(\"./pima_model/96 - 0.6361.hdf5\")\n",
    "print(model_1.evaluate(X, Y)[1])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d789e4",
   "metadata": {},
   "source": [
    "### 딥러닝으로 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d509adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4edb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Dataset/housing.csv\", delim_whitespace = True, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40a00df3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y = create_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "20c58822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 879us/step - loss: 1191.3270\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 823us/step - loss: 245.6603\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 922us/step - loss: 139.6486\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 855us/step - loss: 94.7137\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 902us/step - loss: 79.3995\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 936us/step - loss: 82.9331\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 73.9985\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 882us/step - loss: 69.8325\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 888us/step - loss: 73.9861\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 884us/step - loss: 65.4000\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 882us/step - loss: 63.0158\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 851us/step - loss: 64.8420\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 61.3537\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 876us/step - loss: 57.1261\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 845us/step - loss: 59.8069\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 56.1712\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 858us/step - loss: 56.1800\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 849us/step - loss: 51.6270\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 810us/step - loss: 49.4200\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 833us/step - loss: 48.8689\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 49.2338\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 885us/step - loss: 46.4862\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 940us/step - loss: 49.2547\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 886us/step - loss: 44.0892\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 845us/step - loss: 41.1119\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 45.0582\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 820us/step - loss: 43.0714\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 842us/step - loss: 41.0616\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 860us/step - loss: 41.2023\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 874us/step - loss: 38.1603\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 920us/step - loss: 39.9132\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 963us/step - loss: 37.1516\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 911us/step - loss: 40.5558\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 926us/step - loss: 36.6677\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 35.0194\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 882us/step - loss: 37.5221\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 888us/step - loss: 38.7938\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 815us/step - loss: 35.8496\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 837us/step - loss: 36.0445\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 802us/step - loss: 34.4908\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 40.5975\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 819us/step - loss: 34.7091\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 855us/step - loss: 39.2500\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 849us/step - loss: 34.1631\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 826us/step - loss: 35.8505\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 810us/step - loss: 32.3131\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 969us/step - loss: 32.9027\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.9407\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 997us/step - loss: 31.6244\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 982us/step - loss: 36.6971\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 859us/step - loss: 31.6074\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 864us/step - loss: 34.3457\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 822us/step - loss: 32.2847\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 924us/step - loss: 31.9099\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 878us/step - loss: 33.8562\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 838us/step - loss: 31.7354\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 889us/step - loss: 32.3788\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 841us/step - loss: 31.3487\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 913us/step - loss: 31.0727\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 928us/step - loss: 33.1733\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 856us/step - loss: 33.7645\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 837us/step - loss: 31.6093\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 34.9379\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 909us/step - loss: 41.1105\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 862us/step - loss: 31.8682\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 794us/step - loss: 35.2288\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 745us/step - loss: 33.4547\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 749us/step - loss: 29.1266\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 29.2323\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 788us/step - loss: 30.9865\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 814us/step - loss: 30.1599\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 724us/step - loss: 36.2300\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 810us/step - loss: 32.1231\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 764us/step - loss: 30.5637\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 803us/step - loss: 31.0528\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 734us/step - loss: 37.3882\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 771us/step - loss: 32.8027\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 31.6298\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 951us/step - loss: 28.5426\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 833us/step - loss: 27.8434\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 839us/step - loss: 28.5258\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 893us/step - loss: 28.0534\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 923us/step - loss: 30.1999\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 30.4664\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 826us/step - loss: 27.0103\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 838us/step - loss: 29.3878\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 857us/step - loss: 28.8970\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 894us/step - loss: 28.2746\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 852us/step - loss: 31.3573\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 789us/step - loss: 28.0293\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 720us/step - loss: 30.0379\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 824us/step - loss: 28.9246\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 933us/step - loss: 26.5396\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.1527\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.8295\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 997us/step - loss: 25.8896\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.0634\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 926us/step - loss: 25.4773\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 770us/step - loss: 25.9061\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 728us/step - loss: 26.7533\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 755us/step - loss: 28.3876\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 708us/step - loss: 26.2880\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 701us/step - loss: 26.4611\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 742us/step - loss: 27.9140\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 834us/step - loss: 30.0229\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 733us/step - loss: 26.6530\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 683us/step - loss: 26.3906\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 851us/step - loss: 28.0197\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.9398\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.5282\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.4866\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.2850\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 919us/step - loss: 29.2714\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 24.7385\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 23.9276\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 27.2607\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 737us/step - loss: 30.3573\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 820us/step - loss: 24.5687\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 793us/step - loss: 26.8554\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 698us/step - loss: 23.0320\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 741us/step - loss: 25.3321\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 712us/step - loss: 24.4727\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 736us/step - loss: 25.3180\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 773us/step - loss: 33.6558\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 25.2984\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 883us/step - loss: 25.0886\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 772us/step - loss: 25.8350\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 746us/step - loss: 23.3092\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 798us/step - loss: 29.6237\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 798us/step - loss: 26.4302\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 855us/step - loss: 29.8167\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 826us/step - loss: 26.7135\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 837us/step - loss: 26.0140\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 826us/step - loss: 27.1469\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 812us/step - loss: 26.9408\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 733us/step - loss: 24.1130\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 760us/step - loss: 23.8150\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 774us/step - loss: 22.6651\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 732us/step - loss: 24.4776\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 24.5342\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 829us/step - loss: 24.5584\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 809us/step - loss: 22.0396\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 718us/step - loss: 33.1408\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 772us/step - loss: 24.1689\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 842us/step - loss: 24.7836\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 808us/step - loss: 23.1900\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 799us/step - loss: 22.1272\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 675us/step - loss: 24.0319\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 798us/step - loss: 22.1871\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 707us/step - loss: 21.8150\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 737us/step - loss: 26.1639\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 683us/step - loss: 25.7225\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 756us/step - loss: 26.8641\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 777us/step - loss: 23.2293\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 768us/step - loss: 22.3011\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 894us/step - loss: 23.0525\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 782us/step - loss: 22.0485\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 838us/step - loss: 22.9235\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 770us/step - loss: 24.8779\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 715us/step - loss: 23.5248\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 693us/step - loss: 23.8898\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 740us/step - loss: 21.9410\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 740us/step - loss: 22.7101\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 783us/step - loss: 22.3466\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 731us/step - loss: 25.8769\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 741us/step - loss: 23.6453\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 723us/step - loss: 23.2024\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 771us/step - loss: 23.7665\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 712us/step - loss: 27.9240\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 684us/step - loss: 23.6130\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 795us/step - loss: 22.9736\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 750us/step - loss: 23.5642\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 745us/step - loss: 21.3995\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 787us/step - loss: 23.9838\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 783us/step - loss: 23.5320\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 834us/step - loss: 24.5940\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 704us/step - loss: 21.3488\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 728us/step - loss: 23.8754\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 724us/step - loss: 21.9220\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 721us/step - loss: 22.6377\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 741us/step - loss: 21.1243\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 786us/step - loss: 20.8456\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 759us/step - loss: 25.5897\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 705us/step - loss: 21.5373\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 751us/step - loss: 23.7864\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 745us/step - loss: 24.5756\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 684us/step - loss: 21.1723\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 764us/step - loss: 20.1143\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 802us/step - loss: 24.2915\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 776us/step - loss: 23.3132\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 677us/step - loss: 21.8828\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 768us/step - loss: 23.5090\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 741us/step - loss: 20.7544\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 719us/step - loss: 21.5934\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 780us/step - loss: 21.5106\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 769us/step - loss: 20.8629\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 725us/step - loss: 21.6407\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 711us/step - loss: 21.2327\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 736us/step - loss: 21.9416\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 706us/step - loss: 26.0078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x141a423aa00>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, \n",
    "                                                   random_state = seed)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = X.shape[1], activation = \"relu\"))\n",
    "model.add(Dense(6, activation = \"relu\"))\n",
    "model.add(Dense(1)) # 분류 하는게 아니기 때문에 output층만 넣어주면 됨\n",
    "\n",
    "# 모델 설정\n",
    "model.compile(loss = \"MSE\", optimizer = \"adam\")\n",
    "\n",
    "# 모델 실행\n",
    "model.fit(X_train, Y_train, epochs = 200, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9594094a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19.877052  , 19.66398   , 25.216846  ,  8.107042  , 11.661325  ,\n",
       "       15.956578  , 21.376907  , 19.856289  , 12.225715  , 14.5429535 ,\n",
       "        6.8514333 , 11.489713  , 15.630711  ,  3.0263422 , 37.827297  ,\n",
       "       26.014845  , 21.012411  , 28.854229  , 25.465384  , 19.217316  ,\n",
       "       20.442122  , 21.675404  , 14.371797  , 24.313854  , 18.0909    ,\n",
       "        9.660403  , 14.192597  , 19.6424    , 30.37129   , 16.913893  ,\n",
       "       13.679952  , 15.721239  , 14.232022  , 18.094883  , 25.061699  ,\n",
       "       19.744362  ,  4.9429297 , 21.204182  , 10.127927  , 12.324111  ,\n",
       "       20.87815   , 16.153646  , 15.155949  , 15.509277  , 15.519721  ,\n",
       "       21.296532  , 11.329627  , 23.972744  ,  7.462847  , 18.555294  ,\n",
       "       14.632919  , 14.575588  , 18.449507  , 26.767986  , 13.663051  ,\n",
       "       14.561099  , 14.956643  ,  7.000858  ,  9.851575  , 17.636057  ,\n",
       "       14.423682  , 15.88252   , 26.713722  , 27.03122   , 20.10191   ,\n",
       "       24.011885  , 17.275654  , 13.877675  , 13.544572  , 19.908913  ,\n",
       "       12.173692  , 15.22296   , 25.39884   , 22.657282  , 20.783783  ,\n",
       "        4.3087144 , 29.94858   , 17.463509  , 22.762539  , 13.5176935 ,\n",
       "       21.268042  , 18.263035  , 17.355513  , 31.58248   , 32.33801   ,\n",
       "       21.233408  , 19.74445   ,  7.8621945 , 28.721745  , 16.069204  ,\n",
       "        8.488976  ,  9.775301  , 18.56063   , 23.35818   , 19.580908  ,\n",
       "       15.522876  , -0.33332184, 21.293575  , 10.055623  , 14.594241  ,\n",
       "       19.651237  , 20.073238  , 28.69325   , 17.422203  , 20.170418  ,\n",
       "       17.67282   ,  5.7701335 , 15.94884   , 19.062162  , 22.799862  ,\n",
       "       30.77607   , 11.820586  , 15.495761  , 10.561171  ,  7.0973134 ,\n",
       "       16.958906  ,  7.1688924 , 20.013277  ,  7.7615848 , 40.344612  ,\n",
       "       22.480864  ,  9.358414  , 12.780744  , 15.278567  , 19.269619  ,\n",
       "       15.159478  , 32.376472  ,  7.6187024 , 17.526312  , 29.903872  ,\n",
       "       14.090767  , 11.019374  , 14.709163  , 19.368805  ,  9.182668  ,\n",
       "       28.356346  , 18.924791  , 14.253177  , 21.299023  , 10.325013  ,\n",
       "        7.830978  , 16.339672  , 30.233255  , 23.761198  , 22.873764  ,\n",
       "       11.951343  , 27.703829  , 25.049988  ,  9.566414  ,  4.4263034 ,\n",
       "       24.799726  , 23.232893  ], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction = model.predict(X_test).flatten() # 1차원으로 형 변환\n",
    "Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6c6c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제가격 : 22.600, 예상가격 : 19.877\n",
      "실제가격 : 50.000, 예상가격 : 19.664\n",
      "실제가격 : 23.000, 예상가격 : 25.217\n",
      "실제가격 : 8.300, 예상가격 : 8.107\n",
      "실제가격 : 21.200, 예상가격 : 11.661\n",
      "실제가격 : 19.900, 예상가격 : 15.957\n",
      "실제가격 : 20.600, 예상가격 : 21.377\n",
      "실제가격 : 18.700, 예상가격 : 19.856\n",
      "실제가격 : 16.100, 예상가격 : 12.226\n",
      "실제가격 : 18.600, 예상가격 : 14.543\n"
     ]
    }
   ],
   "source": [
    "# 예측 값과 실제 값 출력\n",
    "for i in range(10):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격 : {:.3f}, 예상가격 : {:.3f}\".format(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e984c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "plt.plot(Y_test, c = \"red\")\n",
    "plt.plot(Y_prediction, c = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ae4cd",
   "metadata": {},
   "source": [
    "# 오후"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f61b4",
   "metadata": {},
   "source": [
    "### CNN Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fcdc7",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d66d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "71091e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b360432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data : 60000, Test_data : 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train_data : {:04d}, Test_data : {:04d}\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fc95e30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAIHCAYAAAAVaM+IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUyElEQVR4nO3dXaidB5no8eex23x0Kiajux2Q4eSMg4VRpOhqtdR0SimxVMWP1gsPjgMWU+pVrXphLypadCRSqKKHIUOq3pw5Tr+sMjfFYqfVo9OzQGSmtcoIUUvQs82kRUOMxD7nIstD1KcnyX7flbWz8vtBIG/W3s96eHmb/ve7117JqgoAgD/0gkUvAABsTCIBAGiJBACgJRIAgJZIAABaIgEAaK2ciSd56UtfWjt27DgTTwUAnIb9+/fHL37xi+weOyORsGPHjphOp2fiqQCA0zCZTJ73Md9uAABaIgEAaIkEAKC17kjIzDsy818y81uZ+coxlwIAFm9dkZCZOyPioqr664i4KSI+PepWAMDCrfdOwq6I+MeIiKr694j409E2AgA2hPVGwoURsXbC8bHM/L1Zmbk7M6eZOV1bWwsA4Oyy3kh4NiK2n3D8XFU9d+IHVNXeqppU1WR1dXXdCwIAi7HeSHgsIm6IiMjMv4qIp0fbCADYENb7jov/HBHXZeZjEfHLOP7iRQBgiawrEmbfWrh55F0AgA3EmykBAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAK2VRS8AnJrnnntu8IyjR4+OsMnG8qUvfWnwjMOHD4+wScSTTz45eMZdd901fJGIuO222wbP+NznPjfCJhFbt24dPOPOO+8cYZOIm2++eZQ55wp3EgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWuv+EcjM/LeIODg73FtV/2OclQCAjWDI+yT8vKquGW0TAGBDGfLthuHv7AIAbFjrioTM/JOIeHlmPpqZ/5SZf958zO7MnGbmdG1tbfCiAMCZta5IqKrDVfXyqroyIv4hIv7o/TKram9VTapqsrq6OnRPAOAMW++dhPNOOHSbAACW0HpfuPiXmXl3RPxm9su/mAEAS2ZdkVBVP4iIK0beBQDYQLyZEgDQEgkAQEskAACtIe+4CHPx7LPPDp7x29/+doRNIr73ve8NnvHQQw+NsEnEM888M3jG3r17hy/C89qxY8fgGR/84AeHLxIR+/btGzzjxS9+8QibROzcuXPwjKuvvnqETThd7iQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBrZdELsDyefvrpUeZccsklg2ccOnRo+CKcM17wgnG+Xtq3b9/gGVu3bh1hk4gbb7xx8IwLL7xwhE0iLrjggsEzVldXR9iE0+VOAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANBaWfQCLI+XvOQlo8y56KKLBs84dOjQCJvwfHbt2jV4xljXy/333z94xubNm0fYJOKqq64aZQ5sFO4kAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBAa2XRC7A8tm7dOsqcL37xi4Nn3HvvvcMXiYjLL7988Izrr79+hE3G8YY3vGGUOQ8++ODgGZs2bRphk4if/exng2d85jOfGWETWD7uJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAACtrKq5P8lkMqnpdDr354HfOXr06ChzNm3aNHjGbbfdNsImEXv27Bk84xvf+MYIm0RceeWVo8wBFm8ymcR0Os3uMXcSAICWSAAAWiIBAGiJBACgJRIAgNZJIyEzVzPzE5l5x+z44sx8ODO/lZmfnv+KAMAinMqdhDsj4mhEvHB2fFdE3FhVV0TEjsx83Zx2AwAW6KSRUFXviYhHIyIycyUitlTV/tnD90XE5XPbDgBYmNN9TcJqRBw84fhgRGzvPjAzd2fmNDOna2tr690PAFiQ042EZyJi2wnH2yOiLYCq2ltVk6qarK6urm87AGBhTisSqupIRGzOzJfN/ugdEfHw6FsBAAu3so7PuTUi7s3MoxHx1ar6/sg7AQAbwClFQlU9EhGPzH7/v8OLFQFg6XkzJQCgJRIAgJZIAABa63nhImx4mzdvXvQK/8/27e1biSzEZz/72VHm7Ny5c/CMzBxhE2Ce3EkAAFoiAQBoiQQAoCUSAICWSAAAWiIBAGiJBACgJRIAgJZIAABaIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWiuLXgCW3S233DLKnMcff3zwjAceeGCETSKeeOKJwTNe9apXjbAJME/uJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQGtl0QvAstu0adMoc/bu3Tt4xsMPPzzCJhFvfetbB89429veNnyRiLjiiisGz3j7298+wiYRmTnKHNgo3EkAAFoiAQBoiQQAoCUSAICWSAAAWiIBAGiJBACgJRIAgJZIAABaIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWllVc3+SyWRS0+l07s8D/P89/vjjo8y59tprB8949tlnR9hkHHffffcoc66//vrBMy644IIRNoFTN5lMYjqdZveYOwkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANBaWfQCwJlz2WWXjTLniSeeGDzjAx/4wAibRNxzzz2DZ7z3ve8dYZOIH/3oR4NnfPjDHx5hk4gXvehFo8zh3OZOAgDQEgkAQEskAAAtkQAAtEQCANASCQBA66SRkJmrmfmJzLxjdvw3mflkZj6SmQ/Nf0UAYBFO5X0S7oyI/4iI82fH2yLiI1X14LyWAgAW76R3EqrqPRHx6Al/tC0iDp3s8zJzd2ZOM3O6tra2/g0BgIVYz2sSViJiT2Y+lpm7n++DqmpvVU2qarK6urr+DQGAhTjtSKiqj1bV6yPijRHxzsx85fhrAQCLdtqRkJm/ex3DkYj4ZUTUqBsBABvCev6Bp7/LzMtmn/tAVT058k4AwAZwSpFQVY9ExCOz34/zT5QBABuaN1MCAFoiAQBoiQQAoJVV8//hhMlkUtPpdO7PA5w9fv3rX48y5zvf+c7gGddcc80Im0SM8ffpDTfcMMImEV/+8pdHmcPym0wmMZ1Os3vMnQQAoCUSAICWSAAAWiIBAGiJBACgJRIAgJZIAABaIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWiIBAGitLHoB4Ny0ZcuWUeZcddVVg2ecd955wxeJiGPHjg2e8ZWvfGX4IhHxgx/8YPCMiy++eIRNOJu5kwAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0Vha9AHD2OXDgwOAZ999//wibRHz7298ePOPYsWMjbDKOSy+9dJQ5r3jFK0aZw7nNnQQAoCUSAICWSAAAWiIBAGiJBACgJRIAgJZIAABaIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWiIBAGitLHoB4NSsra0NnvH5z39+hE0ivvCFLwye8fTTT4+wycZy3nnnDZ6xY8eO4YtERGaOModzmzsJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQGtl0QvARvarX/1q8Iyvfe1rI2wS8fGPf3zwjB/+8IcjbLJ8rr766lHmfOpTnxo847Wvfe0Im8A43EkAAFoiAQBoiQQAoCUSAICWSAAAWif96YbM3BYRfx8RfxbHo+JvI2JTRPz3iNgSEf+rqj48xx0BgAU4lR+BPD8ibq2qA5n5poj4UET8RUTcWFX7M/OezHxdVf3rXDcFAM6ok367oaoOVNWB2eGhiDgaEVuqav/sz+6LiMvnsx4AsCin/JqEzHxZHL+LcGdEHDzhoYMRsb35+N2ZOc3M6dra2uBFAYAz65QiITPfHBG3R8T7IuI/I2LbCQ9vj4g/qoCq2ltVk6qarK6ujrAqAHAmnTQSMvPVEfGWqrqpqg5W1ZGI2Dy7sxAR8Y6IeHieSwIAZ96pvHDx2ojYmZmPzI5/EhG3RsS9mXk0Ir5aVd+f034AwIKcNBKqak9E7Gke8mJFAFhi3kwJAGiJBACgJRIAgNapvHARzqjDhw8PnvHTn/50hE0i3v3udw+e8d3vfneETZbPrl27RpnzsY99bPCMSy+9dIRNIjJzlDmwUbiTAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALRWFr0AG8ORI0cGz7jllluGLxIR3/zmNwfPeOqpp0bYZPlcd911o8y5/fbbB8+45JJLhi8SES984QtHmQP8MXcSAICWSAAAWiIBAGiJBACgJRIAgJZIAABaIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWiIBAGiJBACgtbLoBc5l+/fvHzzjk5/85PBFIuLrX//64Bk//vGPR9hk+Zx//vmjzLnjjjsGz3j/+98/wiYRmzZtGmUOsLG5kwAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0Vha9wLnsvvvuGzxj3759I2yysbzmNa8ZPONd73rXCJtErKwM/09k9+7dI2wSsWXLllHmAJwqdxIAgJZIAABaIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAICWSAAAWiIBAGiJBACgJRIAgJZIAABaIgEAaIkEAKCVVTX3J5lMJjWdTuf+PADA6ZlMJjGdTrN7zJ0EAKAlEgCAlkgAAFoiAQBoiQQAoLVysg/IzG0R8fcR8WdxPCr+NiLeEBEfiYj/ExG/qapdc9wRAFiAk0ZCRJwfEbdW1YHMfFNEfCginoqIj1TVg3PdDgBYmJN+u6GqDlTVgdnhoYg4HBHbZr8HAJbUKb8mITNfFsfvItwVx+9A7MnMxzJz9/N8/O7MnGbmdG1tbZRlAYAz55QiITPfHBG3R8T7ZncWPlpVr4+IN0bEOzPzlX/4OVW1t6omVTVZXV0dd2sAYO5O5YWLr46It1TVTSf82UpVHYuIIxHxy4iY/3s7AwBn1Km8cPHaiNiZmY/Mjn8SET/PzMtmn/9AVT05p/0AgAU5aSRU1Z6I2HMGdgEANhBvpgQAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBASyQAAC2RAAC0RAIA0BIJAEBLJAAALZEAALREAgDQEgkAQEskAAAtkQAAtEQCANASCQBAK6tq/k+SuRYRPz7Jh700In4x92XOTc7tfDm/8+Pczo9zO19n0/n9L1W12j1wRiLhVGTmtKomi95jGTm38+X8zo9zOz/O7Xwty/n17QYAoCUSAIDWRoqEvYteYIk5t/Pl/M6Pczs/zu18LcX53TCvSQAANpaNdCcBANhANkQkZOYdmfkvmfmtzHzlovdZJpn5b5n5yOzXf1v0Pme7zFzNzE9k5h2z44sz8+HZtfvpRe93tmvO799k5pOz6/ehRe93tsrMbZn5P2fn8dHM/K+u3fE8z/ldimt3ZdELZObOiLioqv46M18VEZ+OiOsWvNYy+XlVXbPoJZbInRHxHxFx/uz4roi4sar2Z+Y9mfm6qvrXhW139vvD87stIj5SVQ8ubKPlcH5E3FpVBzLzTRHxoYj4i3DtjqU7v0/FEly7G+FOwq6I+MeIiKr694j408Wus3SeW/QCy6Sq3hMRj0ZEZOZKRGypqv2zh++LiMsXtNpSOPH8zmyLiEOL2WZ5VNWBqjowOzwUEUfDtTua5vwejiW5djdCJFwYEWsnHB/LzI2w11kvM/8kIl4+u/31T5n554veacmsRsTBE44PRsT2Be2yrFYiYk9mPpaZuxe9zNkuM18Wx7/KvTNcu6M74fzeFUty7W6E/xk/G79/cT5XVb76HUFVHa6ql1fVlRHxD3H8LwbG80wc/2rhd7bH7wcvA1XVR6vq9RHxxoh4p9csrV9mvjkibo+I90XEf4Zrd1Qnnt/ZnYWluHY3QiQ8FhE3RERk5l9FxNOLXWd5ZOZ5Jxz6C2BkVXUkIjbPvnqIiHhHRDy8wJWWzuxbOhERRyLilxHhZ7bXITNfHRFvqaqbquqga3dcf3h+Z3+2FNfuwl+4GBH/HBHXZeZjcfxE3rTgfZbJX2bm3RHxm9mvmxe8zzK6NSLuzcyjEfHVqvr+ohdaMn+XmZfF8b+rHqiqJxe90Fnq2ojYmZmPzI5/Eq7dMXXn9+fLcO16MyUAoLURvt0AAGxAIgEAaIkEAKAlEgCAlkgAAFoiAQBoiQQAoCUSAIDW/wWnZaY/Rs8iOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (9, 9))\n",
    "plt.imshow(X_train[0], cmap = \"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0fb0645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  3 18 18 18126136175 26166255247127  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 30 36 94154170253253253253253225172253242195 64  0  0  0  0\n",
      "  0  0  0  0  0  0  0 49238253253253253253253253253251 93 82 82 56 39  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 18219253253253253253198182247241  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 80156107253253205 11  0 43154  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 14  1154253 90  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0139253190  2  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 11190253 70  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 35241225160108  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 81240253253119 25  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 45186253253150 27  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 93252253187  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0249253249 64  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46130183253253207  2  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 39148229253253253250182  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 24114221253253253253201 78  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 23 66213253253253253198 81  2  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 18171219253253253253195 80  9  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 55172226253253253253244133 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0136253253253212135132 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "for row in X_train[0]:\n",
    "    for col in row:\n",
    "        sys.stdout.write(\"%3d\" % col)\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f8c093ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "12fdc869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 정규화 : 서능을 좋게 하기 위해서 -> 숫자를 0~1 사이의 수로 변환\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n",
    "# X_train[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3982614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ddce689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "# 타겟 값을 분류해서 category로 변경\n",
    "Y_train = np_utils.to_categorical(Y_class_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_class_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ccbcb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0], Y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687bdfa",
   "metadata": {},
   "source": [
    "### 모델 기본 프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "80224fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "model_dir = \"./CNN_model\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath = \"./\" + model_dir + \"/{epoch:02d}-{val_loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4c03b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = X_train.shape[1], activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d19d4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 설정\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = \"val_loss\",\n",
    "                             verbose = 1, save_best_only = True)\n",
    "\n",
    "# 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor = \"val_loss\", patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b08858a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"ACC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "26baf276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15075, saving model to ././CNN_model\\01-0.1508.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15075 to 0.10530, saving model to ././CNN_model\\02-0.1053.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10530 to 0.08199, saving model to ././CNN_model\\03-0.0820.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08199 to 0.07904, saving model to ././CNN_model\\04-0.0790.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07904 to 0.06885, saving model to ././CNN_model\\05-0.0688.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06885 to 0.06552, saving model to ././CNN_model\\06-0.0655.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06552 to 0.06255, saving model to ././CNN_model\\07-0.0626.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06255 to 0.06242, saving model to ././CNN_model\\08-0.0624.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06242 to 0.06175, saving model to ././CNN_model\\09-0.0618.hdf5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06175\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06175\n"
     ]
    }
   ],
   "source": [
    "# 모델 fit\n",
    "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 200,\n",
    "         batch_size = 200, verbose = 0, callbacks = [checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "97470704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0772 - ACC: 0.9808\n",
      "\n",
      " Test ACC : 0.9808\n"
     ]
    }
   ],
   "source": [
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test ACC : %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fa945b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 set의 오차 출력\n",
    "y_vloss = history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "44455734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 셋 오차 출력\n",
    "y_loss = history.history[\"loss\"]\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.plot(x_len, y_vloss, marker = \"o\", c = \"red\", label = \"Test\")\n",
    "plt.plot(x_len, y_loss, marker = \"o\", c = \"blue\", label = \"Train\")\n",
    "\n",
    "plt.legend(loc = \"best\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
